{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization in Deep Learning — Comprehensive Explanation\n",
        "\n",
        "---\n",
        "\n",
        "## 1) What “Regularization” Actually Does\n",
        "\n",
        "**Goal:** Improve generalization by discouraging solutions that overfit spurious patterns.  \n",
        "**How:** Add bias (structural or statistical preferences) to reduce variance.  \n",
        "\n",
        "**Mechanisms:**  \n",
        "(a) Penalize parameters or functions,  \n",
        "(b) Manipulate data or labels,  \n",
        "(c) Alter architecture or optimization to prefer “simpler” or “flatter” solutions.  \n",
        "\n",
        "Formally, most methods can be seen as minimizing a *regularized risk*:\n",
        "\n",
        "$$\n",
        "\\min_\\theta \\; L_{\\text{emp}}(\\theta) + \\lambda \\, \\Omega(\\theta, f_\\theta, D)\n",
        "$$\n",
        "\n",
        "where \\( L_{\\text{emp}} \\) is empirical loss, and \\( \\Omega \\) encodes a simplicity or robustness prior.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Parameter & Function Penalties (Explicit Priors)\n",
        "\n",
        "### L2 (Ridge / Weight Decay)\n",
        "\n",
        "**Penalty:**\n",
        "$$\n",
        "\\Omega = \\frac{1}{2} \\|\\theta\\|_2^2\n",
        "$$\n",
        "\n",
        "**Effect:** Smoothly shrinks weights; keeps many small but nonzero → stable gradients, good with collinearity.  \n",
        "**Implementation Nuance:** *AdamW* decouples weight decay from the gradient, performing better than naïvely adding L2 to the loss with Adam.\n",
        "\n",
        "---\n",
        "\n",
        "### L1 (Lasso)\n",
        "\n",
        "**Penalty:**\n",
        "$$\n",
        "\\Omega = \\|\\theta\\|_1\n",
        "$$\n",
        "\n",
        "**Effect:** Promotes sparsity (feature selection, pruning readiness).  \n",
        "**Gotcha:** Can hurt when many correlated features; *Elastic Net* helps.\n",
        "\n",
        "---\n",
        "\n",
        "### Elastic Net\n",
        "\n",
        "**Penalty:**\n",
        "$$\n",
        "\\alpha \\|\\theta\\|_1 + \\frac{1}{2}(1 - \\alpha)\\|\\theta\\|_2^2\n",
        "$$\n",
        "\n",
        "**Use When:** You want both sparsity and stability.\n",
        "\n",
        "---\n",
        "\n",
        "### Group Lasso / Structured Sparsity\n",
        "\n",
        "**Penalty:**\n",
        "$$\n",
        "\\sum_g \\|\\theta_g\\|_2\n",
        "$$\n",
        "over predefined groups (e.g., channels, attention heads).\n",
        "\n",
        "**Use:** Compress models by zeroing whole groups → hardware-friendly.\n",
        "\n",
        "---\n",
        "\n",
        "### Max-Norm\n",
        "\n",
        "**Constraint:**\n",
        "$$\n",
        "\\|w_j\\|_2 \\leq c\n",
        "$$\n",
        "per unit or filter.\n",
        "\n",
        "**Use:** Keeps layers well-conditioned; older but still effective for some CNNs and RNNs.\n",
        "\n",
        "---\n",
        "\n",
        "### Spectral Norm / Orthogonal Regularization\n",
        "\n",
        "**Idea:** Control the Lipschitz constant or maintain near-orthogonal weight matrices.  \n",
        "**Use:** Stabilizes GANs, improves robustness, and helps deep nets avoid exploding/vanishing gradients.\n",
        "\n",
        "---\n",
        "\n",
        "### Jacobian / Gradient Penalties\n",
        "\n",
        "**Penalty:**\n",
        "$$\n",
        "\\|\\nabla_x f_\\theta(x)\\|_2\n",
        "$$\n",
        "(or WGAN-GP style on discriminators).\n",
        "\n",
        "**Effect:** Smooths the function with respect to inputs → robustness and semi-supervised gains.\n",
        "\n",
        "---\n",
        "\n",
        "### Entropy / Confidence Penalty & Label Smoothing\n",
        "\n",
        "**Entropy Regularization:**  \n",
        "Add  \n",
        "$$\n",
        "-\\beta \\, H(p_\\theta(y|x))\n",
        "$$  \n",
        "to discourage overconfident predictions.\n",
        "\n",
        "**Label Smoothing:**  \n",
        "Replace one-hot \\( y \\) with  \n",
        "\\( (1-\\epsilon) \\) on the true class + \\( \\epsilon/K \\) elsewhere → better calibration, less overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Stochastic Regularizers (Noise as a Prior)\n",
        "\n",
        "### Dropout (Unit-Wise) & DropConnect (Weight-Wise)\n",
        "\n",
        "**Mechanism:** Randomly zero units or weights at train time, scale at test time.  \n",
        "**Intuition:** Approximate model averaging over subnetworks → combats co-adaptation.  \n",
        "\n",
        "**Variants:**  \n",
        "- *SpatialDropout* (for conv feature maps)  \n",
        "- *DropBlock* (contiguous spatial masks for CNNs)  \n",
        "- *Stochastic Depth / LayerDrop* (skip whole layers — excellent in ResNets/Transformers)\n",
        "\n",
        "---\n",
        "\n",
        "### Gaussian Noise (Inputs, Activations, Gradients)\n",
        "\n",
        "**Effect:** Acts as a denoising prior; for inputs, it’s akin to Tikhonov (L2) regularization in linear models.  \n",
        "**Practice:** Mild activation noise aids robustness; gradient noise helps escape sharp minima.\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Data-Side Regularization (Build a Better Training Distribution)\n",
        "\n",
        "### Classic Augmentations\n",
        "\n",
        "Flips, crops, color jitter, blur; *RandAugment* and *AutoAugment* automatically select augmentation policies.\n",
        "\n",
        "---\n",
        "\n",
        "### Cutout / Mixup / CutMix / Manifold Mixup\n",
        "\n",
        "- **Cutout:** Zero random patches → forces global reasoning.  \n",
        "- **Mixup:**  \n",
        "  $$\n",
        "  (\\tilde{x}, \\tilde{y}) = \\lambda (x_i, y_i) + (1 - \\lambda)(x_j, y_j)\n",
        "  $$\n",
        "- **CutMix:** Paste image patches and mix labels by patch area → preserves local realism.  \n",
        "- **Manifold Mixup:** Mix features at hidden layers → stronger smoothing.\n",
        "\n",
        "---\n",
        "\n",
        "### Semi-Supervised Consistency\n",
        "\n",
        "Π-Model, *Mean Teacher*, *FixMatch*, *VAT*: enforce prediction consistency under augmentations or perturbations; huge gains with few labels.\n",
        "\n",
        "---\n",
        "\n",
        "### Domain-Specific\n",
        "\n",
        "- **SpecAugment (audio)**  \n",
        "- **Token masking / word dropout (NLP)**  \n",
        "- **Geometric / textural warps (vision)**  \n",
        "- **SMOTE (imbalanced tabular)**\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Architectural & Training-Procedure Regularizers\n",
        "\n",
        "### Early Stopping\n",
        "\n",
        "The simplest yet strongest when data are scarce — stop when validation loss bottoms.  \n",
        "Acts as implicit regularization that caps effective capacity.\n",
        "\n",
        "---\n",
        "\n",
        "### Normalization Layers (BatchNorm, LayerNorm, GroupNorm, WeightNorm)\n",
        "\n",
        "**Effect:** Reduce internal covariate shift, add noise via batch statistics, and smooth the loss landscape.  \n",
        "**Tip:** When batches are small, prefer LayerNorm or GroupNorm over BatchNorm.\n",
        "\n",
        "---\n",
        "\n",
        "### Averaging & Momentum of Weights\n",
        "\n",
        "- **SWA (Stochastic Weight Averaging):** Averages checkpoints → flatter minima.  \n",
        "- **EMA (Exponential Moving Average / Polyak averaging):** Use moving-average weights at inference → better stability and calibration.\n",
        "\n",
        "---\n",
        "\n",
        "### Optimizer-Level\n",
        "\n",
        "- **Weight Decay (Decoupled):** Standard L2-style regularization in optimizers like AdamW.  \n",
        "- **SAM (Sharpness-Aware Minimization):** Minimizes worst-case loss in a small neighborhood → biases toward flat minima (better generalization).  \n",
        "- **Lookahead:** Couples fast/slow weights for smoother updates.\n",
        "\n",
        "---\n",
        "\n",
        "### Knowledge Distillation\n",
        "\n",
        "Train a *student* on *teacher* soft targets; acts like label smoothing with richer structural information.\n",
        "\n",
        "---\n",
        "\n",
        "### Pruning & Quantization-Aware Training\n",
        "\n",
        "Capacity control and compression; when done during training, they act as regularizers (*lottery ticket hypothesis* style).\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Bayesian & Probabilistic Views (Principled Priors)\n",
        "\n",
        "- **MAP = Weight Decay:**  \n",
        "  L2 corresponds to Gaussian prior; L1 to Laplace prior.\n",
        "\n",
        "- **Variational Inference & Bayesian NNs:**  \n",
        "  Learn a distribution \\( q(\\theta) \\) minimizing \\( KL(q || p) \\).  \n",
        "\n",
        "- **MC-Dropout:**  \n",
        "  Interprets dropout as approximate Bayesian inference → yields uncertainty estimates.\n",
        "\n",
        "- **PAC-Bayes & Flat-Minima Intuition:**  \n",
        "  Theoretical bounds favor solutions robust to parameter noise, motivating SGD noise, SWA, SAM, label smoothing, and mixup.\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Transformer / NLP / Audio / Vision Specifics\n",
        "\n",
        "- **Transformers:** Attention dropout, stochastic depth, label smoothing, AdamW weight decay, token masking (BERT), SpecAugment (speech), MAE/SimMIM objectives (self-supervised).  \n",
        "- **CNNs:** DropBlock, CutMix, RandAugment, stochastic depth, spectral norm for stability (GANs, robust CNNs).  \n",
        "- **RNNs:** Variational dropout (shared mask across time), zoneout, gradient clipping (prevents explosion).\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Sharpness, Margins, and Why Flatness Helps\n",
        "\n",
        "**Flat minima (low curvature)** generalize better — achieved through noise, dropout, BatchNorm noise, SWA, or SAM.  \n",
        "**Large margins (in logit space)** correlate with generalization; label smoothing slightly reduces margin but improves calibration; mixup enlarges vicinal margins.\n",
        "\n",
        "---\n",
        "\n",
        "## 9) Practical Recipes (What to Try First)\n",
        "\n",
        "### Vision (ImageNet-like)\n",
        "\n",
        "- AdamW or SGD+Nesterov with decoupled weight decay (1e−4–5e−4).  \n",
        "- Label smoothing (ε ≈ 0.1), RandAugment, Mixup (0.2–0.4), CutMix (p ≈ 0.5).  \n",
        "- Stochastic depth (0.1–0.2) for deep nets; DropPath for transformers.  \n",
        "- EMA of weights (0.999) at inference.  \n",
        "- Optionally use SWA/SWALR late or SAM if compute allows.\n",
        "\n",
        "---\n",
        "\n",
        "### NLP (Transformer Encoder/Decoder)\n",
        "\n",
        "- AdamW with wd = 0.01 (BERT-style).  \n",
        "- Dropout 0.1 on attention and hidden layers.  \n",
        "- Label smoothing (0.1), token masking for pretraining.  \n",
        "- Layer drop for very deep models; weight tying for parameter efficiency.  \n",
        "- EMA can stabilize fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "### Audio / Speech\n",
        "\n",
        "- SpecAugment, label smoothing, AdamW.  \n",
        "- Mixup in spectrogram domain + small dropout; optionally SWA.\n",
        "\n",
        "---\n",
        "\n",
        "### Tabular\n",
        "\n",
        "- Modest L2, early stopping, target noise, or label smoothing for noisy labels.  \n",
        "- Mixup for imbalanced classes (use carefully).  \n",
        "- Gradient boosting often beats deep nets unless data is large and feature-rich.\n",
        "\n",
        "---\n",
        "\n",
        "## 10) Hyperparameters & Diagnostics\n",
        "\n",
        "- **Weight Decay:** too high → underfit; too low → overfit. Sweep log-scale \\([10^{-6}, 10^{-2}]\\).  \n",
        "- **Dropout Rate:** 0.1–0.5 typical; too high harms representation learning.  \n",
        "- **Label Smoothing:** ε ∈ [0.05, 0.2]; trade-off calibration vs. accuracy.  \n",
        "- **Mixup α:** 0.2–0.8; higher → smoother but slower.  \n",
        "- **Stochastic Depth:** linearly increase with depth; total drop prob 0.1–0.2 for 50–100 layers.  \n",
        "- **SAM ρ:** 0.05–0.2 common; ~2× compute cost.\n",
        "\n",
        "**Red Flags**\n",
        "- Train ≪ Val accuracy → add regularization (augmentations, wd, dropout).  \n",
        "- Train ≈ Val poor → underpowered model or over-regularization.  \n",
        "- Overconfident predictions → label smoothing, temperature scaling, or mixup.\n",
        "\n",
        "---\n",
        "\n",
        "## 11) Lesser-Known but Powerful Tricks\n",
        "\n",
        "- **Manifold Mixup:** Mix features in hidden layers → stronger regularization.  \n",
        "- **Mixout:** Stochastic interpolation with reference weights (e.g., pretrained) to prevent over-drift during fine-tuning.  \n",
        "- **Orthogonal Initialization + Regularization:** Stabilizes very deep networks.  \n",
        "- **Consistency Regularization (FixMatch-style):** Strong/weak augmentation views — powerful for semi-supervised learning.  \n",
        "- **Adversarial Training (FGSM/PGD, TRADES):** Robustness-oriented regularization; may reduce clean accuracy if mis-tuned.\n",
        "\n",
        "---\n",
        "\n",
        "## 12) How These Pieces Connect (Unifying Lenses)\n",
        "\n",
        "- **Bayesian View:** Penalties ↔ Priors; Noise ↔ Posterior Averaging.  \n",
        "- **Margin/Flatness View:** Augmentations, smoothing, SWA/SAM bias toward flat, large-margin solutions.  \n",
        "- **Compression View:** Regularizers induce sparsity or low-complexity representations; compressed models often generalize better.\n",
        "\n",
        "---\n",
        "\n",
        "## 13) Quick “When to Use What” Cheat-Sheet\n",
        "\n",
        "| Scenario | Recommended Regularizers |\n",
        "|-----------|---------------------------|\n",
        "| **Small data / high overfit** | Heavy augmentations, label smoothing, dropout, early stopping, modest wd |\n",
        "| **Very deep nets** | Stochastic depth, DropPath, AdamW, warmup + cosine decay, possibly SAM |\n",
        "| **Noisy labels** | Label smoothing, mixup, co-teaching, loss correction |\n",
        "| **Need calibration** | Label smoothing, temperature scaling, EMA, SWA |\n",
        "| **Need robustness** | Adversarial / consistency training, Jacobian penalties, spectral norm |\n",
        "| **Fine-tuning pretrained LMs/ViTs** | Low LR, AdamW, small dropout, mixout (stay near pretrained weights), layerwise LR decay |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "We8LuLwpFmw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Canonical Literature of Regularization in Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "## A) Classical, Norm-Based & Structured Penalties\n",
        "\n",
        "| **Method / Concept** | **Reference** | **Venue / Year** | **Contribution** |\n",
        "|-----------------------|----------------|------------------|------------------|\n",
        "| **Tikhonov Regularization** | A. N. Tikhonov | *On the Stability of Inverse Problems*, 1943 | Introduced the foundational idea of stabilizing ill-posed problems through additive norm penalties. |\n",
        "| **Ridge Regression** | Hoerl & Kennard | *Technometrics*, 1970 | Introduced ℓ₂ regularization for linear regression — basis for weight decay. |\n",
        "| **Lasso (ℓ₁)** | Tibshirani | *J. Royal Stat. Soc. B*, 1996 | Promoted sparsity via ℓ₁ penalty — variable selection and compression. |\n",
        "| **Elastic Net** | Zou & Hastie | *J. Royal Stat. Soc. B*, 2005 | Combined ℓ₁ (sparsity) and ℓ₂ (stability) for correlated features. |\n",
        "| **Group Lasso** | Yuan & Lin | *J. Royal Stat. Soc. B*, 2006 | Enforced structured sparsity by grouping variables. |\n",
        "| **Overlapping / Structured Sparsity** | Bach et al. | *Foundations and Trends in ML*, 2012 | Extended group sparsity to overlapping groups; structured model compression. |\n",
        "| **Max-Norm Constraints** | Srebro & Shraibman (theory, 2005); Hinton et al. (practice, 2012) | *Matrix Factorization Theory / Deep Nets* | Constrained per-unit norm to keep layers well-conditioned. |\n",
        "| **Weight Decay (MAP view)** | Krogh & Hertz | *NIPS*, 1992 | Interpreted weight decay as a Maximum A Posteriori (MAP) estimate under Gaussian priors. |\n",
        "| **AdamW (Decoupled Weight Decay)** | Loshchilov & Hutter | *ICLR*, 2019 | Decoupled weight decay from adaptive gradient updates for proper regularization. |\n",
        "| **Orthogonal Regularization** | Brock et al. | *arXiv*, 2017 | Promoted near-orthogonal weight matrices to stabilize optimization. |\n",
        "| **Spectral Normalization** | Miyato et al. | *ICLR*, 2018 | Controlled Lipschitz constant via spectral norm scaling — improved GAN stability. |\n",
        "| **Jacobian / Double-Backprop** | Drucker & LeCun | *NIPS*, 1992 | Penalized input–output Jacobian to encourage smoother mappings. |\n",
        "| **Contractive Autoencoders** | Rifai et al. | *ICML*, 2011 | Applied Jacobian penalties for robustness in autoencoder representations. |\n",
        "\n",
        "---\n",
        "\n",
        "## B) Stochastic Regularizers: Dropout & Friends\n",
        "\n",
        "| **Technique** | **Reference** | **Venue / Year** | **Contribution** |\n",
        "|----------------|----------------|------------------|------------------|\n",
        "| **Dropout** | Srivastava et al. | *JMLR*, 2014 | Randomly deactivate neurons; model averaging interpretation. |\n",
        "| **DropConnect** | Wan et al. | *ICML*, 2013 | Randomly drop weights instead of units. |\n",
        "| **SpatialDropout (CNNs)** | Tompson et al. | *CVPR*, 2015 | Applied dropout across feature maps in CNNs. |\n",
        "| **DropBlock** | Ghiasi et al. | *NeurIPS*, 2018 | Spatially contiguous dropout for CNNs — regularizes structured features. |\n",
        "| **Stochastic Depth (ResNets)** | Huang et al. | *ECCV*, 2016 | Randomly skip residual blocks during training — prevents overfitting in deep nets. |\n",
        "| **Shake-Shake Regularization** | Gastaldi | *arXiv*, 2017 | Stochastic combination of branches in residual blocks. |\n",
        "| **ShakeDrop** | Yamada et al. | *ECCV Workshop*, 2018 | Extension of Shake-Shake to various architectures. |\n",
        "| **Variational Dropout** | Kingma, Salimans & Welling | *NIPS*, 2015 | Bayesian interpretation of dropout with learned dropout rates. |\n",
        "| **Concrete Dropout** | Gal, Hron & Kendall | *NeurIPS*, 2017 | Differentiable dropout rate sampling via concrete distributions. |\n",
        "| **Zoneout (RNNs)** | Krueger et al. | *ICLR*, 2016 | Randomly preserve hidden activations across timesteps to regularize RNNs. |\n",
        "\n",
        "---\n",
        "\n",
        "## C) Data-Side & Vicinal Risk Regularization\n",
        "\n",
        "| **Method** | **Reference** | **Venue / Year** | **Key Contribution** |\n",
        "|-------------|----------------|------------------|----------------------|\n",
        "| **Augmentation as Regularization (AlexNet)** | Krizhevsky et al. | *NIPS*, 2012 | Demonstrated large-scale augmentations’ regularization effect. |\n",
        "| **Cutout** | DeVries & Taylor | *arXiv*, 2017 | Randomly mask patches; enforces global reasoning. |\n",
        "| **Mixup** | Zhang et al. | *ICLR*, 2018 | Linear interpolation of inputs/labels — vicinal risk minimization. |\n",
        "| **Manifold Mixup** | Verma et al. | *ICML*, 2019 | Mix hidden-layer representations instead of inputs. |\n",
        "| **CutMix** | Yun et al. | *ICCV*, 2019 | Combine image patches and labels by spatial proportion. |\n",
        "| **FMix** | Harris et al. | *arXiv*, 2020 | Mix using frequency-domain masks for more diverse compositions. |\n",
        "| **AugMix** | Hendrycks et al. | *ICLR*, 2020 | Blend multiple augmentations for robustness. |\n",
        "| **AutoAugment** | Cubuk et al. | *CVPR*, 2019 | Search-based automated augmentation policy. |\n",
        "| **RandAugment** | Cubuk et al. | *CVPR*, 2020 | Simplified, search-free augmentation parameterization. |\n",
        "| **TrivialAugment** | Müller & Hutter | *ICCV*, 2021 | Minimal augmentation strategy achieving competitive regularization. |\n",
        "\n",
        "---\n",
        "\n",
        "## D) Confidence, Entropy & Label Smoothing\n",
        "\n",
        "| **Technique** | **Reference** | **Venue / Year** | **Contribution** |\n",
        "|----------------|----------------|------------------|------------------|\n",
        "| **Entropy Regularization (Semi-supervised)** | Grandvalet & Bengio | *NIPS*, 2005 | Encouraged high-entropy (less confident) predictions on unlabeled data. |\n",
        "| **Label Smoothing** | Szegedy et al. | *CVPR*, 2016 | Smoothed target distributions to improve calibration. |\n",
        "| **Confidence Penalty** | Pereyra et al. | *ICLR*, 2017 | Penalized overconfident output distributions. |\n",
        "| **When Does Label Smoothing Help?** | Müller, Kornblith & Hinton | *NeurIPS*, 2019 | Empirical and theoretical analysis of smoothing’s effects on calibration and generalization. |\n",
        "\n",
        "---\n",
        "\n",
        "## E) Consistency, Semi-Supervision & Adversarial Consistency\n",
        "\n",
        "| **Method** | **Reference** | **Venue / Year** | **Idea** |\n",
        "|-------------|----------------|------------------|-----------|\n",
        "| **Temporal Ensembling / Π-Model** | Laine & Aila | *ICLR Workshop*, 2016 | Consistency under augmentation with temporal averaging. |\n",
        "| **Mean Teacher** | Tarvainen & Valpola | *NeurIPS*, 2017 | Teacher-student consistency via exponential moving average weights. |\n",
        "| **Virtual Adversarial Training (VAT)** | Miyato et al. | *TPAMI*, 2018 | Adversarial perturbations to enforce local smoothness. |\n",
        "| **Unsupervised Data Augmentation (UDA)** | Xie et al. | *NeurIPS*, 2019 | Combined strong augmentations with consistency objectives. |\n",
        "| **FixMatch** | Sohn et al. | *NeurIPS*, 2020 | Unified pseudo-labeling and consistency with weak/strong augmentations. |\n",
        "\n",
        "---\n",
        "\n",
        "## F) Adversarial Training as Regularization\n",
        "\n",
        "| **Method** | **Reference** | **Venue / Year** | **Core Idea** |\n",
        "|-------------|----------------|------------------|----------------|\n",
        "| **FGSM Adversarial Training** | Goodfellow, Shlens & Szegedy | *ICLR*, 2015 | Fast gradient method to enhance robustness. |\n",
        "| **PGD Adversarial Training** | Madry et al. | *ICLR*, 2018 | Strong iterative adversarial defense; robustness-as-regularization. |\n",
        "| **TRADES** | Zhang et al. | *ICML*, 2019 | Tradeoff between robustness and accuracy via KL divergence regularization. |\n",
        "| **WGAN-GP** | Gulrajani et al. | *NeurIPS*, 2017 | Gradient penalty for GAN discriminator Lipschitz constraint. |\n",
        "| **R1 / R2 Gradient Penalties** | Mescheder et al. | *ICML*, 2018 | Regularized GAN objectives for stable convergence. |\n",
        "| **Path Length Regularization (StyleGAN2)** | Karras et al. | *CVPR*, 2020 | Controlled latent–image Jacobian magnitude for high-fidelity generation. |\n",
        "\n",
        "---\n",
        "\n",
        "## G) Normalization (Implicit Regularization)\n",
        "\n",
        "| **Normalization Type** | **Reference** | **Venue / Year** | **Effect** |\n",
        "|--------------------------|----------------|------------------|-------------|\n",
        "| **Batch Normalization** | Ioffe & Szegedy | *ICML*, 2015 | Stabilized training by normalizing batch statistics; added stochastic noise. |\n",
        "| **Layer Normalization** | Ba, Kiros & Hinton | *arXiv*, 2016 | Normalization across features per sample; batch-size independent. |\n",
        "| **Group Normalization** | Wu & He | *ECCV*, 2018 | Normalized within feature groups; suited for small batches. |\n",
        "| **Weight Normalization** | Salimans & Kingma | *NeurIPS*, 2016 | Reparameterized weights by magnitude and direction for smoother optimization. |\n",
        "\n",
        "---\n",
        "\n",
        "## H) Optimization, Flat Minima & Implicit Bias\n",
        "\n",
        "| **Concept / Method** | **Reference** | **Venue / Year** | **Key Insight** |\n",
        "|------------------------|----------------|------------------|-----------------|\n",
        "| **Flat Minima Theory** | Hochreiter & Schmidhuber | *Neural Computation*, 1997 | Linked low-curvature regions to better generalization. |\n",
        "| **Early Stopping** | Morgan & Bourlard (1990); Prechelt (1998) | *Empirical & Theoretical Studies* | Implicit regularization by halting before overfitting. |\n",
        "| **Implicit Bias of SGD (Max-Margin)** | Soudry et al. | *JMLR*, 2018 | Proved SGD converges to max-margin classifiers under separability. |\n",
        "| **SWA (Stochastic Weight Averaging)** | Izmailov et al. | *UAI*, 2018 | Averaging checkpoints → flatter minima, better generalization. |\n",
        "| **SAM (Sharpness-Aware Minimization)** | Foret et al. | *ICLR*, 2021 | Optimized for flatness by minimizing neighborhood worst-case loss. |\n",
        "| **Lookahead Optimizer** | Zhang et al. | *NeurIPS*, 2019 | Smoothed optimization trajectory via fast/slow weight coupling. |\n",
        "| **Polyak Averaging** | Polyak & Juditsky | *SIAM JCO*, 1992 | Averaged iterates to reduce variance and stabilize convergence. |\n",
        "\n",
        "---\n",
        "\n",
        "## I) Knowledge Distillation & Related\n",
        "\n",
        "| **Method** | **Reference** | **Venue / Year** | **Contribution** |\n",
        "|-------------|----------------|------------------|------------------|\n",
        "| **Knowledge Distillation** | Hinton, Vinyals & Dean | *NeurIPS Workshop*, 2015 | Transferred teacher soft predictions to student networks. |\n",
        "| **Born-Again Networks** | Furlanello et al. | *ICML*, 2018 | Iterative self-distillation — student learns from its predecessor. |\n",
        "| **Mixout** | Lee et al. | *ICLR*, 2020 | Regularized fine-tuning by interpolating with pretrained weights. |\n",
        "\n",
        "---\n",
        "\n",
        "## J) Bayesian Views & PAC-Bayes\n",
        "\n",
        "| **Concept** | **Reference** | **Venue / Year** | **Key Contribution** |\n",
        "|--------------|----------------|------------------|----------------------|\n",
        "| **Bayesian Neural Networks (Foundations)** | MacKay (1992); Neal (1995) | *PhD Theses / Monographs* | Introduced Bayesian inference for neural weights. |\n",
        "| **Variational Bayesian NNs** | Graves (2011); Blundell et al. (2015) | *ICML* | Variational inference for uncertainty estimation. |\n",
        "| **Dropout as Bayesian Approximation** | Gal & Ghahramani | *ICML*, 2016 | Showed dropout approximates Bayesian inference. |\n",
        "| **PAC-Bayes Bounds** | McAllester | *COLT*, 1999 | Theoretical generalization framework for randomized predictors. |\n",
        "| **Non-Vacuous Deep PAC-Bayes Bounds** | Dziugaite & Roy | *UAI*, 2017 | Demonstrated meaningful generalization bounds for deep nets. |\n",
        "\n",
        "---\n",
        "\n",
        "## K) Pruning, Sparsity & Compression (as Regularization)\n",
        "\n",
        "| **Method** | **Reference** | **Venue / Year** | **Contribution** |\n",
        "|-------------|----------------|------------------|------------------|\n",
        "| **Optimal Brain Damage** | LeCun, Denker & Solla | *NeurIPS*, 1990 | Hessian-based pruning removing unimportant weights. |\n",
        "| **Optimal Brain Surgeon** | Hassibi & Stork | *NeurIPS*, 1993 | Second-order pruning preserving network accuracy. |\n",
        "| **Magnitude Pruning at Scale** | Han et al. | *NIPS / ICLR (arXiv)*, 2015 | Large-scale pruning for model compression. |\n",
        "| **Lottery Ticket Hypothesis** | Frankle & Carbin | *ICLR*, 2019 | Sparse subnetworks can match full-network performance. |\n",
        "| **Quantization-Aware Training** | Jacob et al. | *CVPR*, 2018 | Integrated quantization into training for low-bit inference. |\n",
        "| **Distillation for Compression** | Buciluă et al. | *KDD*, 2006 | Early demonstration of teacher-student compression. |\n",
        "\n",
        "---\n",
        "\n",
        "## L) Robustness, Lipschitz & Geometry\n",
        "\n",
        "| **Method / Concept** | **Reference** | **Venue / Year** | **Idea** |\n",
        "|------------------------|----------------|------------------|----------|\n",
        "| **Lipschitz Networks / Spectral Control** | Cisse et al. | *ICML*, 2017 | Enforced Lipschitz constraints via spectral control. |\n",
        "| **Parseval Networks (Orthogonality)** | Brock et al. | *ICLR*, 2017 | Encouraged orthogonal weight matrices for stability. |\n",
        "| **Spectral Normalization** | Miyato et al. | *ICLR*, 2018 | Constrained spectral norm for controlled Lipschitz continuity. |\n",
        "| **Certified Robustness via Regularization** | Ross & Doshi-Velez | *AAAI*, 2018 | Linked input-gradient norms to adversarial robustness. |\n",
        "| **Sensitivity & Generalization (Jacobian Norm Link)** | Novak et al. | *ICLR*, 2018 | Connected input sensitivity to generalization error. |\n",
        "\n",
        "---\n",
        "\n",
        "## M) Domain-Specific Regularization\n",
        "\n",
        "| **Domain** | **Technique** | **Reference** | **Venue / Year** |\n",
        "|-------------|----------------|----------------|------------------|\n",
        "| **Vision** | Label-Preserving Transformations | Simard et al. | *ICANN*, 1998 |\n",
        "| **Speech / Audio** | SpecAugment | Park et al. | *Interspeech*, 2019 |\n",
        "| **NLP** | Token Masking (BERT) | Devlin et al. | *NAACL*, 2019 |\n",
        "| **Transformers** | DropHead / Head Pruning | Michel et al. | *NeurIPS*, 2019 |\n",
        "\n",
        "---\n",
        "\n",
        "## N) Calibration & Loss-Side Regularization\n",
        "\n",
        "| **Method / Study** | **Reference** | **Venue / Year** | **Focus** |\n",
        "|---------------------|----------------|------------------|-----------|\n",
        "| **On Calibration of Modern Neural Networks** | Guo et al. | *ICML*, 2017 | Demonstrated deep nets’ miscalibration; inspired label smoothing. |\n",
        "| **Focal Loss** | Lin et al. | *ICCV*, 2017 | Reweighted hard examples for imbalanced data. |\n",
        "| **Label Smoothing Effects** | Müller et al. | *NeurIPS*, 2019 | Empirical calibration–accuracy tradeoff. |\n",
        "\n",
        "---\n",
        "\n",
        "## O) Theory of Capacity Control & Generalization\n",
        "\n",
        "| **Concept** | **Reference** | **Venue / Year** | **Core Contribution** |\n",
        "|--------------|----------------|------------------|-----------------------|\n",
        "| **Rademacher Complexity & ℓ₁/ℓ₂ Control** | Bartlett & Mendelson | *COLT*, 2002 | Theoretical link between norm constraints and generalization bounds. |\n",
        "| **Margins & Generalization** | Neyshabur et al. | *NeurIPS*, 2018 | Connected margin-based theory to deep learning generalization. |\n",
        "| **Compression–Generalization Link** | Arora et al. | *ICLR*, 2018 | Explained generalization via model compressibility. |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "LXMnKAf4GcUI"
      }
    }
  ]
}