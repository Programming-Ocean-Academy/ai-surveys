{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMP0d+wqjxVGToborLR3jRf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ“œ Optimization Techniques in AI, ML, and Deep Learning\n","\n","---\n","\n","## ðŸ”¹ 1. What is Optimization?\n","\n","Optimization in AI/ML refers to the process of **minimizing or maximizing an objective (loss) function** to improve a modelâ€™s performance.\n","\n","- In **Machine Learning**, optimization ensures the algorithm generalizes well to unseen data.  \n","- In **Deep Learning**, optimization is the heart of training neural networks, where millions (or billions) of parameters must be tuned efficiently.  \n","\n","**General formulation:**\n","\n","$$\n","\\theta^* = \\arg\\min_{\\theta} \\; L(f(x;\\theta), y)\n","$$\n","\n","---\n","\n","## ðŸ”¹ 2. Classical Optimization in ML\n","\n","- **Gradient Descent (GD):**\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\eta \\, \\nabla_\\theta L(f(x;\\theta_t), y)\n","$$\n","\n","- **Momentum (Polyak, 1983):**\n","\n","$$\n","v_{t+1} = \\mu v_t - \\eta \\nabla_\\theta L_t\n","$$\n","\n","$$\n","\\theta_{t+1} = \\theta_t + v_{t+1}\n","$$\n","\n","- **Nesterov Accelerated Gradient (NAG):**\n","\n","$$\n","v_{t+1} = \\mu v_t - \\eta \\nabla_\\theta L(\\theta_t + \\mu v_t)\n","$$\n","\n","$$\n","\\theta_{t+1} = \\theta_t + v_{t+1}\n","$$\n","\n","---\n","\n","## ðŸ”¹ 3. Advanced Optimizers in Deep Learning\n","\n","- **Adagrad (2011):**\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\nabla_\\theta L_t\n","$$\n","\n","- **RMSProp (2012):**\n","\n","$$\n","E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma) g_t^2\n","$$\n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_t\n","$$\n","\n","- **Adam (2014):**\n","\n","$$\n","m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n","$$  \n","\n","$$\n","v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n","$$  \n","\n","$$\n","\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad\n","\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\n","$$  \n","\n","$$\n","\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t}+\\epsilon}\n","$$\n","\n","---\n","\n","## ðŸ”¹ 4. Optimization Tricks in DL Training\n","\n","- **Learning Rate Scheduling (cosine annealing):**\n","\n","$$\n","\\eta_t = \\eta_0 \\cdot \\cos\\!\\left(\\frac{\\pi t}{T}\\right)\n","$$\n","\n","- **Gradient Clipping:**\n","\n","$$\n","g_t \\leftarrow \\frac{g_t}{\\max(1, \\|g_t\\|/c)}\n","$$\n","\n","- **Weight Initialization:**\n","\n","$$\n","\\text{Xavier: } \\; \\mathcal{U}\\!\\left[-\\frac{\\sqrt{6}}{\\sqrt{n_{in}+n_{out}}}, \\; \\frac{\\sqrt{6}}{\\sqrt{n_{in}+n_{out}}}\\right]\n","$$  \n","\n","$$\n","\\text{He: } \\; \\mathcal{N}(0, \\tfrac{2}{n_{in}})\n","$$\n","\n","- **Sharpness-Aware Minimization (SAM, 2021):**\n","\n","$$\n","\\min_\\theta \\; \\max_{\\|\\epsilon\\|\\leq\\rho} L(f(x;\\theta+\\epsilon), y)\n","$$\n","\n","---\n","\n","## ðŸ”¹ 5. Optimization Challenges in Deep Learning\n","\n","- **Vanishing/Exploding Gradients:**\n","\n","$$\n","\\prod_{t=1}^T W_t \\quad \\to \\quad 0 \\;\\; \\text{or} \\;\\; \\infty\n","$$\n","\n","---\n","\n","## âœ… Key Takeaways\n","\n","- **ML era:** convex optimization â†’ Gradient Descent, SVMs.  \n","- **DL era:** adaptive optimizers (Adam, RMSProp) + training tricks (LR schedules, normalization).  \n","- **Modern era (2020s):** scaling to **billions of params**, robustness (flat minima, adversarial), and efficiency (distributed & optimizer-free methods).  \n"],"metadata":{"id":"8BBDF2Z0RXD9"}},{"cell_type":"markdown","source":["# ðŸ“Š Comparison of Optimization Algorithms in Deep Learning\n","\n","| Optimizer | Update Rule (simplified) | Pros | Cons | Typical Use Cases |\n","|-----------|--------------------------|------|------|-------------------|\n","| **SGD (Stochastic Gradient Descent)** | $$\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t)$$ | Simple, memory efficient, good generalization | Slow convergence, sensitive to learning rate | Small to medium models, baseline training |\n","| **Momentum** | $$v_t = \\beta v_{t-1} + (1-\\beta)\\nabla L, \\quad \\theta_{t+1} = \\theta_t - \\eta v_t$$ | Faster convergence, helps escape shallow minima | Can overshoot, extra hyperparameter ($$\\beta$$) | CNN training, image recognition |\n","| **NAG (Nesterov Accelerated Gradient)** | $$v_t = \\beta v_{t-1} + \\nabla L(\\theta - \\eta \\beta v_{t-1})$$ | Lookahead improves stability, faster than Momentum | Slightly more complex, tuning needed | Sequence models, RNNs |\n","| **Adagrad** | $$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\nabla L$$ | Adaptive learning rate per parameter, good for sparse features | Learning rate decays too fast | NLP, text embeddings |\n","| **RMSProp** | $$E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta) g_t^2, \\quad \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_t$$ | Controls Adagradâ€™s decay issue, stable | Still sensitive to hyperparams | RNNs, speech recognition |\n","| **Adam (Adaptive Moment Estimation)** | Combines Momentum + RMSProp with bias correction | Fast, robust, widely used, less tuning needed | Can overfit, sometimes worse generalization than SGD | Standard for most DL tasks (NLP, CV, GANs) |\n","| **AdamW** | Adam + decoupled weight decay | Better generalization than Adam | Still requires careful LR tuning | Transformers, LLMs |\n","| **LAMB (Layer-wise Adaptive Moments)** | Adam variant with layer-wise normalization | Enables training of very large models (BERT, GPT) | More complex, heavier compute | Large-scale models, foundation models |\n","| **SAM (Sharpness-Aware Minimization, 2021)** | $$\\min_\\theta \\; \\max_{\\|\\epsilon\\|\\leq\\rho} L(f(x;\\theta+\\epsilon), y)$$ | Improves robustness, better generalization | Slower, higher compute cost | Vision Transformers, LLM fine-tuning |\n","\n","---\n","\n","## âœ… Key Insights\n","- **Classical baseline**: SGD (+Momentum, NAG).  \n","- **Adaptive methods**: Adagrad â†’ RMSProp â†’ Adam (standard for DL).  \n","- **Modern large-scale**: AdamW, LAMB, SAM for foundation models.  \n","- **Trade-off**: SGD often generalizes better, while Adam converges faster.  \n"],"metadata":{"id":"HZxSEoBpSeID"}}]}