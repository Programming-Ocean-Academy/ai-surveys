{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Boltzmann, Restricted Boltzmann, and Deep Belief Networks\n",
        "\n",
        "## 1. Boltzmann Machines (BMs, Hinton & Sejnowski, 1985)\n",
        "- **Structure**: Fully connected, undirected graph of visible and hidden units.  \n",
        "- **Training**: Very hard to train because every unit can connect to every other unit (complex dependencies).  \n",
        "- **Nature**: Stochastic, energy-based, generative model.  \n",
        "- **Problem**: Inference is computationally expensive (requires slow Gibbs sampling).  \n",
        "\n",
        "## 2. Restricted Boltzmann Machines (RBMs, Hinton, 1986; popularized in 2006)\n",
        "- **Key Restriction**: No intra-layer connections (visible units don’t connect to each other; hidden units don’t connect to each other).  \n",
        "- **Effect**: Bipartite structure makes inference much easier.  \n",
        "- **Training**: Efficient using Contrastive Divergence.  \n",
        "- **Role**: Became the practical building block for deep generative models.  \n",
        "- **Use Case**: Feature extraction, dimensionality reduction, collaborative filtering (Netflix Prize era).  \n",
        "\n",
        "## 3. Deep Belief Networks (DBNs, Hinton et al., 2006)\n",
        "- **Construction**: Stack multiple RBMs layer by layer.  \n",
        "- **Training Method**: Greedy layer-wise pretraining with RBMs, then fine-tuning with backpropagation.  \n",
        "- **Purpose**: First deep models that could be trained effectively before GPUs and better activation functions made backpropagation practical.  \n",
        "- **Impact**: Sparked the deep learning revival in the mid-2000s.  \n",
        "- **Use Case**: Early speech recognition, vision tasks, unsupervised pretraining.  \n",
        "\n",
        "---\n",
        "\n",
        "## Comparison Table\n",
        "\n",
        "| Model                  | Structure                        | Training Difficulty | Key Idea                         | Role in AI History                                |\n",
        "|------------------------|----------------------------------|---------------------|----------------------------------|--------------------------------------------------|\n",
        "| **Boltzmann Machine**  | Fully connected, stochastic      | Very hard           | Energy-based, generative network | Theoretical foundation, rarely practical         |\n",
        "| **Restricted BM**      | Bipartite (no intra-layer edges) | Easier              | Contrastive Divergence learning  | Practical generative model, feature learner      |\n",
        "| **Deep Belief Network**| Stack of RBMs                    | Moderate            | Layer-wise unsupervised pretrain | Enabled training of deep nets, revived AI field  |\n",
        "\n",
        "---\n",
        "\n",
        "## In One Line\n",
        "- **BM** = general, but too hard to train.  \n",
        "- **RBM** = simplified, practical building block.  \n",
        "- **DBN** = stacked RBMs → first practical deep architecture.  \n"
      ],
      "metadata": {
        "id": "MJStcXBgZQiZ"
      }
    }
  ]
}