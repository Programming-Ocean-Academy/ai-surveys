{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Key Academic Works After McCulloch & Pitts (1943) (1943–2000)\n",
        "\n",
        "Below is a curated timeline of **landmark neural network and AI papers/models** between 1943 and 2000 that are both historically significant and can be reconstructed for modern study.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1940s–1950s Foundations\n",
        "\n",
        "| Year | Author(s) | Work | Contribution | Modern Implementation |\n",
        "|------|-----------|------|--------------|-----------------------|\n",
        "| 1949 | Donald Hebb | *The Organization of Behavior* | Hebbian learning rule: “neurons that fire together, wire together.” | Simple linear neurons with Hebbian update rule. |\n",
        "| 1958 | Frank Rosenblatt | *The Perceptron* | Introduced perceptron learning algorithm for classification. | Single-layer perceptron trained on logical gates. |\n",
        "\n",
        "---\n",
        "\n",
        "## 1960s Advances\n",
        "\n",
        "| Year | Author(s) | Work | Contribution | Modern Implementation |\n",
        "|------|-----------|------|--------------|-----------------------|\n",
        "| 1960 | Bernard Widrow & Ted Hoff | *ADALINE / MADALINE* | Introduced LMS rule and adaptive networks. | Linear neurons trained with gradient descent for regression. |\n",
        "| 1969 | Marvin Minsky & Seymour Papert | *Perceptrons* | Highlighted perceptron’s limits (e.g., XOR problem). | Demonstration of perceptron’s failure on XOR dataset. |\n",
        "\n",
        "---\n",
        "\n",
        "## 1970s Shift Toward Learning\n",
        "\n",
        "| Year | Author(s) | Work | Contribution | Modern Implementation |\n",
        "|------|-----------|------|--------------|-----------------------|\n",
        "| 1972 | Shun-Ichi Amari | *Learning theory of pattern recognition* | Early work on statistical learning in neural nets. | Gradient descent dynamics for linear separability. |\n",
        "| 1974 | Paul Werbos | *PhD thesis (Backpropagation)* | First explicit formulation of backpropagation. | Two-layer network with custom backpropagation. |\n",
        "| 1976 | James Anderson | *Associative memory models* | Introduced distributed memory representation. | Hopfield-like associative net. |\n",
        "\n",
        "---\n",
        "\n",
        "## 1980s Neural Network Revival\n",
        "\n",
        "| Year | Author(s) | Work | Contribution | Modern Implementation |\n",
        "|------|-----------|------|--------------|-----------------------|\n",
        "| 1982 | John Hopfield | *Neural networks and physical systems with emergent collective computational abilities* | Hopfield networks with energy minimization. | Hopfield network for recovering corrupted binary patterns. |\n",
        "| 1985 | Hinton & Sejnowski | *Boltzmann Machines* | Stochastic energy-based models. | Boltzmann Machine with Gibbs sampling. |\n",
        "| 1986 | Rumelhart, Hinton, Williams | *Learning representations by back-propagating errors* | Popularized backpropagation in MLPs. | MLP trained on XOR or MNIST with backpropagation. |\n",
        "| 1987 | Teuvo Kohonen | *Self-Organizing Maps (SOM)* | Competitive learning and topological mapping. | SOM for clustering tasks. |\n",
        "| 1989 | Yann LeCun et al. | *Backpropagation Applied to Handwritten Zip Code Recognition* | Early convolutional neural network (LeNet-1 precursor). | LeNet-style CNN trained on MNIST. |\n",
        "\n",
        "---\n",
        "\n",
        "## 1990s Deepening Architectures\n",
        "\n",
        "| Year | Author(s) | Work | Contribution | Modern Implementation |\n",
        "|------|-----------|------|--------------|-----------------------|\n",
        "| 1990 | Jeffrey Elman | *Finding structure in time* | Introduced **Elman recurrent neural network (RNN)**. | RNN for sequence prediction. |\n",
        "| 1992 | Sepp Hochreiter | *Untersuchungen zu dynamischen neuronalen Netzen* | Proposed the **LSTM** architecture to address vanishing gradients. | Early LSTM on sequence modeling tasks. |\n",
        "| 1995 | Christopher Bishop | *Neural Networks for Pattern Recognition* | Provided a statistical treatment of neural nets. | Regularization and Bayesian-style neural nets. |\n",
        "| 1997 | Hochreiter & Schmidhuber | *Long Short-Term Memory* | Full formulation of LSTM architecture. | LSTM implemented from scratch. |\n",
        "| 1998 | Yann LeCun et al. | *Gradient-Based Learning Applied to Document Recognition* | Introduced **LeNet-5** CNN for digit recognition. | LeNet-5 trained on MNIST. |\n",
        "| 1999 | Schölkopf, Smola, Müller | *Kernel PCA* | Extended kernel methods for dimensionality reduction. | Kernel PCA for nonlinear feature extraction. |\n",
        "\n",
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "Between **1943–2000**, the major milestones include:\n",
        "\n",
        "- **Binary/logical neuron models**: Hebb, Rosenblatt.  \n",
        "- **Associative networks**: Anderson, Hopfield.  \n",
        "- **Energy-based models**: Boltzmann Machines.  \n",
        "- **Learning algorithms**: Werbos, Rumelhart et al. (backpropagation).  \n",
        "- **Convolutional nets**: LeCun’s LeNet.  \n",
        "- **Recurrent nets**: Elman RNN, LSTM.  \n",
        "- **Unsupervised methods**: Kohonen’s SOM, Kernel PCA.  \n",
        "\n",
        "All of these works form the backbone of modern deep learning and can be reconstructed today for both teaching and research.\n"
      ],
      "metadata": {
        "id": "mMd5kqdO2oGo"
      }
    }
  ]
}