{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0LFJgP0BYHon2twPNImcG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ“œ Multimodal Deep Learning Breakthroughs\n","\n","---\n","\n","## ðŸ”¹ Early Fusion Approaches (2000sâ€“2010s)\n","\n","- Early multimodal learning combined **text + vision** (image captioning) or **speech + vision** (audio-visual speech recognition).  \n","- Approaches were typically **shallow models**:\n","  - Extract features separately from each modality.  \n","  - Concatenate features.  \n","  - Train a supervised classifier or predictor.  \n","\n","These methods demonstrated feasibility but lacked scalability and representation power.\n","\n","---\n","\n","## ðŸ”¹ Modern Multimodal Models\n","\n","### **CLIP (Contrastive Languageâ€“Image Pretraining)** â€“ Radford et al. (2021, OpenAI)  \n","*\"Learning Transferable Visual Models From Natural Language Supervision.\"*  \n","- Trained on **400M imageâ€“text pairs** with contrastive learning.  \n","- Achieved **zero-shot transfer** on diverse vision tasks.  \n","- Pioneered **vision-language pretraining**.  \n","\n","---\n","\n","### **DALLÂ·E (Text-to-Image Generation)** â€“ Ramesh et al. (2021, OpenAI)  \n","*\"Zero-Shot Text-to-Image Generation.\"*  \n","- Combined **autoregressive Transformers + VQ-VAE**.  \n","- First large-scale **text â†’ image generative model**.  \n","- Inspired follow-ups (Imagen, Stable Diffusion).  \n","\n","---\n","\n","### **ALIGN** â€“ Jia et al. (2021, Google)  \n","*\"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision.\"*  \n","- Googleâ€™s **CLIP-like model**, but trained on **massive noisy imageâ€“text pairs**.  \n","- Demonstrated scaling laws in multimodal contrastive learning.  \n","\n","---\n","\n","### **PaLI (Pathways Language and Image)** â€“ Google Research (2022)  \n","- Unified **multimodal Transformer** for **text + image** tasks.  \n","- Applied to OCR, captioning, VQA, multilingual multimodal tasks.  \n","\n","---\n","\n","### **PaLM-E (Embodied Multimodal LLM)** â€“ Google Research (2023)  \n","*\"PaLM-E: An Embodied Multimodal Language Model.\"*  \n","- Extended **PaLM** with **robotics input** (vision + sensor data).  \n","- Moves beyond perception to **embodied reasoning and action**.  \n","- First step towards **general-purpose embodied AI agents**.  \n","\n","---\n","\n","## ðŸ”¹ Key Modalities Combined\n","\n","- **Text + Vision:** CLIP, ALIGN, PaLI.  \n","- **Text + Vision (Generative):** DALLÂ·E, Imagen (Google, 2022), Stable Diffusion (2022).  \n","- **Audio + Text:** Wav2Vec 2.0 (Facebook, 2020), Whisper (OpenAI, 2022).  \n","- **Vision + Language + Robotics:** PaLM-E (2023).  \n","- **General Multimodality (text, image, audio, video, sensor):** active research frontier (2023â€“2025).  \n","\n","---\n","\n","## âœ… Why It Matters\n","\n","- Multimodal deep learning **bridges perception and reasoning**:  \n","  - Vision (seeing), Speech (hearing), Language (understanding), Robotics (acting).  \n","- Foundation multimodal models (e.g., **CLIP, DALLÂ·E, PaLM-E**) enable:  \n","  - **Zero-shot transfer.**  \n","  - **Generative AI across modalities.**  \n","  - **Embodied AI agents.**  \n","- They pave the way for **general-purpose AI** capable of understanding and interacting across multiple input/output modalities.  \n","\n","---\n"],"metadata":{"id":"tUpg5vVgJc6p"}}]}