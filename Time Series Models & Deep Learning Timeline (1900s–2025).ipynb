{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSQoMbviMmqGOjUM5lkvQu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ“œ Time Series Models & Deep Learning Timeline (1900sâ€“2025)\n","\n","---\n","\n","## ðŸ“š Key Milestones\n","\n","| **Era** | **Model / Concept** | **Year** | **Authors / Org** | **Key Contributions** |\n","|---------|----------------------|----------|-------------------|-----------------------|\n","| **Statistical Foundations** | **Autoregressive (AR) Models** | 1927 | Yule | First formal AR process for time series modeling. |\n","| | **Moving Average (MA) Models** | 1937 | Slutsky | Linear filters to smooth randomness in time series. |\n","| | **ARMA Models** | 1938 / 1970s | Wold; Box & Jenkins | Unified AR + MA processes â†’ classical time series foundation. |\n","| | **ARIMA Models** | 1970 | Box & Jenkins | Added differencing to handle non-stationarity. |\n","| **State-Space & Probabilistic** | **Kalman Filter** | 1960 | Kalman | Recursive Bayesian estimation for dynamic systems. |\n","| | **Hidden Markov Models (HMMs)** | 1966 | Baum & Petrie | Probabilistic sequence modeling; dominated speech recognition for decades. |\n","| **Neural Network Era** | **Time Delay Neural Networks (TDNNs)** | 1989 | Waibel et al. (CMU) | First NN architecture designed for sequential/time-series input. |\n","| | **Recurrent Neural Networks (RNNs)** | 1990 | Elman | Introduced context layers for temporal dependencies. |\n","| | **LSTM (Long Short-Term Memory)** | 1997 | Hochreiter & Schmidhuber | Solved vanishing gradient problem; enabled long-sequence learning. |\n","| **Deep Learning for Time Series** | **Sequence-to-Sequence (Seq2Seq)** | 2014 | Sutskever, Vinyals & Le (Google) | Encoderâ€“decoder RNNs for sequential prediction; breakthrough in MT & forecasting. |\n","| | **Attention Mechanism** | 2014 | Bahdanau et al. | Added alignment â†’ boosted sequence modeling accuracy. |\n","| | **Temporal Convolutional Networks (TCN)** | 2018 | Bai, Kolter & Koltun | Dilated CNNs outperform RNNs in time series tasks. |\n","| **Transformer Era** | **Transformer** | 2017 | Vaswani et al. (Google Brain) | Self-attention â†’ revolutionized sequence modeling (basis for modern time series). |\n","| | **Informer** | 2021 | Zhou et al. | Efficient Transformer for long-horizon time series forecasting. |\n","| | **Temporal Fusion Transformer (TFT)** | 2021 | Lim et al. | Multivariate forecasting with attention + interpretability. |\n","| | **TimesNet** | 2023 | Wu et al. | General-purpose deep time series model; competitive with Transformers. |\n","\n","---\n","\n","## âœ… Summary Families\n","- **Statistical models:** AR (1927), ARIMA (1970), Kalman Filter (1960), HMM (1966).  \n","- **Neural networks:** TDNN (1989), RNN (1990), LSTM (1997).  \n","- **Deep learning:** Seq2Seq (2014), Attention (2014), TCN (2018).  \n","- **Transformer-based:** Informer (2021), TFT (2021), TimesNet (2023).  \n"],"metadata":{"id":"IVNW7Dzk8gqk"}}]}