{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPKGnJHXOTCnhCeebJbgYVl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 📜 Regression Losses in AI/ML/DL\n","\n","---\n","\n","## 1) Core point-estimate losses (predict a single number)\n","\n","| Loss                           | Formula (ŷ = prediction)                            | When to use                               | Pros                            | Cons                               |\n","| ------------------------------ | --------------------------------------------------- | ----------------------------------------- | ------------------------------- | ---------------------------------- |\n","| **MSE / L2**                   | $$\\frac{1}{n}\\sum (y-\\hat{y})^2$$                   | Gaussian noise, penalize large errors     | Convex, smooth, easy            | Outlier-sensitive, scale-dependent |\n","| **RMSE**                       | $$\\sqrt{\\text{MSE}}$$                               | Same as MSE but interpretable units       | Easy to interpret               | Same issues as MSE                 |\n","| **MAE / L1**                   | $$\\frac{1}{n}\\sum |y-\\hat{y}|$$                     | Laplace noise, outliers present           | Robust to outliers              | Non-smooth at 0, slower to optimize |\n","| **Huber**                      | $$\\phi_\\delta(r)=\\begin{cases}\\tfrac12 r^2,& |r|\\le\\delta \\\\ \\delta(|r|-\\tfrac12\\delta),& \\text{else}\\end{cases}$$ | Mix of L2 (small r) & L1 (large r) | Robust + smooth | Need to choose $$\\delta$$ |\n","| **Pseudo-Huber / Charbonnier** | $$\\delta^2(\\sqrt{1+(r/\\delta)^2}-1)\\quad \\text{or}\\quad \\sqrt{r^2+\\epsilon^2}$$ | Smooth robust alternative                 | Differentiable everywhere       | Hyperparam sensitivity             |\n","| **Log-Cosh**                   | $$\\sum \\log \\cosh(r)$$                              | Gentle robust loss                        | Smooth, easy                    | Slightly costlier than MSE         |\n","| **RMSLE**                      | $$\\sqrt{\\tfrac1n\\sum(\\log(1+y)-\\log(1+\\hat{y}))^2}$$| Positive targets, relative errors matter  | Penalizes under-prediction less | Undefined if $$y<0$$               |\n","\n","> Residual: $$r = y - \\hat{y}$$.  \n","> Scaling targets often improves convergence.\n","\n","---\n","\n","## 2) Asymmetric losses (quantiles, tails, risk)\n","\n","| Loss                   | Formula                                            | Use case                                     | Notes                                    |\n","| ---------------------- | -------------------------------------------------- | -------------------------------------------- | ---------------------------------------- |\n","| **Quantile / Pinball** | $$L_\\tau(r)=\\max(\\tau r,( \\tau-1) r)$$             | Predict $$\\tau$$-quantiles (e.g., P90 latency) | Robust, handles asymmetry                |\n","| **Expectile**          | $$L_\\tau(r) = (\\tau - \\mathbf{1}_{r<0}) r^2$$      | Risk-sensitive regression                     | Squared variant of quantiles             |\n","| **Tilted-Huber**       | Huber on sign-weighted residual                    | Smooth quantile loss                          | More stable than raw quantile loss       |\n","\n","---\n","\n","## 3) Robust M-estimators (heavy tails, outliers)\n","\n","| Loss                 | Formula (ρ(r))                                            | When / Notes                       |\n","| -------------------- | --------------------------------------------------------- | ---------------------------------- |\n","| **Tukey (bisquare)** | $$\\tfrac{c^2}{6}\\left[1-(1-(r/c)^2)^3\\right],\\; |r|\\le c;\\;\\text{else const}$$       | Strong outlier rejection            |\n","| **Cauchy**           | $$\\log(1+(r/c)^2)$$                                       | Heavy-tailed noise                 |\n","| **Geman–McClure**    | $$\\tfrac{r^2}{r^2+c^2}$$                                  | Vision, feature matching           |\n","| **Welsch / Leclerc** | $$1-\\exp(-(r/c)^2)$$                                      | Smooth robust alternative          |\n","| **Fair**             | $$c^2\\Big(\\tfrac{|r|}{c}-\\log(1+|r|/c)\\Big)$$             | Gentle robust loss                 |\n","\n","> Scale $$c$$ often chosen from MAD (median absolute deviation).\n","\n","---\n","\n","## 4) Distributional / probabilistic regression\n","\n","Instead of point ŷ, predict parameters of $$p(y|x;\\theta)$$ and minimize **negative log-likelihood (NLL)**.\n","\n","| Family                        | NLL (per sample)                                       | Predict              | When / Notes                        |\n","| ----------------------------- | ------------------------------------------------------ | -------------------- | ----------------------------------- |\n","| **Gaussian (homosced.)**      | $$\\tfrac{(y-\\mu)^2}{2\\sigma^2}+\\tfrac12\\log 2\\pi\\sigma^2$$ (σ fixed) | μ | Classic; reduces to MSE if σ const |\n","| **Gaussian (heterosced.)**    | $$\\tfrac{(y-\\mu)^2}{2\\sigma^2}+\\tfrac12\\log\\sigma^2$$  | μ, log σ²           | Learn aleatoric uncertainty          |\n","| **Laplace**                   | $$\\tfrac{|y-\\mu|}{b} + \\log(2b)$$                      | μ, log b             | Robust (L1-like)                     |\n","| **Student-t**                 | $$\\log\\!\\Big(1+\\tfrac{(y-\\mu)^2}{\\nu s^2}\\Big)$$ + const | μ, log s, ν        | Heavy-tailed noise                   |\n","| **Mixture Density Net (MDN)** | $$-\\log\\sum_k \\pi_k \\,\\mathcal{N}(y|\\mu_k,\\sigma_k^2)$$| mixture params       | Multi-modal targets                  |\n","| **Poisson**                   | $$\\lambda - y\\log\\lambda$$                             | λ>0                  | Count data                           |\n","| **Neg. Binomial**             | $$-\\log \\text{NB}(y|r,p)$$                             | mean, dispersion     | Over-dispersed counts                |\n","| **Gamma**                     | $$-\\log \\text{Gamma}(y|k,\\theta)$$                     | k, θ                 | Positive skewed targets              |\n","| **Tweedie**                   | Tweedie deviance                                       | mean, p              | Zero-inflated positives              |\n","\n","> For vector targets $$y\\in \\mathbb{R}^d$$, use multivariate Gaussian:  \n","> $$\\tfrac12[(y-\\mu)^T\\Sigma^{-1}(y-\\mu)+\\log|\\Sigma|]$$.\n","\n","---\n","\n","## 5) Forecasting & business losses\n","\n","| Loss            | Formula                                       | Caveats                      | Use cases                         |\n","| --------------- | --------------------------------------------- | ---------------------------- | --------------------------------- |\n","| **MAPE**        | $$\\frac1n\\sum \\left|\\frac{y-\\hat{y}}{y}\\right|$$ | Division by 0, bias to small y | Demand forecasting, KPIs          |\n","| **sMAPE**       | $$\\frac{2}{n}\\sum \\frac{|y-\\hat{y}|}{|y|+|\\hat{y}|}$$ | Non-convex, tricky optimization | Seasonality, business forecasting |\n","| **MASE/RMSSE**  | Error normalized by naive/seasonal baseline    | Needs baseline series        | Forecast competitions (M4, M5)    |\n","| **CRPS**        | $$\\int (F(y)-\\mathbf{1}\\{t\\le y\\})^2 \\,dt$$    | Needs forecast distribution  | Probabilistic forecasts           |\n","| **Energy Score**| Distance between forecast distribution & obs. | Sampling required            | Multivariate forecasting          |\n","\n","---\n","\n","## 6) Geometry & domain-specific regression losses\n","\n","| Domain               | Loss formula                                               | Notes                          |\n","| -------------------- | ---------------------------------------------------------- | ------------------------------ |\n","| **Directions**       | $$1-\\frac{y\\cdot \\hat{y}}{\\|y\\|\\|\\hat{y}\\|}$$              | Orientation regression         |\n","| **Rotations (SO(3))**| Geodesic angle (e.g., quaternion log-map)                  | Pose estimation                |\n","| **Geographic**       | Haversine distance                                         | Lat/Lon regression             |\n","| **Images / Signals** | SSIM/MS-SSIM, perceptual (VGG), total variation            | Better perceptual fidelity     |\n","| **Speech/Audio**     | Log-mel MSE, spectral convergence                          | Perceptual audio quality       |\n","\n","---\n","\n","## 7) Multi-task & masking\n","\n","- **Masked losses:**  \n","  $$\\frac{\\sum m_i \\ell_i}{\\sum m_i}$$ where $$m_i$$ masks missing labels.  \n","- **Task balancing (uncertainty weighting):**  \n","  $$\\sum \\tfrac{1}{2\\sigma_t^2}\\mathcal{L}_t + \\tfrac12 \\log \\sigma_t^2$$.  \n","- **Curriculum/dynamic reweighting:** GradNorm, homoscedastic/heteroscedastic methods.  \n","\n","---\n","\n","## 8) Practical guidance\n","\n","- Gaussian noise → **MSE/RMSE**.  \n","- Outliers / heavy tails → **MAE, Huber, Student-t NLL**.  \n","- Asymmetric costs (P90, P95) → **Quantile (Pinball) Loss**.  \n","- Count data → **Poisson / NegBin**.  \n","- Positive skewed data → **Gamma / RMSLE**.  \n","- Multi-modal targets → **MDN**.  \n","- Need uncertainty → **Heteroscedastic Gaussian NLL, CRPS**.  \n","- Perceptual tasks (CV, audio) → **L1 + SSIM/Perceptual**.  \n","- Forecasting → **MASE, RMSSE, Quantile Loss**.  \n","\n","---\n","\n","## 9) Optimization & stability tips\n","\n","- **Scale targets** (standardize/log) → smoother optimization.  \n","- **Clamp predicted variances** to avoid NaNs.  \n","- **Warm-up strategy:** start with MAE, then switch to MSE.  \n","- **Handle imbalance:** reweight samples.  \n","- **Evaluation ≠ training:** train with MSE, report MAE/quantile/RMSLE.  \n","\n","---\n","\n","## 🔑 Quick reference formulas\n","\n","- MSE: $$\\tfrac1n\\sum (y-\\hat{y})^2$$  \n","- MAE: $$\\tfrac1n\\sum |y-\\hat{y}|$$  \n","- Huber: piecewise quadratic/linear at threshold δ  \n","- Quantile: $$L_\\tau = \\max(\\tau(y-\\hat{y}), (\\tau-1)(y-\\hat{y}))$$  \n","- Gaussian NLL: $$\\tfrac{(y-\\mu)^2}{2\\sigma^2}+\\tfrac12\\log\\sigma^2$$  \n","- Poisson NLL: $$\\lambda - y\\log\\lambda$$  \n","- CRPS: $$\\mathbb{E}|Y-Y'| - \\tfrac12\\mathbb{E}|Y-Y''|$$  \n","\n","---\n"],"metadata":{"id":"PfurCPzTVtIU"}},{"cell_type":"markdown","source":["# 📌 Key Points on Regression Losses in AI/ML/DL\n","\n","---\n","\n","## 🔹 Core Losses\n","- **MSE / RMSE** → best for Gaussian noise, penalizes large errors heavily.  \n","- **MAE** → robust to outliers, but slower gradients (non-smooth).  \n","- **Huber / Charbonnier / Log-Cosh** → hybrids that balance robustness with smooth optimization.  \n","\n","---\n","\n","## 🔹 Asymmetric & Risk-Sensitive\n","- **Quantile Loss (Pinball)** → predict quantiles (e.g., P90, P95) → risk-aware forecasting.  \n","- **Expectile Loss** → squared variant of quantiles → emphasizes tail risks.  \n","\n","---\n","\n","## 🔹 Robust M-Estimators\n","- **Tukey, Cauchy, Welsch, Fair** → down-weight extreme outliers.  \n","- Useful in **vision, sensor data, and noisy environments**.  \n","\n","---\n","\n","## 🔹 Probabilistic Losses\n","- **Gaussian NLL** → predict mean $$\\mu$$ and variance $$\\sigma^2$$.  \n","- **Laplace / Student-t NLL** → handle heavier tails.  \n","- **Mixture Density Nets (MDN)** → capture **multi-modal targets**.  \n","- **Poisson / Neg. Binomial / Gamma / Tweedie** → for **count or skewed positive data**.  \n","\n","---\n","\n","## 🔹 Domain-Specific & Business Metrics\n","- **MAPE / sMAPE** → business KPIs (demand, finance), but unstable if $$y\\approx 0$$.  \n","- **MASE / RMSSE / CRPS / Energy Score** → forecasting & probabilistic benchmarks.  \n","- **Geodesic / Cosine / Perceptual Losses** → geometry (pose, angles), images, and speech/audio quality.  \n","\n","---\n","\n","## 🔹 Practical Training Tips\n","- **Scale targets** (normalize, log-transform skewed data).  \n","- **Clamp predicted variances** when learning uncertainty to avoid NaNs.  \n","- **Warm-up strategy:** start with MAE, then switch to MSE for stability.  \n","- **Combine pixel-level + perceptual losses** for images/audio.  \n","- **Match training loss to evaluation metric** → e.g., use quantile loss if business cares about P95.  \n","\n","---\n"],"metadata":{"id":"XU2XviF9WYuK"}},{"cell_type":"markdown","source":["# 📊 Comparative Table of Regression Loss Functions in AI/ML/DL\n","\n","---\n","\n","### 🔹 Core Losses\n","\n","| Loss Function                  | Formula (simplified)                           | Pros                                   | Cons                     | When to Use                                          |\n","| ------------------------------ | ---------------------------------------------- | -------------------------------------- | ------------------------ | ---------------------------------------------------- |\n","| **MSE (L2)**                   | $$\\frac{1}{n}\\sum (y-\\hat{y})^2$$              | Smooth, convex, penalizes large errors | Sensitive to outliers    | Standard regression, Gaussian noise, stable datasets |\n","| **RMSE**                       | $$\\sqrt{\\text{MSE}}$$                          | Interpretable in original units        | Same issues as MSE       | Reporting/benchmarking errors                        |\n","| **MAE (L1)**                   | $$\\frac{1}{n}\\sum |y-\\hat{y}|$$                | Robust to outliers                     | Non-smooth, slower conv. | Skewed/noisy data, outlier presence                  |\n","| **Huber**                      | Quadratic near 0, linear otherwise             | Mix of MSE & MAE                       | Needs $$\\delta$$ tuning  | Balanced case: some outliers but not dominant        |\n","| **Pseudo-Huber / Charbonnier** | $$\\sum \\delta^2\\left(\\sqrt{1+(r/\\delta)^2}-1\\right)$$ | Differentiable everywhere              | Hyperparameter tuning    | Vision, robotics, continuous control                 |\n","| **Log-Cosh**                   | $$\\sum \\log(\\cosh(y-\\hat{y}))$$                | Smooth, robust                         | Slightly slower than MSE | Robust regression with smooth gradients              |\n","| **RMSLE**                      | $$\\sqrt{\\tfrac{1}{n}\\sum(\\log(1+y)-\\log(1+\\hat{y}))^2}$$ | Handles exponential growth             | Undefined if $$y<0$$     | Finance, sales, demand forecasting                   |\n","\n","---\n","\n","### 🔹 Asymmetric / Risk-Sensitive\n","\n","| Loss                        | Pros                          | Cons                                | When to Use                                |\n","| --------------------------- | ----------------------------- | ----------------------------------- | ------------------------------------------ |\n","| **Quantile Loss (Pinball)** | Captures quantiles (P90, P95) | Needs multiple models for intervals | Forecasting risk, service-level guarantees |\n","| **Expectile Loss**          | Smooth quantile alternative   | Less common                         | Risk-sensitive finance, insurance          |\n","\n","---\n","\n","### 🔹 Robust M-Estimators\n","\n","| Loss                       | Pros                       | Cons                  | When to Use                      |\n","| -------------------------- | -------------------------- | --------------------- | -------------------------------- |\n","| **Tukey’s Biweight**       | Ignores extreme outliers   | Non-convex            | Computer vision, sensor data     |\n","| **Cauchy / Geman–McClure** | Heavy-tail robustness      | Can underfit extremes | Vision, medical data             |\n","| **Welsch / Fair**          | Smooth robust alternatives | Task-specific         | Image matching, depth regression |\n","\n","---\n","\n","### 🔹 Probabilistic Losses\n","\n","| Loss                               | Pros                          | Cons                  | When to Use                         |\n","| ---------------------------------- | ----------------------------- | --------------------- | ----------------------------------- |\n","| **Gaussian NLL**                   | Models uncertainty (variance) | Sensitive to σ errors | General regression with uncertainty |\n","| **Laplace NLL**                    | Robust to heavy tails         | Less smooth           | NLP & noisy targets                 |\n","| **Student-t NLL**                  | Handles very heavy tails      | Adds ν parameter      | Finance, extreme-value domains      |\n","| **Mixture Density Networks (MDN)** | Captures multimodality        | Expensive, unstable   | Multi-modal outputs (pose, speech)  |\n","| **Poisson NLL**                    | Natural for counts            | Only for integers     | Event counts, word frequencies      |\n","| **Negative Binomial**              | Models over-dispersion        | More parameters       | Epidemiology, finance               |\n","| **Gamma / Tweedie**                | For positive skewed data      | Domain-specific       | Energy, insurance, healthcare       |\n","\n","---\n","\n","### 🔹 Forecasting & Business\n","\n","| Loss             | Pros                               | Cons               | When to Use               |\n","| ---------------- | ---------------------------------- | ------------------ | ------------------------- |\n","| **MAPE**         | Scale-free, intuitive              | Division by zero   | Demand, KPIs              |\n","| **sMAPE**        | Better for seasonal series         | Non-convex         | Business forecasting      |\n","| **MASE / RMSSE** | Relative to naive baseline         | Needs baseline     | Competitions (M4/M5)      |\n","| **CRPS**         | Probabilistic, proper scoring rule | Needs distribution | Probabilistic forecasting |\n","| **Energy Score** | Multivariate version of CRPS       | Expensive          | Multivariate forecasts    |\n","\n","---\n","\n","### 🔹 Domain-Specific\n","\n","| Loss                       | Pros                      | Cons                 | When to Use                                     |\n","| -------------------------- | ------------------------- | -------------------- | ----------------------------------------------- |\n","| **Cosine Distance**        | Angle similarity          | No magnitude info    | Directional regression (NLP embeddings, vision) |\n","| **Geodesic Loss**          | Proper for rotations      | Domain-specific math | Robotics, pose estimation                       |\n","| **Haversine Loss**         | Handles Earth curvature   | Only for geodata     | Geospatial prediction                           |\n","| **SSIM / Perceptual Loss** | Better perceptual quality | Non-convex           | Images, audio, GANs                             |\n","| **Spectral Losses**        | Capture frequency         | Not universal        | Speech/audio regression                         |\n","\n","---\n","\n","✅ **Rule of Thumb**  \n","- **MSE/MAE/Huber** → general regression.  \n","- **Quantile/Expectile** → risk or service-level forecasting.  \n","- **Probabilistic NLL** → uncertainty & distributions.  \n","- **Domain-specific losses** → when geometry, perception, or human-judged quality matters.  \n"],"metadata":{"id":"RJVeYbAzWxdw"}}]}