{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Symbol Meanings in ASR Context\n",
        "\n",
        "## Key Acronyms\n",
        "- **ASR (Automatic Speech Recognition):** Task of converting spoken audio waveforms into written text.  \n",
        "- **HMM (Hidden Markov Model):** Statistical framework for sequence modeling. Speech is represented as hidden states with probabilistic transitions.  \n",
        "- **DNN (Deep Neural Network):** Learns complex, non-linear acoustic feature representations.  \n",
        "- **DNN-HMM:** Hybrid system where DNNs provide state likelihoods within the HMM framework, replacing traditional GMMs.  \n",
        "- **CTC (Connectionist Temporal Classification):** Training criterion that aligns variable-length speech inputs to output sequences (characters/phonemes) without frame-level alignment.  \n",
        "- **Deep Speech / Listen, Attend and Spell (LAS):** End-to-end architectures that directly map audio features to text, eliminating handcrafted components like lexicons and alignments.\n",
        "\n",
        "---\n",
        "\n",
        "## Two Main Approaches\n",
        "\n",
        "### 1. DNN-HMM Systems\n",
        "- **Pipeline:** Traditional ASR used HMMs with Gaussian Mixture Models (GMMs).  \n",
        "- **Innovation:** Replace GMMs with DNNs to improve acoustic modeling.  \n",
        "- **Function:** DNNs output posterior probabilities for HMM states, improving accuracy.  \n",
        "- **Limitation:** Still requires pronunciation dictionaries and language models, keeping the pipeline complex.\n",
        "\n",
        "### 2. CTC-Based Systems\n",
        "- **Pipeline:** Direct mapping from speech features → output sequences.  \n",
        "- **Strength:** Removes explicit frame-level alignment; the network learns when to output each symbol.  \n",
        "- **Typical Models:** RNNs, LSTMs, Transformers.  \n",
        "- **Advantage:** No handcrafted pronunciation dictionaries; architecture is simpler.  \n",
        "- **Limitation:** Requires large data and compute resources to reach top performance.\n",
        "\n",
        "---\n",
        "\n",
        "##  Big Picture\n",
        "- **DNN-HMM:** A modernization of the traditional ASR pipeline by incorporating neural acoustic models.  \n",
        "- **CTC:** A step toward **end-to-end ASR**, simplifying the architecture and enabling models like **Deep Speech** and **Listen, Attend and Spell**.  \n"
      ],
      "metadata": {
        "id": "32qRpGieGGTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Speech Recognition Terminology\n",
        "\n",
        "## 1. Core Foundations\n",
        "- **ASR (Automatic Speech Recognition):** Mapping spoken audio into written text.  \n",
        "- **Speech Signal:** Continuous waveform representing spoken language.  \n",
        "- **Frame:** Short segment of speech (≈25 ms) for acoustic analysis.  \n",
        "- **Feature Extraction:** Transform raw audio into compact representations (e.g., MFCC, spectrogram).  \n",
        "- **Phoneme:** Smallest unit of sound in a language.  \n",
        "- **WER (Word Error Rate):** \\((\\text{Substitutions} + \\text{Deletions} + \\text{Insertions}) \\div \\text{Total Words}\\).  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Acoustic Modeling\n",
        "- **Acoustic Model:** Relates audio features to phonetic units.  \n",
        "- **HMM (Hidden Markov Model):** Probabilistic sequence model (pre-deep learning era).  \n",
        "- **GMM (Gaussian Mixture Model):** Classical acoustic distribution model for HMMs.  \n",
        "- **DNN (Deep Neural Network):** Learns nonlinear acoustic mappings.  \n",
        "- **Hybrid DNN-HMM:** Combines DNN outputs with HMM sequence modeling.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Feature Representations\n",
        "- **MFCC (Mel-Frequency Cepstral Coefficients):** Handcrafted features inspired by human hearing.  \n",
        "- **PLP (Perceptual Linear Prediction):** Alternative perceptual feature method.  \n",
        "- **Spectrogram:** Time–frequency representation of speech.  \n",
        "- **Mel-Spectrogram:** Frequency-warped representation emphasizing perceptual ranges.  \n",
        "- **Filterbank Energies:** Summed spectral power in fixed frequency bands.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Sequence Learning & Training\n",
        "- **CTC (Connectionist Temporal Classification):** Loss function for alignment-free sequence training.  \n",
        "- **Seq2Seq:** Maps input speech frames to output text sequences.  \n",
        "- **Attention Mechanism:** Lets models focus on relevant frames during decoding.  \n",
        "- **LAS (Listen, Attend and Spell):** Seq2Seq + attention model for ASR.  \n",
        "- **RNN (Recurrent Neural Network):** Processes temporal data sequentially.  \n",
        "- **LSTM (Long Short-Term Memory):** RNN variant solving vanishing gradient issues.  \n",
        "- **GRU (Gated Recurrent Unit):** Simplified LSTM with fewer parameters.  \n",
        "- **Transformer:** Parallel sequence model using self-attention; modern ASR backbone.  \n",
        "- **Conformer:** Combines CNNs + Transformers for state-of-the-art speech models.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Decoding & Inference\n",
        "- **Decoder:** Maps model outputs to text sequences.  \n",
        "- **Beam Search:** Keeps multiple candidate hypotheses during decoding.  \n",
        "- **Greedy Decoding:** Picks the top token at each step without alternatives.  \n",
        "- **Language Model (LM):** Improves decoding with linguistic knowledge.  \n",
        "- **Pronunciation Dictionary:** Maps words to phonetic transcriptions (traditional ASR).  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. End-to-End & Modern Architectures\n",
        "- **Deep Speech:** End-to-end ASR with CTC (Baidu).  \n",
        "- **Wav2Vec / Wav2Vec 2.0:** Self-supervised learning from raw audio.  \n",
        "- **HuBERT:** Pretraining via clustering speech representations.  \n",
        "- **Whisper:** Transformer-based ASR (OpenAI).  \n",
        "- **Streaming ASR:** Real-time recognition with low latency.  \n",
        "- **Non-Autoregressive ASR:** Parallel sequence prediction instead of token-by-token.  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Evaluation Metrics\n",
        "- **WER (Word Error Rate):** Core accuracy measure.  \n",
        "- **CER (Character Error Rate):** Used for languages without word boundaries.  \n",
        "- **RTF (Real-Time Factor):** Decoding speed relative to audio length.  \n",
        "- **Latency:** Delay between input speech and transcription output.  \n",
        "\n",
        "---\n",
        "\n",
        "## 8. Advanced Mathematical Tools\n",
        "- **Viterbi Algorithm:** Dynamic programming for optimal decoding in HMMs.  \n",
        "- **Forward-Backward Algorithm:** Computes state sequence probabilities.  \n",
        "- **KL Divergence:** Loss function in probabilistic models.  \n",
        "- **Cross-Entropy Loss:** Standard classification objective.  \n",
        "- **Perplexity:** Uncertainty measure for language models.  \n",
        "\n",
        "---\n",
        "\n",
        "## 9. Real-World Deployment\n",
        "- **IVR (Interactive Voice Response):** Automated phone menus powered by ASR.  \n",
        "- **Dictation Systems:** Speech-to-text in domains like medical/legal.  \n",
        "- **Voice Assistants:** Siri, Alexa, Google Assistant → ASR + NLU.  \n",
        "- **Call Center Analytics:** Real-time ASR for customer insights.  \n",
        "- **Multilingual ASR:** Handling multiple languages and accents.  \n",
        "- **Low-Resource ASR:** Building systems for underrepresented languages.  \n",
        "\n",
        "---\n",
        "\n",
        "## 10. Future & Research Directions\n",
        "- **Self-Supervised Pretraining:** Learning from raw audio without labels.  \n",
        "- **Zero-Shot ASR:** Generalizing to unseen languages/tasks without retraining.  \n",
        "- **Code-Switching:** Recognizing speech mixing multiple languages.  \n",
        "- **Robust ASR:** Adapting to noise, accents, and spontaneous speech.  \n",
        "- **End-to-End Multimodal ASR:** Combining audio with visual cues (lip-reading, gestures).  \n"
      ],
      "metadata": {
        "id": "sYskLc07GL5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Out-of-Vocabulary (OOV) Words in NLP and ASR\n",
        "\n",
        "## Why It Matters\n",
        "- **Traditional Models:** Early ASR and statistical NLP systems had fixed vocabularies.  \n",
        "- **Problem:** Any unseen word (e.g., rare names, slang, foreign terms) was considered OOV.  \n",
        "- **Consequence:**  \n",
        "  - Replaced with `<UNK>` tokens.  \n",
        "  - Misrecognition or incorrect substitutions.  \n",
        "\n",
        "---\n",
        "\n",
        "## Approaches to Handle OOV Words\n",
        "\n",
        "### 1. Subword Modeling\n",
        "- Break words into smaller subunits (prefixes, suffixes, morphemes, syllables).  \n",
        "- **Techniques:** Byte Pair Encoding (BPE), WordPiece, SentencePiece.  \n",
        "- **Example:**  \n",
        "  - Word: *electroencephalogram*  \n",
        "  - Subword units: *electro + encephalo + gram*.  \n",
        "\n",
        "### 2. Character-Level Modeling\n",
        "- Treat each character as a modeling unit.  \n",
        "- **Advantage:** Any word can be generated.  \n",
        "- **Limitation:** Long dependencies are harder to capture.  \n",
        "\n",
        "### 3. Phoneme / Pronunciation Models (ASR)\n",
        "- Words decomposed into phonemes.  \n",
        "- **Example:** *Qatar* → decoded phonetically even if unseen in training.  \n",
        "- **Benefit:** Helps ASR handle new or rare spoken words.  \n",
        "\n",
        "### 4. Open-Vocabulary Language Models\n",
        "- Neural architectures (e.g., Transformers) trained at the subword or character level.  \n",
        "- **Effect:** Essentially eliminate OOV issues.  \n",
        "- **Examples:** GPT, BERT, Whisper.  \n",
        "\n",
        "### 5. Post-Processing / Placeholder Handling\n",
        "- Replace `<UNK>` with external resources or heuristics.  \n",
        "- **Examples:**  \n",
        "  - Use dictionaries or web search.  \n",
        "  - In machine translation: replace `<UNK>` with the aligned source word.  \n",
        "\n",
        "---\n",
        "\n",
        "## Example\n",
        "Input speech:  \n",
        "*\"I’m flying with AirAstana tomorrow.\"*\n",
        "\n",
        "- **Old ASR:** → \"I’m flying with `<UNK>` tomorrow.\"  \n",
        "- **Subword ASR:** → \"Air + A + stana\" → reconstructs correctly.  \n",
        "- **Character/Phoneme Model:** Decodes sound patterns to approximate *AirAstana*.  \n",
        "\n",
        "---\n",
        "\n",
        "##  In Short\n",
        "**Handling OOV words** ensures NLP and ASR systems recognize, generate, or approximate unseen words.  \n",
        "- **Subword modeling:** Break down words.  \n",
        "- **Character models:** Handle arbitrary text.  \n",
        "- **Phoneme models:** Decode unseen pronunciations.  \n",
        "- **Open-vocabulary LMs:** Naturally absorb new words.  \n",
        "- **Post-processing:** Replace `<UNK>` using context or resources.  \n",
        "\n",
        "This progression moved systems from **fixed-vocabulary brittleness** to **flexible open-vocabulary robustness**.\n"
      ],
      "metadata": {
        "id": "TraXvy3QGSmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Role of Gaussian Mixture Models (GMMs) in NLP and ASR\n",
        "\n",
        "## 1. What are GMMs?\n",
        "A **Gaussian Mixture Model (GMM)** represents a probability distribution as a weighted sum of multiple Gaussian components:\n",
        "\n",
        "$$\n",
        "p(x) = \\sum_{k=1}^K \\pi_k \\, \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)\n",
        "$$\n",
        "\n",
        "- \\( \\pi_k \\): mixture weight (probability of choosing component \\( k \\))  \n",
        "- \\( \\mu_k, \\Sigma_k \\): mean and covariance of Gaussian component \\( k \\)  \n",
        "\n",
        "**Interpretation:** Instead of modeling data with a single Gaussian, GMMs capture multimodality — complex distributions composed of several clusters.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Historical Role in NLP & ASR\n",
        "\n",
        "### Acoustic Modeling in ASR\n",
        "- **Pre-deep learning era:** GMMs modeled the distribution of **acoustic features** (MFCCs, filterbanks).  \n",
        "- Combined with HMMs → **GMM-HMM systems** formed the backbone of speech recognition until the 2010s.  \n",
        "\n",
        "### Clustering / Topic Modeling\n",
        "- Used to cluster **word embeddings, documents, or phoneme units**.  \n",
        "- Example: grouping similar-sounding phonemes or semantically similar words.  \n",
        "\n",
        "### Word Embeddings (Pre-Neural Era)\n",
        "- Early models represented **polysemy** with GMMs.  \n",
        "- Example: \"bank\" clustered into *financial* vs. *river* senses.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Transition to Deep Learning\n",
        "- **Limitation:** GMMs assume distributions are Gaussian blobs → too simple for complex speech/language data.  \n",
        "- **Replacement:** Deep Neural Networks (DNNs) model nonlinear relationships with much higher accuracy.  \n",
        "- **Result:** Rise of **DNN-HMM hybrids**, later replaced by **end-to-end neural systems** (CTC, seq2seq, Transformers).  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Modern Uses in NLP\n",
        "While GMMs are no longer dominant, they remain useful in niche applications:\n",
        "\n",
        "- **Speaker Diarization:** Clustering speaker embeddings (“who spoke when”).  \n",
        "- **Low-Resource Settings:** Lightweight modeling where deep nets are impractical.  \n",
        "- **Bayesian Word Embeddings:** Capturing uncertainty or multiple senses of words.  \n",
        "- **Generative Initialization:** Pretraining or regularizing neural embeddings.  \n",
        "\n",
        "---\n",
        "\n",
        "##  In Short\n",
        "- **Past:** GMMs were the statistical foundation of ASR and NLP, powering GMM-HMM pipelines.  \n",
        "- **Transition:** Replaced by DNNs for acoustic modeling, enabling hybrid and later end-to-end systems.  \n",
        "- **Present:** Still relevant for clustering, diarization, sense disambiguation, and lightweight modeling.  \n",
        "\n",
        "**Conceptual Legacy:** GMMs shaped the statistical foundations of sequence modeling in NLP/ASR and remain a bridge to modern probabilistic and neural methods.\n"
      ],
      "metadata": {
        "id": "ES2p2wKxGXsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Origins and Role of Gaussian Mixture Models (GMMs)\n",
        "\n",
        "## 1. Statistical Origins\n",
        "- **Quandt & Ramsey (1974)**  \n",
        "  *“Switching Regressions”*  \n",
        "  *Journal of the American Statistical Association, 67(338), 306–310.*  \n",
        "  - Introduced the idea of modeling data with a **mixture of Gaussian distributions**.  \n",
        "  - Applied the **Expectation-Maximization (EM)** approach in a regression setting.  \n",
        "  - This laid the theoretical foundation for mixture modeling.  \n",
        "\n",
        "- **Dempster, Laird & Rubin (1977)**  \n",
        "  *“Maximum Likelihood from Incomplete Data via the EM Algorithm”*  \n",
        "  *Journal of the Royal Statistical Society, Series B.*  \n",
        "  - Established the **EM algorithm** formally.  \n",
        "  - Made parameter estimation in GMMs practical and efficient.  \n",
        "  - Became the cornerstone method for clustering and density estimation with mixtures.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Adoption in Speech Recognition & NLP\n",
        "- **1980s–1990s:** GMMs entered ASR as part of **GMM-HMM systems**.  \n",
        "  - **GMMs:** Modeled the distribution of **acoustic features** (MFCCs, filterbanks).  \n",
        "  - **HMMs:** Captured sequential structure of speech.  \n",
        "  - Together: GMM-HMM became the **standard acoustic modeling framework** for decades.  \n",
        "\n",
        "- **Rabiner (1989):**  \n",
        "  *“A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition”*  \n",
        "  - Widely cited tutorial that popularized GMM-HMMs.  \n",
        "  - Cemented GMMs as the statistical backbone of speech recognition.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Legacy\n",
        "- **Statistical Origin:**  \n",
        "  - Quandt & Ramsey (1974): Mixture modeling framework.  \n",
        "  - Dempster et al. (1977): General EM algorithm, essential for estimation.  \n",
        "\n",
        "- **ASR/NLP Adoption:**  \n",
        "  - GMM-HMM became dominant acoustic modeling paradigm (1980s–2010s).  \n",
        "  - Enabled large-vocabulary continuous speech recognition.  \n",
        "\n",
        "- **Transition:**  \n",
        "  - GMMs replaced by **Deep Neural Networks (DNNs)** for acoustic modeling (post-2010).  \n",
        "  - Yet conceptually important: probabilistic mixtures inspired clustering, Bayesian embeddings, and generative modeling.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Summary\n",
        "- **Foundational papers:**  \n",
        "  - Quandt & Ramsey (1974) → mixture of Gaussians (switching regressions).  \n",
        "  - Dempster, Laird & Rubin (1977) → EM algorithm, made estimation tractable.  \n",
        "\n",
        "- **NLP/ASR impact:**  \n",
        "  - Adopted in the late 1980s through **GMM-HMM systems**, popularized by **Rabiner (1989)**.  \n",
        "  - Served as the workhorse acoustic model for decades until displaced by neural methods.  \n",
        "\n",
        "**Thus, GMMs stand at the intersection of classical statistics and modern speech/NLP modeling — bridging the statistical literature of the 1970s with the ASR breakthroughs of the 1980s and 1990s.**\n"
      ],
      "metadata": {
        "id": "6MS0DsthGeMi"
      }
    }
  ]
}