{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“œ Chronological Evolution of Attention Mechanisms\n",
        "\n",
        "---\n",
        "\n",
        "## **Timeline of Key Papers**\n",
        "\n",
        "| Year | Authors | Paper | Idea | Contribution | Gap Filled |\n",
        "|------|---------|-------|------|--------------|------------|\n",
        "| **1994** | Bengio et al. | *Learning long-term dependencies with gradient descent is difficult* | Identified vanishing/exploding gradients in RNNs. | Showed why RNNs fail on long sequences. | Motivated improved memory mechanisms. |\n",
        "| **1997** | Hochreiter & Schmidhuber | *Long Short-Term Memory* | Introduced gating (input, forget, output) in RNNs. | Preserved information across longer spans. | Overcame gradient issues but still compressed sequences into fixed vectors. |\n",
        "| **2014** | Sutskever, Vinyals & Le | *Sequence to Sequence Learning with Neural Networks* | Encoderâ€“decoder using LSTMs for MT. | End-to-end Seq2Seq translation became possible. | Handled variable-length sequences, but suffered from single-context bottleneck. |\n",
        "| **2014** | Bahdanau, Cho & Bengio | *Neural Machine Translation by Jointly Learning to Align and Translate* | Additive attention to remove Seq2Seq bottleneck. | Decoder attends to all encoder hidden states. | Solved information bottleneck, improved long-sentence translation. |\n",
        "| **2015** | Luong et al. | *Effective Approaches to Attention-based Neural MT* | Multiplicative (dot-product) attention; local/global strategies. | Faster, simpler, more efficient alignment. | Improved computational efficiency and flexibility. |\n",
        "| **2016** | Cheng, Dong & Lapata | *Long Short-Term Memory-Networks for Machine Reading* | Introduced self-attention (intra-attention). | Captured token relationships within a sequence. | Extended attention beyond translation â†’ general machine reading. |\n",
        "| **2017** | Vaswani et al. | *Attention is All You Need* | Transformer with scaled dot-product + multi-head attention. | Removed recurrence and convolution. | Enabled parallelization, achieved state-of-the-art NLP performance. |\n",
        "| **2018â€“2020** | Devlin et al. (BERT), Radford et al. (GPT), Dosovitskiy et al. (ViT) | *BERT (2018), GPT (2018), ViT (2020)* | Attention generalized to pretraining and vision. | BERT â†’ bidirectional encoding; GPT â†’ autoregressive decoding; ViT â†’ vision. | Expanded attention to **NLP, vision, multimodal** tasks. |\n",
        "| **2021â€“2025** | Multiple (Sparse Transformers, Performer, Longformer, RoPE, MQA) | *Efficient Attention Variants* | Reduced quadratic cost of attention. | Enabled very long context windows and trillion-parameter LLMs. | Scaled transformers into GPT-4, Claude, LLaMA, etc. |\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… **Key Takeaways**\n",
        "\n",
        "- **Seq2Seq (2014, Sutskever et al.)** â†’ encoderâ€“decoder, but with bottleneck.  \n",
        "- **Bahdanau (2014)** â†’ introduced attention to remove bottleneck.  \n",
        "- **Luong (2015)** â†’ made attention efficient with multiplicative scoring.  \n",
        "- **Cheng (2016)** â†’ extended attention to self-attention for machine reading.  \n",
        "- **Vaswani (2017)** â†’ Transformer: fully attention-based, no recurrence.  \n",
        "- **2018+** â†’ Attention generalized to **NLP, vision, multimodal**, then scaled massively for LLMs.  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "dNkxkvB-qwej"
      }
    }
  ]
}