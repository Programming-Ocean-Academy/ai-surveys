{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Chronological Evolution of Attention in NLP and Beyond\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Early Seq2Seq and RNN Foundations\n",
        "- **Sutskever, Vinyals, Le (2014) ‚Äì *Sequence to Sequence Learning with Neural Networks***\n",
        "  - Introduced Seq2Seq with LSTMs for machine translation.  \n",
        "  - Used encoder‚Äìdecoder framework.  \n",
        "  - Limitation: suffered from fixed-length context vector bottleneck.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Birth of Attention\n",
        "- **Bahdanau, Cho, Bengio (2014) ‚Äì *Neural Machine Translation by Jointly Learning to Align and Translate***\n",
        "  - Introduced **additive attention**.  \n",
        "  - Decoder could access all encoder hidden states, not just the final one.  \n",
        "  - Solved the long-sequence bottleneck in Seq2Seq.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Improvements to Attention\n",
        "- **Luong, Pham, Manning (2015) ‚Äì *Effective Approaches to Attention-based Neural Machine Translation***\n",
        "  - Proposed **multiplicative (dot-product) attention**.  \n",
        "  - More efficient than additive attention.  \n",
        "  - Distinguished **global vs. local attention**.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Expansion to Self-Attention\n",
        "- **Cheng, Dong, Lapata (2016) ‚Äì *Long Short-Term Memory-Networks for Machine Reading***\n",
        "  - Introduced **intra-attention (self-attention)**.  \n",
        "  - Modeled relationships between tokens in the same sequence.  \n",
        "  - Paved the way for the Transformer.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Transformers and Scaled Dot-Product Attention\n",
        "- **Vaswani et al. (2017) ‚Äì *Attention is All You Need***\n",
        "  - Introduced the **Transformer architecture**.  \n",
        "  - Key innovations:  \n",
        "    - Scaled dot-product attention.  \n",
        "    - Multi-head attention.  \n",
        "    - Positional encoding (to model word order).  \n",
        "  - Eliminated recurrence and convolutions ‚Üí enabled full parallelization.  \n",
        "  - Became backbone of **GPT, BERT, ViT, and modern LLMs**.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Transformer Variants for NLP\n",
        "- **Devlin et al. (2018) ‚Äì *BERT: Pre-training of Deep Bidirectional Transformers***  \n",
        "  - Encoder-only transformer.  \n",
        "  - State-of-the-art on benchmarks (GLUE, SQuAD).  \n",
        "\n",
        "- **Radford et al. (2018‚Äì2023) ‚Äì *GPT series***  \n",
        "  - Decoder-only transformers.  \n",
        "  - Enabled large-scale autoregressive text generation.  \n",
        "  - Foundation of **ChatGPT** and the LLM revolution.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Attention in Vision & Multimodal AI\n",
        "- **Dosovitskiy et al. (2020) ‚Äì *An Image is Worth 16x16 Words***  \n",
        "  - Introduced **Vision Transformer (ViT)**.  \n",
        "  - Showed attention can outperform CNNs in vision tasks.  \n",
        "\n",
        "- **Ramesh et al. (2021, 2022) ‚Äì *DALL¬∑E series***  \n",
        "  - Combined **transformers + attention** with **diffusion models**.  \n",
        "  - Enabled large-scale text-to-image generation.  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Verdict\n",
        "- **Bahdanau et al. (2014)** ‚Üí introduced attention.  \n",
        "- **Luong et al. (2015)** ‚Üí improved efficiency with multiplicative attention.  \n",
        "- **Cheng et al. (2016)** ‚Üí pioneered self-attention.  \n",
        "- **Vaswani et al. (2017)** ‚Üí Transformer revolution.  \n",
        "- **BERT / GPT (2018‚Äì)** ‚Üí dominance in NLP.  \n",
        "- **ViT / DALL¬∑E (2020‚Äì)** ‚Üí attention expands into vision & multimodal generative AI.  \n"
      ],
      "metadata": {
        "id": "WFYuHAnNaaEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîé Evolution of Self-Attention in NLP and Beyond\n",
        "\n",
        "---\n",
        "\n",
        "## üå± Foundations\n",
        "- **Christopher Manning, Prabhakar Raghavan, Hinrich Sch√ºtze (2008) ‚Äì *Introduction to Information Retrieval***\n",
        "  - Early reference for **tokenization** and **text sequence representation**.  \n",
        "  - Provided groundwork for later **self-attention models**.  \n",
        "\n",
        "---\n",
        "\n",
        "## üåä The Transformer Breakthrough\n",
        "- **Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin (2017) ‚Äì *Attention Is All You Need (NeurIPS 2017)***\n",
        "  - Introduced **self-attention** and the **Transformer architecture**.  \n",
        "  - Solved RNN/CNN limitations:  \n",
        "    - Parallelization.  \n",
        "    - Long-range dependency modeling.  \n",
        "    - Scalability.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìñ Contextual Understanding\n",
        "- **Devlin, Chang, Lee, Toutanova (2019) ‚Äì *BERT***\n",
        "  - Applied **bidirectional self-attention** for contextual embeddings.  \n",
        "  - Revolutionized NLP tasks: **Q&A, NLI, sentiment analysis**.  \n",
        "\n",
        "- **OpenAI (Radford et al., 2018‚Äì2023) ‚Äì *GPT series***\n",
        "  - Showcased **decoder-only self-attention** for **generative modeling**.  \n",
        "  - Expanded context windows ‚Üí improved **fluency** and **reasoning**.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Advances in Self-Attention Variants\n",
        "- **Li et al. (2020)** ‚Äì *BiLSTM + Self-Attention for Sentiment Classification*  \n",
        "  - Combined **LSTM + self-attention** for sentiment analysis.  \n",
        "\n",
        "- **Yu & Fujita (2020)** ‚Äì *Parallel Scheduling Self-Attention*  \n",
        "  - Optimized scheduling for **efficiency and scalability**.  \n",
        "\n",
        "- **Zhao, Jia, Koltun (2020) ‚Äì *Exploring Self-Attention for Image Recognition (CVPR 2020)***  \n",
        "  - Brought self-attention into **computer vision**.  \n",
        "  - Early step toward **Vision Transformers (ViT)**.  \n",
        "\n",
        "- **Saratchandran et al. (2024) ‚Äì *Rethinking Softmax: Self-Attention with Polynomial Activations***  \n",
        "  - Proposed **alternatives to softmax** for attention computation.  \n",
        "\n",
        "- **Zeng et al. (2024) ‚Äì *Scaling of Search and Learning: RL Perspective on o1***  \n",
        "  - Linked **self-attention scaling** with **reinforcement learning** for foundation models.  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Verdict\n",
        "- **Vaswani et al. (2017)** ‚Üí origin of self-attention (**Transformer**).  \n",
        "- **Devlin et al. (2019)** ‚Üí contextual bidirectional attention (**BERT**).  \n",
        "- **Radford et al. (2018‚Äì)** ‚Üí generative power of self-attention (**GPT**).  \n",
        "- **Li, Yu, Zhao (2020)** ‚Üí adaptations across NLP + vision.  \n",
        "- **Saratchandran & Zeng (2024)** ‚Üí pushing **efficiency** and **scalability** of self-attention.  \n"
      ],
      "metadata": {
        "id": "srM8sN-saksP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìú History and Evolution of Attention Mechanisms\n",
        "\n",
        "---\n",
        "\n",
        "## üå± Early Inspirations\n",
        "- **1950s‚Äì1960s**: Cognitive psychology & neuroscience ‚Üí *Cocktail party effect*, early filter models of attention.  \n",
        "- **1980s**: Sigma‚Äìpi units and higher-order neural networks anticipated **multiplicative mechanisms**.  \n",
        "- **1990s**: *Fast weight controllers* and *dynamic links* ‚Üí early **key‚Äìvalue pair** inspirations.  \n",
        "- **1998**: Bilateral filters in image processing (pairwise affinities).  \n",
        "- **2005**: Non-local means for denoising ‚Üí Gaussian similarity kernels, precursors to fixed attention weights.  \n",
        "\n",
        "---\n",
        "\n",
        "## üîë Breakthroughs in Machine Learning\n",
        "- **2014 ‚Äì Bahdanau et al.**  \n",
        "  - Introduced **additive attention** in Seq2Seq RNNs for translation.  \n",
        "  - Solved the *fixed-context bottleneck*.  \n",
        "\n",
        "- **2015 ‚Äì Xu et al. (*Show, Attend and Tell*)**  \n",
        "  - Extended attention to **image captioning**.  \n",
        "\n",
        "- **2016 ‚Äì Cheng, Dong & Lapata**  \n",
        "  - Self-attention in RNNs for **intra-sequence dependencies**.  \n",
        "\n",
        "- **2017 ‚Äì Vaswani et al. (*Attention Is All You Need*)**  \n",
        "  - Introduced the **Transformer architecture**.  \n",
        "  - Scaled dot-product attention, multi-head attention, positional encoding.  \n",
        "  - Removed recurrence & convolutions, enabling **parallelization**.  \n",
        "\n",
        "- **2018**  \n",
        "  - Wang et al. ‚Üí **Non-local neural networks** for vision.  \n",
        "  - Veliƒçkoviƒá et al. ‚Üí **Graph Attention Networks (GATs)**.  \n",
        "\n",
        "- **2019‚Äì2020**  \n",
        "  - Efficient Transformers: **Reformer, Linformer, Performer, Longformer**.  \n",
        "\n",
        "- **2019+ Applications**  \n",
        "  - **ViT (Vision Transformers)** for image classification.  \n",
        "  - **AlphaFold** for protein folding.  \n",
        "  - **CLIP** for vision‚Äìlanguage grounding.  \n",
        "  - Dense segmentation (CCNet, DANet).  \n",
        "\n",
        "- **Surveys**  \n",
        "  - Niu et al. (2021), Soydaner (2022).  \n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Core Variants of Attention\n",
        "\n",
        "- **Additive Attention (Bahdanau, 2014):**  \n",
        "  $$\n",
        "  \\text{Attention}(Q,K,V) = \\text{softmax} \\big( \\tanh(W_Q Q + W_K K) \\big) V\n",
        "  $$\n",
        "\n",
        "- **Multiplicative (Dot-Product) Attention (Luong, 2015):**  \n",
        "  $$\n",
        "  \\text{Attention}(Q,K,V) = \\text{softmax}(Q W K^\\top) V\n",
        "  $$\n",
        "\n",
        "- **Scaled Dot-Product Attention (Vaswani, 2017):**  \n",
        "  $$\n",
        "  \\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
        "  $$\n",
        "\n",
        "- **Self-Attention:** $Q, K, V$ all derived from the same sequence.  \n",
        "- **Masked Attention:** Restricts attention to past tokens (autoregressive).  \n",
        "- **Multi-Head Attention:**  \n",
        "  $$\n",
        "  \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) W^O\n",
        "  $$  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Optimizations\n",
        "- **FlashAttention (Dao et al., 2022):** Blockwise, memory-efficient kernels.  \n",
        "- **FlexAttention (Meta, 2023):** User-modifiable scoring kernels.  \n",
        "- **Efficient Transformers (2019‚Äì2020):** Reformer, Linformer, Performer, Longformer for long sequences.  \n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Applications\n",
        "- **NLP:** Machine translation, summarization, QA, sentiment classification.  \n",
        "- **Vision:** ViTs for detection, segmentation, captioning.  \n",
        "- **Speech:** Recognition and sequence modeling.  \n",
        "- **Science:** Protein folding (AlphaFold), multimodal AI (CLIP).  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Interpretation & Visualization\n",
        "- **Alignment matrices:** Show word‚Äìword relations in translation.  \n",
        "- **Attention maps:** Used in ViTs as saliency/explanation tools.  \n",
        "- **Debate:** High attention weight ‚â† strong causal influence.  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Takeaway\n",
        "Attention evolved from **cognitive inspiration ‚Üí RNN helpers ‚Üí Bahdanau & Luong ‚Üí Transformers ‚Üí efficient/self-attention models**.  \n",
        "Today, it underpins **NLP, vision, speech, science, and multimodal AI**, becoming a universal mechanism for representation learning.  \n"
      ],
      "metadata": {
        "id": "kVZycyCkawly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìú Chronological Evolution of Attention Mechanisms\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Early Cognitive & Biological Roots\n",
        "- **1950s‚Äì1960s** ‚Üí Psychology & biology of attention: cocktail party effect, filter models.  \n",
        "- **1980s** ‚Üí Sigma‚Äìpi units, higher-order neural nets anticipated multiplicative interactions.  \n",
        "- **1990s** ‚Üí Fast weight controllers and dynamic links ‚Üí proto key‚Äìvalue systems.  \n",
        "- **1998** ‚Üí Bilateral filter in image processing (pairwise affinities).  \n",
        "- **2005** ‚Üí Non-local means (Gaussian kernels) as fixed attention weights in denoising.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Seq2Seq & RNN Foundations\n",
        "- **Sutskever, Vinyals, Le (2014)** ‚Äì *Sequence to Sequence Learning with Neural Networks*  \n",
        "  - Encoder‚Äìdecoder with LSTMs.  \n",
        "  - Solved variable-length mapping but suffered from bottlenecked fixed-length vectors.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Birth of Neural Attention\n",
        "- **Bahdanau, Cho, Bengio (2014)** ‚Äì *Neural Machine Translation by Jointly Learning to Align and Translate*  \n",
        "  - Introduced **additive attention**.  \n",
        "  - Allowed the decoder to access all encoder states, not just the final one.  \n",
        "  - Solved the Seq2Seq information bottleneck.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Variants & Expansions\n",
        "- **Luong et al. (2015)** ‚Äì *Multiplicative (dot-product) attention*.  \n",
        "  - More efficient, introduced global vs. local alignment.  \n",
        "- **Xu et al. (2015)** ‚Äì *Show, Attend and Tell* (vision captioning).  \n",
        "  - Applied attention to computer vision.  \n",
        "- **Cheng et al. (2016)** ‚Äì *Self-attention in RNNs*.  \n",
        "  - Modeled intra-sequence dependencies, step toward Transformers.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Transformers Era\n",
        "- **Vaswani et al. (2017)** ‚Äì *Attention Is All You Need*.  \n",
        "  - Introduced **Transformer architecture**, scaled dot-product, multi-head attention.  \n",
        "  - Removed recurrence and convolution, enabled parallelization and scalability.  \n",
        "- **Devlin et al. (2019)** ‚Äì *BERT*.  \n",
        "  - Bidirectional encoder-only attention, contextual embeddings.  \n",
        "- **Radford et al. (2018‚Äì2023)** ‚Äì *GPT series*.  \n",
        "  - Decoder-only, generative text modeling.  \n",
        "- **Dosovitskiy et al. (2020)** ‚Äì *ViT*.  \n",
        "  - Vision Transformer, extending attention to images.  \n",
        "- **Ramesh et al. (2021‚Äì2022)** ‚Äì *DALL¬∑E series*.  \n",
        "  - Multimodal: text-to-image generation via attention + diffusion.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Optimizations & Variants\n",
        "- **Reformer, Linformer, Performer (2019‚Äì2020)** ‚Äì Efficient Transformers for long sequences.  \n",
        "- **Dao et al. (2022)** ‚Äì *FlashAttention*.  \n",
        "  - Memory-efficient blockwise computation.  \n",
        "- **Meta (2023)** ‚Äì *FlexAttention*.  \n",
        "  - Flexible, user-modifiable kernels.  \n",
        "- **Saratchandran et al. (2024)** ‚Äì Polynomial activations to replace softmax.  \n",
        "- **Zeng et al. (2024)** ‚Äì Scaling perspective linking self-attention and reinforcement learning.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß© Applications Across Modalities\n",
        "- **NLP:** Translation, summarization, QA, sentiment classification.  \n",
        "- **Vision:** ViTs, attention maps for detection, segmentation.  \n",
        "- **Speech:** Sequence modeling and recognition.  \n",
        "- **Science:** Protein folding (*AlphaFold*), multimodal alignment (*CLIP*).  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Takeaway\n",
        "Attention evolved from **cognitive inspirations** ‚Üí **RNN helpers** ‚Üí **Bahdanau & Luong breakthroughs** ‚Üí **Transformers** ‚Üí **efficient/self-attention variants**, now achieving **ubiquity across NLP, vision, science, and multimodal AI**.  \n"
      ],
      "metadata": {
        "id": "n3CGBRr2a59v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìë Comprehensive Collection of Attention Mechanism Equations\n",
        "\n",
        "---\n",
        "\n",
        "## üîë Core Equations\n",
        "\n",
        "### 1. **Additive Attention** (Bahdanau, 2014)\n",
        "\n",
        "$$\n",
        "\\text{score}(q, k) = v_a^\\top \\tanh(W_q q + W_k k)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\alpha_i = \\frac{\\exp(\\text{score}(q, k_i))}{\\sum_j \\exp(\\text{score}(q, k_j))}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Attention}(q, K, V) = \\sum_i \\alpha_i v_i\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Multiplicative / Dot-Product Attention** (Luong, 2015)\n",
        "\n",
        "$$\n",
        "\\text{score}(q, k) = q^\\top k\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\alpha_i = \\frac{\\exp(\\text{score}(q, k_i))}{\\sum_j \\exp(\\text{score}(q, k_j))}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Attention}(q, K, V) = \\sum_i \\alpha_i v_i\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Scaled Dot-Product Attention** (Vaswani et al., 2017)\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "- $Q \\in \\mathbb{R}^{m \\times d_k}$ ‚Üí queries  \n",
        "- $K \\in \\mathbb{R}^{n \\times d_k}$ ‚Üí keys  \n",
        "- $V \\in \\mathbb{R}^{n \\times d_v}$ ‚Üí values  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Masked Attention** (causal masking for autoregressive models)\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + M\\right)V\n",
        "$$\n",
        "\n",
        "where mask $M$ is defined as:\n",
        "\n",
        "$$\n",
        "M_{ij} =\n",
        "\\begin{cases}\n",
        "0 & \\text{if } j \\leq i \\\\\n",
        "-\\infty & \\text{if } j > i\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Multi-Head Attention**\n",
        "\n",
        "Each head:\n",
        "\n",
        "$$\n",
        "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
        "$$\n",
        "\n",
        "Concatenation:\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Self-Attention\n",
        "\n",
        "For sequence matrix $H$:\n",
        "\n",
        "$$\n",
        "H' = \\text{Attention}(HW^Q, HW^K, HW^V)\n",
        "$$\n",
        "\n",
        "‚û°Ô∏è This allows **all tokens to attend to all others** in the sequence.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìê Positional Encoding (Vaswani et al., 2017)\n",
        "\n",
        "For token at position $pos$, dimension $i$:\n",
        "\n",
        "$$\n",
        "PE_{(pos,2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "PE_{(pos,2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## üåÄ Variants\n",
        "\n",
        "### ‚Ä¢ General Attention (Luong, 2015)\n",
        "\n",
        "$$\n",
        "\\text{score}(q, k) = q^\\top W k\n",
        "$$\n",
        "\n",
        "### ‚Ä¢ Bilinear Attention\n",
        "\n",
        "$$\n",
        "\\text{score}(q, k) = q^\\top W k\n",
        "$$\n",
        "\n",
        "(similar to Luong but more generalized with a learned matrix $W$).  \n",
        "\n",
        "### ‚Ä¢ Polynomial Activation Self-Attention (Saratchandran et al., 2024)\n",
        "\n",
        "$$\n",
        "\\alpha = \\text{poly}(QK^\\top) \\quad \\text{instead of softmax}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Coverage\n",
        "\n",
        "This collection spans:\n",
        "\n",
        "- Additive Attention (Bahdanau, 2014)  \n",
        "- Multiplicative / Dot-Product Attention (Luong, 2015)  \n",
        "- Scaled Dot-Product Attention (Transformers, 2017)  \n",
        "- Masked Attention (causal language models)  \n",
        "- Multi-Head Attention  \n",
        "- Self-Attention  \n",
        "- Positional Encoding  \n",
        "- Modern Variants (Polynomial/Efficient Attention)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "fowVtL8ypDqK"
      }
    }
  ]
}