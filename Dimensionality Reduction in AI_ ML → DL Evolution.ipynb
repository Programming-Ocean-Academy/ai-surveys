{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/PVDBbkaQXpgTWst0sUeD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üìú Dimensionality Reduction in AI: ML ‚Üí DL Evolution\n","\n","---\n","\n","## üîπ Definition\n","- **Dimensionality reduction** = process of transforming high-dimensional data into a **lower-dimensional representation** while preserving essential structure or variance.  \n","- **Goals:**  \n","  - Remove noise & redundancy.  \n","  - Improve computational efficiency.  \n","  - Enable visualization (2D/3D).  \n","  - Prevent overfitting in ML/DL models.  \n","\n","---\n","\n","## üîπ Dimensionality Reduction in Classical ML\n","\n","### 1. Linear Methods\n","| **Method** | **Year** | **Authors** | **Key Idea** |\n","|------------|----------|--------------|--------------|\n","| **PCA (Principal Component Analysis)** | 1901, 1933 | Pearson, Hotelling | Projects data onto directions of maximum variance. |\n","| **LDA (Linear Discriminant Analysis)** | 1936 | Fisher | Supervised method maximizing class separability. |\n","\n","### 2. Nonlinear & Manifold Learning\n","| **Method** | **Year** | **Authors** | **Key Idea** |\n","|------------|----------|--------------|--------------|\n","| **MDS (Multidimensional Scaling)** | 1952 | Torgerson | Preserves pairwise distances in reduced space. |\n","| **Isomap** | 2000 | Tenenbaum et al. | Nonlinear DR using geodesic distances on manifolds. |\n","| **t-SNE** | 2008 | van der Maaten & Hinton | Popular visualization tool preserving local similarity. |\n","| **UMAP** | 2018 | McInnes et al. | Scalable alternative to t-SNE, preserves global structure. |\n","\n","‚û°Ô∏è These methods laid the foundation for **visualization and feature reduction** in ML.  \n","\n","---\n","\n","## üîπ Dimensionality Reduction in Deep Learning\n","\n","### 1. Autoencoders\n","- **Basic Autoencoder (1986):** Rumelhart, Hinton & Williams ‚Üí encoder‚Äìdecoder learns compressed latent space.  \n","- **Deep Autoencoder (2006):** Hinton & Salakhutdinov (*Science*) ‚Üí stacked RBMs for nonlinear DR.  \n","\n","### 2. Variational Autoencoders (VAEs)\n","- **Kingma & Welling (2013):** *Auto-Encoding Variational Bayes*.  \n","- Probabilistic DR + generative modeling.  \n","\n","### 3. Deep Manifold Learning\n","- **Deep Embedded Clustering (DEC, 2016):** Learns embeddings + clustering jointly.  \n","- **Contrastive Representation Learning (SimCLR, MoCo, 2020):** Embeddings align semantically similar data in latent space.  \n","\n","---\n","\n","## üîπ Applications of Dimensionality Reduction\n","- **Visualization:** 2D/3D embeddings for exploratory analysis.  \n","- **Preprocessing:** Feature reduction before supervised ML/DL.  \n","- **Compression:** Reduced storage/memory (e.g., autoencoders for images).  \n","- **Noise Reduction:** PCA denoising, denoising autoencoders.  \n","- **Generative Models:** Latent spaces (VAE, GAN, Diffusion) rely on reduced representations.  \n","\n","---\n","\n","## ‚úÖ Key Insights\n","- **In Classical ML:** PCA, LDA, and manifold learning (Isomap, t-SNE, UMAP) dominated DR.  \n","- **In Deep Learning:** Autoencoders and VAEs enabled **nonlinear & scalable** dimensionality reduction.  \n","- **Today:** DR is built into **representation learning & foundation models** ‚Üí latent embeddings (text, vision, multimodal) serve as powerful compressed representations.  \n"],"metadata":{"id":"RP3owiRVGc3-"}}]}