{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP vs. ASR (Automatic Speech Recognition)\n",
        "\n",
        "## 1. Core Focus\n",
        "- **NLP**: Deals with textual data (tokens, sentences, documents). The central task is extracting meaning, structure, and relationships from written language.  \n",
        "- **ASR (Automatic Speech Recognition)**: Converts audio signals into text. The challenge is mapping a variable-length acoustic waveform into a discrete sequence of words.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Input Representation\n",
        "- **NLP**: Uses discrete tokens (words, subwords, characters) often encoded with embeddings (Word2Vec, GloVe, Transformers’ learned embeddings).  \n",
        "- **ASR**: Begins with a continuous audio signal (waveform), which is transformed into acoustic features (MFCCs, spectrograms, mel-filterbanks) before modeling.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Modeling Paradigms\n",
        "- **NLP**: Transformer architectures dominate (BERT, GPT, T5), pretrained on large text corpora for contextual understanding.  \n",
        "- **ASR**: Earlier systems combined HMMs with GMMs/DNNs; modern ASR relies on RNNs (LSTMs), CNNs, and increasingly on end-to-end Transformers (e.g., wav2vec 2.0, Whisper).  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Tasks\n",
        "- **NLP**: Sentiment analysis, machine translation, summarization, question answering, dialogue systems.  \n",
        "- **ASR**: A system for speech-to-text transcription, keyword spotting, speaker diarization, and real-time captioning.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Key Challenges\n",
        "- **NLP**:  \n",
        "  - Ambiguity in syntax/semantics.  \n",
        "  - Contextual understanding (sarcasm, pragmatics).  \n",
        "  - Multilingual and low-resource languages.  \n",
        "\n",
        "- **ASR**:  \n",
        "  - Noise, accents, prosody, and coarticulation.  \n",
        "  - A need for domain adaptation (medical vs. conversational speech).  \n",
        "  - Real-time latency constraints.  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. Overlap and Synergy\n",
        "- **ASR** often serves as the front end to NLP systems: audio → text → downstream NLP (translation, intent classification, etc.).  \n",
        "- Both benefit from self-supervised pretraining (BERT for NLP, wav2vec/Whisper for speech).  \n",
        "- Unified multimodal models are emerging (e.g., SpeechT5, SeamlessM4T) that bridge a gap between text and speech.  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Evaluation\n",
        "- **NLP**: Accuracy, F1, BLEU, ROUGE, perplexity.  \n",
        "- **ASR**: Word Error Rate (WER), Character Error Rate (CER).  \n",
        "\n",
        "---\n",
        "\n",
        " **In short:** NLP operates on symbolic text, while ASR bridges a raw acoustic signal into text. Deep learning unifies both through sequence models, and their synergy underpins modern conversational AI systems.\n"
      ],
      "metadata": {
        "id": "iF1nAFX1zCVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Breakthrough Academic Papers in NLP and ASR\n",
        "\n",
        "## Natural Language Processing (NLP)\n",
        "\n",
        "- **Mikolov et al., 2013 – Word2Vec (NeurIPS Workshop)**\n",
        "  - Introduced distributed word embeddings, capturing semantic similarity efficiently.  \n",
        "- **Bahdanau et al., 2014 – Neural Machine Translation by Jointly Learning to Align and Translate (ICLR)**\n",
        "  - First attention mechanism, enabling better translation quality.  \n",
        "- **Vaswani et al., 2017 – Attention Is All You Need (NeurIPS)**\n",
        "  - Introduced the Transformer architecture, replacing recurrence and enabling parallelism.  \n",
        "- **Devlin et al., 2018 – BERT: Pre-training of Deep Bidirectional Transformers (NAACL)**\n",
        "  - Bidirectional contextual embeddings; set new benchmarks across NLP tasks.  \n",
        "- **Brown et al., 2020 – Language Models Are Few-Shot Learners (NeurIPS)**\n",
        "  - GPT-3 demonstrated large-scale pretraining for few-shot and zero-shot capabilities.  \n",
        "\n",
        "---\n",
        "\n",
        "## Automatic Speech Recognition (ASR)\n",
        "\n",
        "- **Rabiner, 1989 – Tutorial on Hidden Markov Models (Proceedings of the IEEE)**\n",
        "  - Established HMMs as the dominant paradigm for ASR.  \n",
        "- **Hinton et al., 2012 – Deep Neural Networks for Acoustic Modeling (IEEE Signal Processing Magazine)**\n",
        "  - Showed DNNs outperform GMMs for ASR acoustic modeling.  \n",
        "- **Graves et al., 2013 – Speech Recognition with Deep RNNs (ICASSP)**\n",
        "  - Applied LSTMs for speech recognition, improving sequence modeling.  \n",
        "- **Chan et al., 2016 – Listen, Attend and Spell (ICASSP)**\n",
        "  - Introduced an end-to-end attention-based ASR model.  \n",
        "- **Baevski et al., 2020 – wav2vec 2.0 (NeurIPS)**\n",
        "  - Self-supervised representation learning for speech; breakthrough in low-resource ASR.  \n",
        "- **Radford et al., 2023 – Whisper (OpenAI Report)**\n",
        "  - Multilingual, multitask, robust ASR model trained on 680k hours; state-of-the-art generalization.  \n"
      ],
      "metadata": {
        "id": "hO3WFUSWy-EL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bEOQQoLyt9Q"
      },
      "outputs": [],
      "source": []
    }
  ]
}