{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìú Clustering Evaluation Metrics (AI/ML/DL)\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 1. Internal Metrics (Unsupervised Quality)\n",
        "\n",
        "Evaluate clustering **without ground-truth labels**: focus on compactness, separation, and structure.\n",
        "\n",
        "- **Within-Cluster Sum of Squares (WCSS / Inertia):**\n",
        "  - $WCSS = \\sum_i \\sum_{x \\in C_i} \\|x - c_i\\|^2$\n",
        "  - Lower values = tighter clusters.\n",
        "  - ‚ùå Not normalized, always decreases with more clusters.\n",
        "\n",
        "- **Silhouette Score (Rousseeuw, 1987):**\n",
        "  $$\n",
        "  s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
        "  $$\n",
        "  - $a(i)$ = average distance of $i$ to its own cluster.\n",
        "  - $b(i)$ = distance of $i$ to nearest other cluster.\n",
        "  - ‚úÖ Balances cohesion vs separation.\n",
        "  - ‚ùå Degrades in high dimensions.\n",
        "\n",
        "- **Davies‚ÄìBouldin Index (DBI, 1979):**\n",
        "  $$\n",
        "  DBI = \\frac{1}{k} \\sum_{i=1}^k \\max_{j \\neq i} \\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)}\n",
        "  $$\n",
        "  - Lower is better.\n",
        "\n",
        "- **Dunn Index (1974):**\n",
        "  $$\n",
        "  DI = \\frac{\\min_{i \\neq j} d(C_i, C_j)}{\\max_k diam(C_k)}\n",
        "  $$\n",
        "  - High = better cluster separation.\n",
        "\n",
        "- **Calinski‚ÄìHarabasz Index (1974):**\n",
        "  $$\n",
        "  CH = \\frac{tr(B_k)}{tr(W_k)} \\cdot \\frac{n-k}{k-1}\n",
        "  $$\n",
        "  - Ratio of between-cluster to within-cluster variance.\n",
        "\n",
        "- **Gap Statistic (Tibshirani, 2001):**\n",
        "  - Compare WCSS against expected value under random reference.\n",
        "  - Helps estimate optimal $k$.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 2. External Metrics (Supervised Evaluation)\n",
        "\n",
        "Require **ground-truth labels** to compare clustering quality.\n",
        "\n",
        "- **Rand Index (RI, 1971):** Measures agreement between clustering and labels.\n",
        "- **Adjusted Rand Index (ARI, 1985):** RI corrected for chance.\n",
        "- **Mutual Information (MI):**\n",
        "  $$\n",
        "  MI(U,V) = \\sum_{i,j} P(i,j) \\log \\frac{P(i,j)}{P(i)P(j)}\n",
        "  $$\n",
        "- **Normalized Mutual Information (NMI):**\n",
        "  $$\n",
        "  NMI = \\frac{MI(U,V)}{\\sqrt{H(U)H(V)}}\n",
        "  $$\n",
        "- **V-Measure:** Harmonic mean of homogeneity and completeness.\n",
        "- **Fowlkes‚ÄìMallows Index (1983):**\n",
        "  $$\n",
        "  FMI = \\sqrt{\\frac{TP}{TP+FP} \\cdot \\frac{TP}{TP+FN}}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 3. Probabilistic & Distribution-Based Metrics\n",
        "\n",
        "For probabilistic clustering (e.g., **GMMs, VAEs, LDA**):\n",
        "\n",
        "- **Log-Likelihood:** Maximize $\\log p(X|\\theta)$.\n",
        "- **AIC (Akaike Information Criterion):**\n",
        "  $$\n",
        "  AIC = 2k - 2\\ln(\\hat{L})\n",
        "  $$\n",
        "- **BIC (Bayesian Information Criterion):**\n",
        "  $$\n",
        "  BIC = k \\ln(n) - 2\\ln(\\hat{L})\n",
        "  $$\n",
        "- **Perplexity:** Common in topic modeling; lower = better fit.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 4. Deep Learning & Modern Clustering Metrics\n",
        "\n",
        "For **deep representation learning + clustering**:\n",
        "\n",
        "- **Clustering Accuracy (ACC):** Align cluster IDs with true labels via Hungarian algorithm.\n",
        "- **NMI (again):** Popular in DEC, SwAV, etc.\n",
        "- **Clustering Purity:**\n",
        "  $$\n",
        "  Purity = \\frac{1}{n} \\sum_i \\max_j |C_i \\cap L_j|\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 5. Task-Specific Metrics\n",
        "\n",
        "- **Graphs / Community Detection:** Modularity, Conductance, Normalized Cut.\n",
        "- **Computer Vision:** Purity, NMI, ARI.\n",
        "- **NLP / Topic Models:** Perplexity, Coherence Score.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary Families\n",
        "\n",
        "- **Internal metrics:** Silhouette, DBI, Dunn, Gap ‚Üí no labels required.\n",
        "- **External metrics:** ARI, NMI, V-Measure, Purity ‚Üí when labels exist.\n",
        "- **Probabilistic metrics:** Likelihood, AIC/BIC, Perplexity ‚Üí for mixture/latent models.\n",
        "- **Deep Learning metrics:** ACC (Hungarian), NMI, Purity ‚Üí embedding-based clustering.\n",
        "\n"
      ],
      "metadata": {
        "id": "7Pp-w9Ub6Dzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Comparative Table: Clustering Evaluation Metrics (AI/ML/DL)\n",
        "\n",
        "| Metric                  | Formula (simplified)                                                   | Intuition                                     | Pros                                     | Cons                                     | When to Use                                   |\n",
        "|--------------------------|------------------------------------------------------------------------|-----------------------------------------------|------------------------------------------|------------------------------------------|-----------------------------------------------|\n",
        "| **WCSS (Inertia)**       | $L = \\sum_{i=1}^n \\|x_i - c_{z_i}\\|^2$                               | Compactness: minimize within-cluster variance | Simple, intuitive (k-Means)              | Always decreases with $k$, not comparable | Choosing $k$ in k-Means                        |\n",
        "| **Silhouette Score**     | $s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$                        | Balance cohesion ($a$) vs separation ($b$)    | Intuitive, normalized $[-1,1]$           | Poor in high dimensions                   | General-purpose clustering validation          |\n",
        "| **Davies‚ÄìBouldin (DBI)** | Avg. similarity between clusters                                      | Separation vs compactness trade-off           | Fast, automatic                          | Harder to interpret                        | Comparing cluster partitions                   |\n",
        "| **Dunn Index**           | $DI = \\frac{\\min d(C_i, C_j)}{\\max diam(C_k)}$                       | Ratio of min inter- to max intra-distance     | Encourages well-separated clusters       | Sensitive to noise, expensive             | Identifying compact, separated clusters        |\n",
        "| **Calinski‚ÄìHarabasz**    | $\\frac{\\text{between-cluster var}}{\\text{within-cluster var}}$        | Variance ratio (separation vs compactness)    | Scales well, efficient                   | Biased toward larger $k$                   | Model selection in clustering                  |\n",
        "| **Gap Statistic**        | $\\text{Gap} = \\log(WCSS) - \\log(\\mathbb{E}[WCSS_{null}])$             | Compare vs random baseline                    | Helps find optimal $k$                   | Computationally heavy                      | Estimating optimal # of clusters               |\n",
        "| **Rand Index (RI)**      | Pairwise agreement fraction                                           | Agreement with ground truth                   | Simple, interpretable                    | Doesn‚Äôt adjust for chance                  | Basic external validation                      |\n",
        "| **Adjusted Rand (ARI)**  | Corrected RI                                                          | Accounts for chance clustering                | Robust to random labeling                | Can be unstable for small data             | Evaluating clustering vs ground truth          |\n",
        "| **Mutual Info (MI)**     | $MI(U,V) = \\sum_{i,j} P(i,j)\\log \\frac{P(i,j)}{P(i)P(j)}$            | Info overlap between clusters & labels        | Works with any clustering                | Unbounded, hard to compare                 | Comparing unsupervised clusters vs labels      |\n",
        "| **Normalized MI (NMI)**  | $NMI = \\frac{MI(U,V)}{\\sqrt{H(U)H(V)}}$                              | Scaled MI $\\in [0,1]$                         | Scale-independent                        | Sensitive to label permutations            | Topic modeling, NLP clustering                 |\n",
        "| **V-Measure**            | Harmonic mean of homogeneity & completeness                          | Balance cluster quality                       | Interpretable, standardized              | Needs labels                               | Document clustering, NLP                       |\n",
        "| **Fowlkes‚ÄìMallows (FMI)**| $FMI = \\sqrt{\\frac{TP}{TP+FP} \\cdot \\frac{TP}{TP+FN}}$               | Geometric mean of cluster precision/recall    | Balanced like F1                         | Rare in DL                                 | Supervised clustering evaluation               |\n",
        "| **Clustering Accuracy**  | $ACC = \\max_{\\pi}\\frac{1}{n}\\sum_i 1[y_i = \\pi(c_i)]$                | Align clusters with labels (Hungarian match)  | Direct measure, intuitive                | Needs ground truth                         | Deep clustering with labels available          |\n",
        "| **Purity**               | $\\text{Purity} = \\frac{1}{n}\\sum_i \\max_j |C_i \\cap L_j|$           | Fraction correctly assigned                   | Simple                                    | Inflates with many clusters                 | Multi-class clustering                         |\n",
        "| **Log-Lik / AIC / BIC**  | $AIC = 2k - 2\\ln(\\hat L)$, $BIC = k\\ln(n) - 2\\ln(\\hat L)$            | Probabilistic model fit                       | Likelihood-based, penalize complexity    | Assume distribution                        | GMMs, VAE-based clustering                     |\n",
        "| **Perplexity (LDA)**     | $Perp = \\exp(-\\frac{1}{N}\\sum \\log p(x))$                            | Topic distribution uncertainty                | Standard in NLP                          | Doesn‚Äôt always align with human quality     | Topic/document clustering                      |\n",
        "| **Modularity (Graphs)**  | $Q = \\frac{1}{2m}\\sum_{ij}(A_{ij} - \\frac{k_ik_j}{2m})\\delta(c_i,c_j)$ | Intra vs inter-edge density in graphs         | Strong for graph communities             | Not for non-graph data                      | Graph clustering, GNN tasks                    |\n",
        "| **Coherence (NLP)**      | Semantic similarity of top words                                     | Human-interpretability for topics             | Domain-relevant                          | Task-specific                               | Topic modeling evaluation                      |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Key Insights\n",
        "\n",
        "- **Internal metrics** ‚Üí no labels: Silhouette, DBI, Dunn, CHI, Gap.  \n",
        "- **External metrics** ‚Üí require labels: ARI, NMI, V-Measure, Purity.  \n",
        "- **Probabilistic metrics** ‚Üí for mixture/latent models: Log-Likelihood, AIC, BIC, Perplexity.  \n",
        "- **Domain-specific metrics** ‚Üí NLP (Coherence), Graphs (Modularity), CV (Purity, ACC).  \n",
        "- **Deep Learning clustering** ‚Üí ACC, NMI, Purity are most used.  \n"
      ],
      "metadata": {
        "id": "7bLYCWfp6an-"
      }
    }
  ]
}