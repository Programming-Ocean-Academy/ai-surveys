{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“˜ Comprehensive List of Techniques Supporting Deep Model Training\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Regularization by Randomization\n",
        "Introduce randomness into activations, weights, or structure to reduce overfitting.\n",
        "\n",
        "- **Dropout** (Srivastava et al., 2014) â†’ randomly drop units  \n",
        "- **DropConnect** â†’ randomly drop weights  \n",
        "- **Stochastic Depth** (Huang et al., 2016) â†’ randomly drop residual blocks  \n",
        "- **Shake-Shake Regularization** â†’ random branch combination in ResNets  \n",
        "- **DropBlock** â†’ structured dropout in feature maps  \n",
        "- **Stochastic Layer Skipping** (SkipNet, 2017) â†’ conditionally skip layers  \n",
        "- **Zoneout (RNNs)** â†’ randomly preserve past hidden states  \n",
        "- **RandAugment / Random Erasing** â†’ random input perturbations  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Normalization Techniques\n",
        "Control internal activations to stabilize gradients and reduce covariate shift.\n",
        "\n",
        "- **Batch Normalization (BN)**  \n",
        "- **Layer Normalization (LN)**  \n",
        "- **Instance Normalization (IN)**  \n",
        "- **Group Normalization (GN)**  \n",
        "- **Weight Normalization**  \n",
        "- **Spectral Normalization** (popular in GANs)  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Architectural Innovations\n",
        "Structural designs that improve gradient flow and optimization.\n",
        "\n",
        "- **Residual Connections / ResNets**  \n",
        "- **Highway Networks** (gated shortcuts)  \n",
        "- **DenseNet** (dense connectivity, feature reuse)  \n",
        "- **Skip Connections in Transformers**  \n",
        "- **Auxiliary Classifiers** (Inception, deeply-supervised nets)  \n",
        "- **Neural ODEs** (continuous-depth modeling)  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Advanced Weight Initialization\n",
        "Smart starting conditions for optimization.\n",
        "\n",
        "- **Xavier Initialization** (Glorot & Bengio, 2010)  \n",
        "- **He Initialization** (ReLU, PReLU; He et al., 2015)  \n",
        "- **Orthogonal Initialization**  \n",
        "- **LSUV** (Layer-Sequential Unit-Variance)  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Gradient & Optimization Stabilizers\n",
        "Techniques to avoid gradient explosion/vanishing and improve convergence.\n",
        "\n",
        "- **Gradient Clipping** (RNNs, Transformers)  \n",
        "- **Residual Gradient Scaling** (ResNets)  \n",
        "- **Adaptive Optimizers** â†’ Adam, RMSProp, Adagrad  \n",
        "- **Learning Rate Schedules** â†’ step decay, cosine annealing, cyclical LR  \n",
        "- **Warmup Schedules** (Transformers, very deep nets)  \n",
        "- **Lookahead Optimizer**  \n",
        "- **Sharpness-Aware Minimization (SAM)** (Izmailov et al., 2021)  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. Ensemble & Implicit Ensemble Techniques\n",
        "Boost generalization by simulating multiple models.\n",
        "\n",
        "- **Bagging / Boosting**  \n",
        "- **Dropout** â†’ implicit ensemble  \n",
        "- **Stochastic Depth** â†’ ensemble of different-depth networks  \n",
        "- **Snapshot Ensembles** (save checkpoints during one training run)  \n",
        "- **SWAG** (Stochastic Weight Averaging-Gaussian)  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Data-Level Techniques\n",
        "Improve data diversity and robustness.\n",
        "\n",
        "- **Standard Augmentation** (flip, crop, rotate, jitter)  \n",
        "- **Mixup** (linear interpolation of samples/labels)  \n",
        "- **CutMix** (patch-level mixing)  \n",
        "- **CutOut** (mask patches)  \n",
        "- **AutoAugment / RandAugment** (learned policies)  \n",
        "- **Adversarial Training** (robustness to perturbations)  \n",
        "\n",
        "---\n",
        "\n",
        "## 8. Regularization by Constraints\n",
        "Apply explicit mathematical constraints to weights.\n",
        "\n",
        "- **Weight Decay (L2 regularization)**  \n",
        "- **L1 Sparsity Regularization**  \n",
        "- **Orthogonality Constraints**  \n",
        "- **Spectral Constraints** (bound Lipschitz constant)  \n",
        "- **Manifold Regularization** (semi-supervised)  \n",
        "\n",
        "---\n",
        "\n",
        "## 9. Noise Injection Techniques\n",
        "Add stochasticity to promote robustness.\n",
        "\n",
        "- **Gaussian Noise** (inputs/weights)  \n",
        "- **Label Smoothing** (target perturbation)  \n",
        "- **Stochastic Gradient Descent** â†’ inherent minibatch noise  \n",
        "- **Bayesian Dropout** (interpreted as variational inference)  \n",
        "\n",
        "---\n",
        "\n",
        "## 10. Curriculum & Sample Selection\n",
        "Order and weight samples during training.\n",
        "\n",
        "- **Curriculum Learning** (easy â†’ hard)  \n",
        "- **Self-Paced Learning**  \n",
        "- **Hard Example Mining**  \n",
        "- **Focal Loss** (down-weight easy samples)  \n",
        "\n",
        "---\n",
        "\n",
        "## 11. Specialized Regularizers\n",
        "Tailored methods for specific architectures.\n",
        "\n",
        "- **Teacher Forcing / Scheduled Sampling** (Seq2Seq)  \n",
        "- **KL Annealing / Î²-VAE** (stabilizing generative models)  \n",
        "- **Consistency Regularization** (Mean Teacher, FixMatch)  \n",
        "- **Contrastive Loss / InfoNCE** (self-supervised representation learning)  \n",
        "\n",
        "---\n",
        "\n",
        "## 12. Optimization Tricks for Scaling Depth\n",
        "Specific techniques for very deep networks.\n",
        "\n",
        "- **Gradient Checkpointing** (memory-efficient backprop)  \n",
        "- **ResNet Identity Mappings** (He et al., 2016b)  \n",
        "- **Stochastic Depth** (deep ResNets)  \n",
        "- **ReZero** (skip connections initialized as identity)  \n",
        "- **Pre-activation ResNets** (better gradient flow)  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Key Takeaway\n",
        "**Stochastic Depth** is part of the **randomized structural regularization family** (Dropout, DropConnect, Shake-Shake, etc.), but training deep models successfully requires a **toolbox of complementary methods**:\n",
        "\n",
        "- **Weights/gradients:** initialization, optimizers, clipping.  \n",
        "- **Activations:** dropout, normalization.  \n",
        "- **Architecture:** residuals, dense connections.  \n",
        "- **Data:** augmentation, adversarial training.  \n",
        "\n",
        "ðŸ‘‰ Collectively, these techniques form the **deep learning toolkit** that makes modern large-scale training possible.\n"
      ],
      "metadata": {
        "id": "oVv1stXJ-jM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“˜ Techniques Supporting Deep Model Training\n",
        "\n",
        "---\n",
        "\n",
        "## Regularization by Randomization\n",
        "\n",
        "| Technique | Paper / Authors | Year |\n",
        "|-----------|----------------|------|\n",
        "| Dropout | Srivastava et al. â€“ *Dropout: A Simple Way to Prevent NN Overfitting* | 2014 |\n",
        "| DropConnect | Wan et al. â€“ *DropConnect* | 2013 |\n",
        "| Stochastic Depth | Huang et al. â€“ *Deep Networks with Stochastic Depth* | 2016 |\n",
        "| Shake-Shake Regularization | Gastaldi â€“ *Shake-Shake Regularization* | 2017 |\n",
        "| DropBlock | Ghiasi et al. â€“ *DropBlock: A Structured Dropout* | 2018 |\n",
        "| SkipNet (Layer Skipping) | Wang et al. â€“ *SkipNet* | 2017 |\n",
        "| Zoneout (RNNs) | Krueger et al. â€“ *Zoneout* | 2016 |\n",
        "| Random Erasing / RandAugment | Zhong et al. â€“ *Random Erasing*; Cubuk et al. â€“ *RandAugment* | 2017 / 2020 |\n",
        "\n",
        "---\n",
        "\n",
        "## Normalization Techniques\n",
        "\n",
        "| Technique | Paper / Authors | Year |\n",
        "|-----------|----------------|------|\n",
        "| Batch Normalization | Ioffe & Szegedy â€“ *Batch Norm* | 2015 |\n",
        "| Layer Normalization | Ba et al. â€“ *Layer Norm* | 2016 |\n",
        "| Instance Normalization | Ulyanov et al. â€“ *Instance Norm* | 2016 |\n",
        "| Group Normalization | Wu & He â€“ *Group Norm* | 2018 |\n",
        "| Weight Normalization | Salimans & Kingma â€“ *Weight Norm* | 2016 |\n",
        "| Spectral Normalization | Miyato et al. â€“ *Spectral Norm GANs* | 2018 |\n",
        "\n",
        "---\n",
        "\n",
        "## Architectural Innovations\n",
        "\n",
        "| Technique | Paper / Authors | Year |\n",
        "|-----------|----------------|------|\n",
        "| Residual Connections (ResNet) | He et al. â€“ *Deep Residual Learning* | 2016 |\n",
        "| Highway Networks | Srivastava et al. â€“ *Highway Networks* | 2015 |\n",
        "| DenseNet | Huang et al. â€“ *Densely Connected CNNs* | 2017 |\n",
        "| Skip Connections (Transformers) | Vaswani et al. â€“ *Attention Is All You Need* | 2017 |\n",
        "| Auxiliary Classifiers (Inception) | Szegedy et al. â€“ *Going Deeper with Inception* | 2015 |\n",
        "| Neural ODEs | Chen et al. â€“ *Neural Ordinary Differential Equations* | 2018 |\n",
        "\n",
        "---\n",
        "\n",
        "## Weight Initialization\n",
        "\n",
        "| Technique | Paper / Authors | Year |\n",
        "|-----------|----------------|------|\n",
        "| Xavier Initialization | Glorot & Bengio â€“ *Understanding Difficulty of Training Deep FFNs* | 2010 |\n",
        "| He Initialization | He et al. â€“ *Delving Deep into Rectifiers* | 2015 |\n",
        "| Orthogonal Initialization | Saxe et al. â€“ *Exact Solutions to Deep Linear Nets* | 2014 |\n",
        "| LSUV Initialization | Mishkin & Matas â€“ *All You Need is LSUV* | 2015 |\n",
        "\n",
        "---\n",
        "\n",
        "## Optimization Stabilizers\n",
        "\n",
        "| Technique | Paper / Authors | Year |\n",
        "|-----------|----------------|------|\n",
        "| Gradient Clipping | Pascanu et al. â€“ *On the Difficulty of Training RNNs* | 2013 |\n",
        "| Residual Gradient Scaling | He et al. â€“ *ResNet* | 2016 |\n",
        "| Adam Optimizer | Kingma & Ba â€“ *Adam* | 2015 |\n",
        "| RMSProp | Tieleman & Hinton â€“ *Lecture Notes* | 2012 |\n",
        "| Adagrad | Duchi et al. â€“ *Adaptive Subgradient Methods* | 2011 |\n",
        "| LR Scheduling (Cosine, Step, Cyclical) | Loshchilov & Hutter â€“ *SGDR* | 2016 |\n",
        "| Warmup Schedules | He et al. â€“ *ResNet-1202* | 2016 |\n",
        "| Lookahead Optimizer | Zhang et al. â€“ *Lookahead Optimizer* | 2019 |\n",
        "| SAM | Foret et al. â€“ *Sharpness-Aware Minimization* | 2021 |\n",
        "\n",
        "---\n",
        "\n",
        "## Ensemble & Implicit Ensembles\n",
        "\n",
        "| Technique | Paper / Authors | Year |\n",
        "|-----------|----------------|------|\n",
        "| Bagging / Boosting | Breiman â€“ *Bagging*; Freund & Schapire â€“ *Boosting* | 1996 / 1997 |\n",
        "| Dropout as Ensemble | Srivastava et al. â€“ *Dropout* | 2014 |\n",
        "| Stochastic Depth Ensemble Effect | Huang et al. â€“ *Stochastic Depth* | 2016 |\n",
        "| Snapshot Ensembles | Huang et al. â€“ *Snapshot Ensembles* | 2017 |\n",
        "| SWAG | Maddox et al. â€“ *Stochastic Weight Averaging-Gaussian* | 2019 |\n",
        "\n",
        "---\n",
        "\n",
        "## Data-Level Techniques\n",
        "\n",
        "| Technique | Paper / Authors | Year |\n",
        "|-----------|----------------|------|\n",
        "| Data Augmentation | Krizhevsky et al. â€“ *ImageNet CNN* | 2012 |\n",
        "| Mixup | Zhang et al. â€“ *Mixup* | 2017 |\n",
        "| CutMix | Yun et al. â€“ *CutMix* | 2019 |\n",
        "| CutOut | DeVries & Taylor â€“ *Cutout* | 2017 |\n",
        "| AutoAugment | Cubuk et al. â€“ *AutoAugment* | 2019 |\n",
        "| Adversarial Training | Goodfellow et al. â€“ *Explaining & Harnessing Adversarial Examples* | 2015 |\n",
        "\n",
        "---\n",
        "\n",
        "## Constraints & Regularizers\n",
        "\n",
        "| Technique | Paper / Authors | Year |\n",
        "|-----------|----------------|------|\n",
        "| Weight Decay (L2) | Krogh & Hertz â€“ *Weight Decay in Backprop* | 1992 |\n",
        "| L1 Sparsity | Tibshirani â€“ *LASSO* | 1996 |\n",
        "| Orthogonality Constraints | Brock et al. â€“ *Orthogonal Regularization RNNs* | 2016 |\n",
        "| Spectral Constraints | Yoshida & Miyato â€“ *Spectral Norm Bounds* | 2017 |\n",
        "| Manifold Regularization | Belkin et al. â€“ *Manifold Regularization* | 2006 |\n",
        "\n",
        "---\n",
        "\n",
        "## Noise Injection\n",
        "\n",
        "| Technique | Paper / Authors | Year |\n",
        "|-----------|----------------|------|\n",
        "| Gaussian Noise in Inputs/Weights | Bishop â€“ *Training with Noise is Equivalent to Tikhonov Regularization* | 1995 |\n",
        "| Label Smoothing | Szegedy et al. â€“ *Rethinking Inception* | 2016 |\n",
        "| SGD Noise | Bottou â€“ *Stochastic Gradient Descent* | 2010 |\n",
        "| Bayesian Dropout | Gal & Ghahramani â€“ *Dropout as Bayesian Approximation* | 2016 |\n",
        "\n",
        "---\n",
        "\n",
        "## Curriculum & Sample Selection\n",
        "\n",
        "| Technique | Paper / Authors | Year |\n",
        "|-----------|----------------|------|\n",
        "| Curriculum Learning | Bengio et al. â€“ *Curriculum Learning* | 2009 |\n",
        "| Self-Paced Learning | Kumar et al. â€“ *Self-Paced Learning* | 2010 |\n",
        "| Hard Example Mining | Shrivastava et al. â€“ *OHEM* | 2016 |\n",
        "| Focal Loss | Lin et al. â€“ *Focal Loss for Dense Detection* | 2017 |\n",
        "\n",
        "---\n",
        "\n",
        "## Specialized Regularizers\n",
        "\n",
        "| Technique | Paper / Authors | Year |\n",
        "|-----------|----------------|------|\n",
        "| Teacher Forcing / Scheduled Sampling | Bengio et al. â€“ *Scheduled Sampling* | 2015 |\n",
        "| KL Annealing / Î²-VAE | Higgins et al. â€“ *Î²-VAE* | 2017 |\n",
        "| Consistency Regularization (Mean Teacher) | Tarvainen & Valpola â€“ *Mean Teacher* | 2017 |\n",
        "| Contrastive Loss / InfoNCE | van den Oord et al. â€“ *CPC* | 2018 |\n",
        "\n",
        "---\n",
        "\n",
        "## Scaling Depth Tricks\n",
        "\n",
        "| Technique | Paper / Authors | Year |\n",
        "|-----------|----------------|------|\n",
        "| Gradient Checkpointing | Chen et al. â€“ *Training Deep Nets with Checkpoints* | 2016 |\n",
        "| Identity Mappings in ResNets | He et al. â€“ *Identity Mappings in ResNets* | 2016b |\n",
        "| Stochastic Depth | Huang et al. â€“ *Stochastic Depth* | 2016 |\n",
        "| ReZero | Bachlechner et al. â€“ *ReZero* | 2020 |\n",
        "| Pre-activation ResNets | He et al. â€“ *Pre-Activation ResNets* | 2016 |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "4UiAepfV_GQ2"
      }
    }
  ]
}