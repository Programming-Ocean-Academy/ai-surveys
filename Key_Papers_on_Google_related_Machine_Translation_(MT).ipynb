{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìë Key Papers on Google / Google-related Machine Translation (MT)\n",
        "\n",
        "| Paper Title | Authors / Year | Main Contribution / Relevance to Google Translate |\n",
        "|-------------|----------------|--------------------------------------------------|\n",
        "| **Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation** | Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio (2014) | One of the early encoder-decoder RNN models. It introduced encoding source phrases into vectors and decoding them into target phrases ‚Äî foundational for later seq2seq architectures used in Google NMT. |\n",
        "| **Sequence to Sequence Learning with Neural Networks** | Ilya Sutskever, Oriol Vinyals, Quoc V. Le (2014) | Seminal seq2seq model with LSTMs. Demonstrated strong performance on WMT translation tasks. Marked Google‚Äôs transition from phrase-based MT to neural models. |\n",
        "| **Neural Machine Translation by Jointly Learning to Align and Translate** | Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio (2014) | Introduced the **attention mechanism**, enabling models to focus on relevant source words when generating target words. Core to Google GNMT and multilingual MT systems. |\n",
        "| **On the Properties of Neural Machine Translation: Encoder-Decoder Approaches** | Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, Yoshua Bengio (2014) | Analyzed encoder-decoder models‚Äô strengths/weaknesses (e.g., handling long sentences, unknown words). Helped inform improvements Google adopted (subword units, deeper models). |\n",
        "| **Addressing the Rare Word Problem in Neural Machine Translation** | Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, Wojciech Zaremba (2015) | Proposed handling rare/low-frequency words with subword units. This approach was adopted by Google Translate to improve rare vocabulary coverage. |\n",
        "| **Google‚Äôs Neural Machine Translation System: Bridging the Gap between Human and Machine Translation** | Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, et al. (2016) | Landmark GNMT paper. Described Google‚Äôs production-scale NMT system with deep LSTMs, attention, residual connections, subword units, and inference optimizations ‚Äî marking the shift of Google Translate to neural MT. |\n",
        "| **Google‚Äôs Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation** | Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi√©gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, Jeffrey Dean (2016) | Introduced a single multilingual model supporting many languages and enabling **zero-shot translation** via shared vocabulary and target language tokens. Crucial for Google‚Äôs scale-up. |\n",
        "| **Attention Is All You Need** | Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, ≈Åukasz Kaiser, Illia Polosukhin (2017) | Introduced the **Transformer architecture**, later replacing RNNs in Google Translate due to better parallelization and long-range dependency modeling. |\n",
        "| **Multilingual Denoising Pre-training for Neural Machine Translation (mBART)** | Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer (2020) | Showed that **multilingual denoising pre-training** boosts translation quality, especially for low-resource languages. Influenced Google‚Äôs multilingual NMT scaling. |\n",
        "\n",
        "---\n",
        "‚úÖ **Summary:**  \n",
        "These papers trace the evolution from **early RNN-based seq2seq models** ‚Üí **attention** ‚Üí **Google‚Äôs GNMT & zero-shot multilingual systems** ‚Üí **Transformers** ‚Üí **mBART-style pretraining**, forming the backbone of modern **Google Translate**.  \n"
      ],
      "metadata": {
        "id": "MPZqhi939BiC"
      }
    }
  ]
}
