{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Boltzmann Machines: Spin Glasses Meet Machine Learning\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Origins\n",
        "\n",
        "- **Physics foundation**:  \n",
        "  - Derived from the **Sherrington–Kirkpatrick (SK)** spin glass model with an external field.  \n",
        "  - Energy-based system with stochastic binary units governed by the **Boltzmann distribution**.  \n",
        "\n",
        "- **AI translation**:  \n",
        "  - Introduced by **Hinton & Sejnowski (1983–85)**.  \n",
        "  - Generalized Hopfield networks by allowing **stochastic updates** instead of deterministic ones.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Structure\n",
        "\n",
        "- **Units**: Binary (on/off).  \n",
        "- **Connectivity**: Fully symmetric, no self-connections.  \n",
        "\n",
        "- **Energy function**:  \n",
        "  $$\n",
        "  E = - \\left( \\sum_{i<j} w_{ij} s_i s_j + \\sum_i \\theta_i s_i \\right)\n",
        "  $$\n",
        "  Same form as Hopfield networks, Ising models, and Markov random fields.  \n",
        "\n",
        "- **Dynamics**:  \n",
        "  - Stochastic updates via **Gibbs sampling**.  \n",
        "  - Unlike Hopfield nets, does not deterministically minimize energy.  \n",
        "\n",
        "- **Training**:  \n",
        "  - Contrastive phases:  \n",
        "    - **Positive phase**: Data clamped.  \n",
        "    - **Negative phase**: Free running.  \n",
        "  - Objective: minimize KL divergence between data distribution \\( P^+(V) \\) and model distribution \\( P^-(V) \\).  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Learning Mechanism\n",
        "\n",
        "- **Gradient rule**:  \n",
        "  $$\n",
        "  \\frac{\\partial G}{\\partial w_{ij}} = -\\frac{1}{R}\\left( p_{ij}^+ - p_{ij}^- \\right)\n",
        "  $$\n",
        "  where \\( p^+ \\) and \\( p^- \\) are correlations under clamped vs free phases.  \n",
        "\n",
        "- **Properties**:  \n",
        "  - Local, Hebbian-like updates.  \n",
        "  - Biologically plausible: updates depend only on connected neurons.  \n",
        "\n",
        "- **Optimization**: Gradient ascent on log-likelihood.  \n",
        "- **Sampling**: Simulated annealing ensures convergence to equilibrium.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Limitations\n",
        "\n",
        "- **Scalability**: Full Boltzmann Machines → intractable (exponential mixing times).  \n",
        "- **Variance trap**: Noisy weights drift, saturating learning.  \n",
        "- **Consequence**: Practical systems require **restricted connectivity**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Variants\n",
        "\n",
        "- **Restricted Boltzmann Machine (RBM)**:  \n",
        "  - Bipartite (visible ↔ hidden, no intra-layer links).  \n",
        "  - Efficient training via **contrastive divergence**.  \n",
        "  - Popular in early deep learning (stacked RBMs → DBNs).  \n",
        "\n",
        "- **Deep Boltzmann Machine (DBM)**:  \n",
        "  - Multiple hidden layers, all undirected.  \n",
        "  - High expressivity but requires approximate inference.  \n",
        "\n",
        "- **Spike-and-Slab RBM**:  \n",
        "  - Binary “spike” + continuous “slab”.  \n",
        "  - Models real-valued data.  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. Historical Significance\n",
        "\n",
        "- **Hopfield (1982)**: Deterministic associative memory.  \n",
        "- **Boltzmann Machine (1985)**: Introduced **stochasticity + learning**.  \n",
        "- **RBM (2000s)**: Enabled **Deep Belief Nets (2006)**, kickstarting deep learning.  \n",
        "\n",
        "- **Modern ML**:  \n",
        "  - Energy-based models influence **generative models, score-based diffusion, Transformers (attention-energy parallels)**.  \n",
        "\n",
        "- **Recognition**: Hopfield & Hinton awarded **2024 Nobel Prize in Physics** for foundational contributions (Hopfield nets + Boltzmann machines).  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Relationship to Spin Glasses & AI\n",
        "\n",
        "- **Spin Glasses (EA & SK)** → rugged landscapes, many minima.  \n",
        "- **Hopfield networks** → deterministic attractors (SK special case).  \n",
        "- **Boltzmann machines** → stochastic spin glasses with learning.  \n",
        "- **RBM/DBM** → practical, scalable deep learning precursors.  \n",
        "\n",
        "---\n",
        "\n",
        " **In short**:  \n",
        "Boltzmann Machines extend Hopfield networks by adding **stochastic sampling and learnable energy landscapes**, becoming the conceptual ancestors of **RBMs, DBNs, and modern generative models**.\n"
      ],
      "metadata": {
        "id": "Xw_8ISbSo-FF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restricted Boltzmann Machines (RBMs)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Origins\n",
        "- Proposed as **Harmoniums** by *Paul Smolensky (1986)*.  \n",
        "- Revitalized by *Geoffrey Hinton (2000s)* via **Contrastive Divergence (CD)** for efficient training.  \n",
        "- Conceptual roots:  \n",
        "  - **Boltzmann Machines (Hinton & Sejnowski, 1985)**.  \n",
        "  - Can be viewed as a **restricted Sherrington–Kirkpatrick (SK)** spin glass with external fields.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Structure\n",
        "- **Graph**: Bipartite (Visible ↔ Hidden).  \n",
        "- **Restriction**: No intra-layer connections.  \n",
        "\n",
        "- **Energy function**:\n",
        "$$\n",
        "E(v,h) = -a^T v - b^T h - v^T W h\n",
        "$$\n",
        "\n",
        "where \\( W \\) = weights, \\( a \\) = visible biases, \\( b \\) = hidden biases.  \n",
        "\n",
        "- **Probability distribution**:\n",
        "$$\n",
        "P(v,h) = \\frac{1}{Z} e^{-E(v,h)}, \\quad P(v) = \\frac{1}{Z} \\sum_h e^{-E(v,h)}\n",
        "$$\n",
        "\n",
        "with partition function \\( Z \\).  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Conditional Independence\n",
        "- Given \\( v \\): hidden units independent  \n",
        "$$\n",
        "P(h_j = 1 \\mid v) = \\sigma\\!\\left(b_j + \\sum_i w_{ij} v_i \\right)\n",
        "$$  \n",
        "\n",
        "- Given \\( h \\): visible units independent  \n",
        "$$\n",
        "P(v_i = 1 \\mid h) = \\sigma\\!\\left(a_i + \\sum_j w_{ij} h_j \\right)\n",
        "$$  \n",
        "\n",
        "- For multinomial visibles → **softmax** replaces sigmoid.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Training\n",
        "- **Objective**: Maximize likelihood of data \\( P^+(V) \\).  \n",
        "- **Challenge**: Gradient requires intractable expectations.  \n",
        "- **Solution**: Hinton’s **Contrastive Divergence (CD)** (2002):  \n",
        "  - Positive phase: clamp data.  \n",
        "  - Negative phase: Gibbs sample and reconstruct.  \n",
        "\n",
        "- **Weight update rule**:\n",
        "$$\n",
        "\\Delta W = \\epsilon \\, ( v h^T - v' h'^T )\n",
        "$$  \n",
        "\n",
        "Local and biologically plausible: depends only on correlations.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Applications\n",
        "- Dimensionality reduction (nonlinear PCA).  \n",
        "- Collaborative filtering (e.g., Netflix Challenge).  \n",
        "- Feature learning & topic modeling.  \n",
        "- Speech recognition, immunology, quantum many-body physics.  \n",
        "- **Most historically**: Foundation for **Deep Belief Networks (DBNs, 2006)**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. Relation to Other Models\n",
        "- **Boltzmann Machines**: RBM = constrained BM (bipartite).  \n",
        "- **Hopfield Networks**: Equivalent energy form, but RBM adds stochasticity + hidden layer.  \n",
        "- **Markov Random Fields (MRFs)**: RBM = special bipartite case.  \n",
        "- **Factor Analysis**: Parallel structure to statistical factor models.  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Extensions\n",
        "- **DBNs**: Stacked RBMs, layer-wise training.  \n",
        "- **DBMs**: Multi-layer undirected, slow to train.  \n",
        "- **Gaussian RBMs**: Continuous visible units.  \n",
        "- **Spike-and-Slab RBMs**: Binary + continuous latent variables.  \n",
        "\n",
        "---\n",
        "\n",
        "## 8. Historical Role\n",
        "- **1986**: Smolensky’s Harmonium.  \n",
        "- **1980s–1990s**: Limited due to training difficulty.  \n",
        "- **2006**: Hinton’s CD breakthrough made RBMs practical.  \n",
        "- **2000s**: RBMs + DBNs fueled the **deep learning revival** via unsupervised pretraining.  \n",
        "- **Today**: Rarely used directly, but foundational for **EBMs, autoencoders, generative models**.  \n",
        "\n",
        "---\n",
        "\n",
        " **In short**:  \n",
        "RBMs simplified Boltzmann Machines into a bipartite form, enabling efficient **contrastive divergence learning**. They bridged **spin glass physics** and **deep generative architectures**, playing a pivotal role in the 2000s deep learning renaissance.\n"
      ],
      "metadata": {
        "id": "5pLb7qOqpNy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Spin Glasses to Neural Networks: The Intellectual Lineage\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Ising Model (1920s)\n",
        "- Original **statistical physics model** of spins on a lattice with nearest-neighbor interactions.  \n",
        "- Defined **binary states** (+1 / −1), **energy landscapes**, and **phase transitions**.  \n",
        "- **Hamiltonian**:  \n",
        "$$\n",
        "H = - \\sum_{\\langle i j \\rangle} J_{ij} S_i S_j\n",
        "$$  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Edwards–Anderson (EA) Model (1975)\n",
        "- A **disordered Ising model** with random couplings \\( J_{ij} \\).  \n",
        "- Showed existence of **spin glass phases** (frozen disorder, many minima).  \n",
        "- Introduced the **overlap order parameter** \\( q \\) to capture memory-like states.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Sherrington–Kirkpatrick (SK) Model (1975)\n",
        "- Infinite-range (mean-field) extension of EA.  \n",
        "- Every spin interacts with every other spin.  \n",
        "- Produced **rugged, hierarchical energy landscapes**.  \n",
        "- Directly analogous to **memory storage** in brains and associative systems.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Hopfield Network (1982)\n",
        "- *John Hopfield* applied SK spin glass mathematics to neural networks.  \n",
        "- **Mapping**: Spins ↔ Neurons, Couplings ↔ Synaptic weights.  \n",
        "- Energy function identical to Ising/SK Hamiltonian.  \n",
        "- **Attractors = stored memories** → associative recall.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Boltzmann Machine (1985)\n",
        "- *Geoffrey Hinton & Terry Sejnowski* extended Hopfield nets.  \n",
        "- Added **stochasticity** and **learning** via Boltzmann distribution.  \n",
        "- Still used the Ising/SK Hamiltonian form.  \n",
        "- Trained weights using **contrastive positive/negative phases**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. Restricted Boltzmann Machine (RBM)  \n",
        "*(Smolensky 1986; Hinton 2000s)*  \n",
        "- Simplified Boltzmann Machine with **bipartite structure** for efficient learning.  \n",
        "- **Training breakthrough**: Contrastive Divergence (Hinton, 2002).  \n",
        "- Became foundation for **Deep Belief Networks (DBNs)** and early deep learning.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Conclusion\n",
        "- **Ising → EA/SK → Hopfield → Boltzmann → RBM** forms the correct intellectual lineage.  \n",
        "- **EA/SK models**: Spin glass perspective (disorder, multiple minima).  \n",
        "- **Hopfield nets**: Deterministic associative memory (SK-inspired).  \n",
        "- **Boltzmann & RBMs**: Stochastic learning extensions, foundational for modern deep learning.  \n"
      ],
      "metadata": {
        "id": "7MK9msVOqyM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Physics to AI: The Lineage of Spin Glasses and Neural Networks\n",
        "\n",
        "---\n",
        "\n",
        "## The Physicists Behind the Names\n",
        "\n",
        "**Ludwig Eduard Boltzmann (1844–1906)**  \n",
        "- Austrian physicist, founder of **statistical mechanics**.  \n",
        "- Introduced the **Boltzmann constant** and **Boltzmann distribution**.  \n",
        "- His ideas on thermal equilibrium inspired **Hinton & Sejnowski** to name the *Boltzmann Machine* (1985).  \n",
        "\n",
        "**Samuel F. Edwards (1930–2015) & Philip W. Anderson (1923–2020)**  \n",
        "- Developed the **Edwards–Anderson (EA) spin glass model** (1975).  \n",
        "- Extended the **Ising model** to include *random, frustrated interactions*.  \n",
        "- Revealed the existence of **spin glass phases** with rugged landscapes.  \n",
        "- Anderson won the **1977 Nobel Prize in Physics** for his work on disordered systems.  \n",
        "\n",
        " **Clarification**:  \n",
        "- *Boltzmann Machines* → named after **Boltzmann**.  \n",
        "- *Edwards–Anderson Model* → named after **Edwards & Anderson**.  \n",
        "- Despite “Eduard” vs “Edwards,” these are **different scientists** with no relation.  \n",
        "\n",
        "---\n",
        "\n",
        "## The Intellectual Lineage of Models\n",
        "\n",
        "### 1. Ising Model (1920s)  \n",
        "- Binary spins \\( S_i \\in \\{+1, -1\\} \\) with **nearest-neighbor interactions**.  \n",
        "- First model of **cooperative phenomena** and **phase transitions**.  \n",
        "- **Hamiltonian**:  \n",
        "$$\n",
        "H = - \\sum_{\\langle i j \\rangle} J_{ij} S_i S_j\n",
        "$$  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. Edwards–Anderson (EA) Model (1975)  \n",
        "- A **disordered Ising model** with random couplings \\( J_{ij} \\).  \n",
        "- Introduced the **overlap parameter** \\( q \\) to capture memory-like frozen states.  \n",
        "- Established the concept of **spin glasses**.  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. Sherrington–Kirkpatrick (SK) Model (1975)  \n",
        "- Infinite-range (mean-field) extension of EA: *every spin interacts with every other spin*.  \n",
        "- Produced a **rugged, hierarchical energy landscape**.  \n",
        "- Solved by **Parisi** with **Replica Symmetry Breaking (RSB)**.  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. Hopfield Network (1982)  \n",
        "- *John Hopfield* applied SK mathematics to **associative memory**.  \n",
        "- Mapping: *Spins ↔ Neurons, Couplings ↔ Synaptic weights*.  \n",
        "- **Energy function identical** to Ising/SK Hamiltonian.  \n",
        "- **Stored patterns = attractors** in the energy landscape.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5. Boltzmann Machine (1985)  \n",
        "- *Hinton & Sejnowski* extended Hopfield nets.  \n",
        "- Added **stochastic binary units** with the **Boltzmann distribution**.  \n",
        "- Allowed **learning** through contrastive phases (clamped vs free).  \n",
        "- Considered a **stochastic Ising model with learning**.  \n",
        "\n",
        "---\n",
        "\n",
        "### 6. Restricted Boltzmann Machine (RBM)  \n",
        "- *Paul Smolensky (1986)* → proposed as “Harmonium.”  \n",
        "- Bipartite structure: **Visible ↔ Hidden**, no intra-layer connections.  \n",
        "- Efficient training via **Contrastive Divergence (Hinton, 2002)**.  \n",
        "- Became the foundation of **Deep Belief Networks (2006)** and the **deep learning revival**.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Unified Conclusion\n",
        "\n",
        "- **Ising model** → foundation of energy-based binary systems.  \n",
        "- **EA/SK models** → added disorder and frustration, creating multiple attractor states.  \n",
        "- **Hopfield networks** → applied SK theory to associative memory in AI.  \n",
        "- **Boltzmann Machines** → introduced stochasticity and learnable probabilities, named after Boltzmann.  \n",
        "- **RBMs** → computationally feasible, enabled **DBNs** and modern deep learning.  \n",
        "\n",
        " **In short:**  \n",
        "**Ising → EA → SK → Hopfield → Boltzmann → RBM → Deep Learning.**  \n",
        "\n",
        "Each step brought us closer to today’s neural architectures, with names tracing back to *Boltzmann, Edwards, and Anderson* — different scientists across different eras.  \n"
      ],
      "metadata": {
        "id": "Vk8P0xEaq83K"
      }
    }
  ]
}