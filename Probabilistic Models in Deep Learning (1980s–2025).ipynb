{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNFQ+MkCOEna6+lRBPruJXC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ“œ Probabilistic Models in Deep Learning (1980sâ€“2025)\n","\n","---\n","\n","## ðŸ“š Key Milestones\n","\n","| **Era** | **Model / Concept** | **Year** | **Authors / Org** | **Key Contributions** |\n","|---------|----------------------|----------|-------------------|-----------------------|\n","| **Foundations** | **Boltzmann Machines** | 1985 | Ackley, Hinton & Sejnowski | Stochastic neural networks modeling probability distributions via energy functions. |\n","| | **Restricted Boltzmann Machines (RBM)** | 1986 | Smolensky | Simplified BM with bipartite structure; building block for later deep models. |\n","| **Probabilistic Graphical Models + Neural Nets** | **Bayesian Networks & PGMs** | 1988â€“1990s | Judea Pearl; Michael Jordan | Probabilistic reasoning frameworks, inspired structured latent-variable models in deep learning. |\n","| | **Helmholtz Machine** | 1995 | Dayan, Hinton, Neal & Zemel | Variational inference with recognition + generative networks; precursor to VAEs. |\n","| **Deep Probabilistic Models** | **Deep Belief Nets (DBN)** | 2006 | Hinton, Osindero & Teh | Stacked RBMs, probabilistic pretraining â†’ helped spark deep learning revival. |\n","| | **Variational Autoencoder (VAE)** | 2013 | Kingma & Welling | First modern probabilistic deep generative model; variational inference + reparameterization trick. |\n","| | **Bayesian Deep Learning** | 2016 | Gal & Ghahramani | Showed dropout â‰ˆ Bayesian inference; advanced model uncertainty in DL. |\n","| **Normalizing Flows (Exact Likelihood)** | **NICE** | 2014 | Dinh, Krueger & Bengio | Introduced invertible flows for tractable likelihood. |\n","| | **RealNVP** | 2016 | Dinh, Sohl-Dickstein & Bengio | Affine coupling layers; more expressive flow models. |\n","| | **Glow** | 2018 | Kingma & Dhariwal (OpenAI) | Invertible 1Ã—1 convolutions; scalable flow-based generative models. |\n","| **Diffusion Probabilistic Models** | **DPMs** | 2015 | Sohl-Dickstein et al. | Forward diffusion (noise) + learned reverse denoising. |\n","| | **DDPM** | 2020 | Ho, Jain & Abbeel (Google Brain) | Stable training, high-quality samples; landmark in modern generative models. |\n","| **Modern Probabilistic Foundations** | **Bayesian Neural Networks (BNNs)** | 1990sâ€“2020s | Neal, MacKay, Blundell, et al. | Priors/posteriors over weights; inference via VI or Monte Carlo. |\n","| | **Probabilistic Programming (Pyro, TFP)** | 2017â€“2018 | Uber AI; Google | Frameworks unifying probabilistic modeling + deep learning. |\n","\n","---\n","\n","## âœ… Summary Families\n","- **Energy-based probabilistic models:** Boltzmann Machines (1985), RBMs (1986), DBNs (2006).  \n","- **Latent-variable generative models:** Helmholtz Machine (1995), VAEs (2013).  \n","- **Exact likelihood models (flows):** NICE (2014), RealNVP (2016), Glow (2018).  \n","- **Stochastic generative processes:** Diffusion Models â†’ DPMs (2015), DDPM (2020).  \n","- **Bayesian neural nets:** Approximate inference in deep nets (1990sâ€“2020s).  \n"],"metadata":{"id":"1hkViVdL7rf0"}}]}