{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhObbWamjq4SabURotNL7z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ“œ CNN Breakthroughs in Deep Learning (1980sâ€“2025)\n","\n","---\n","\n","## ðŸ“š Key Milestones\n","\n","| **Era** | **Model** | **Year** | **Authors / Org** | **Key Contributions** |\n","|---------|-----------|----------|-------------------|-----------------------|\n","| **Foundations & Precursors** | **Neocognitron** | 1980 | Kunihiko Fukushima | First hierarchical model with convolution + pooling, inspired by the visual cortex. |\n","| **First Practical CNNs** | **LeNet-5** | 1998 | LeCun, Bottou, Bengio & Haffner (AT&T Labs) | End-to-end CNN for handwriting recognition; deployed in ATMs for reading checks. |\n","| **Revival of Deep CNNs** | **AlexNet** | 2012 | Krizhevsky, Sutskever & Hinton (Toronto) | 8-layer CNN, GPU-trained, won ImageNet 2012 by a large margin â†’ sparked deep learning boom. |\n","| **Scaling Up CNNs** | **ZFNet** | 2013 | Zeiler & Fergus | Used deconvolution to visualize features, optimized AlexNet filters. |\n","| | **VGGNet** | 2014 | Simonyan & Zisserman (Oxford) | Very deep networks (16â€“19 layers) with small 3Ã—3 filters; simple yet powerful design. |\n","| | **GoogLeNet (Inception v1)** | 2014 | Szegedy et al. (Google) | Introduced inception modules, 22 layers deep but computationally efficient. |\n","| **Ultra-Deep CNNs & Residual Learning** | **ResNet** | 2015 | He et al. (Microsoft Research) | Introduced residual blocks; enabled 152-layer networks; won ImageNet 2015. |\n","| | **DenseNet** | 2016 | Huang et al. | Densely connected layers â†’ improved gradient flow and parameter efficiency. |\n","| **Efficiency & Real-World CNNs** | **MobileNet** | 2017 | Howard et al. (Google) | Depthwise separable convolutions for efficient mobile/edge deployment. |\n","| | **EfficientNet** | 2019 | Tan & Le (Google) | Compound scaling rule (depth, width, resolution); SOTA accuracyâ€“efficiency tradeoff. |\n","| **Hybrid & Modern CNNs** | **RegNet** | 2020 | Radosavovic et al. (Facebook AI) | Introduced design spaces for scalable, efficient CNN families. |\n","| | **ConvNeXt** | 2022 | Liu et al. (Facebook AI) | Modernized CNN design to rival Transformers; proved CNNs remain competitive. |\n","\n","---\n","\n","## âœ… Summary Families\n","- **Early foundations:** Neocognitron (1980), LeNet-5 (1998).  \n","- **Deep CNN revolution:** AlexNet (2012), ZFNet (2013), VGGNet (2014), GoogLeNet (2014).  \n","- **Ultra-deep scaling:** ResNet (2015), DenseNet (2016).  \n","- **Efficiency focus:** MobileNet (2017), EfficientNet (2019).  \n","- **Modern CNNs:** RegNet (2020), ConvNeXt (2022).  \n"],"metadata":{"id":"9eVCRwFM5FVh"}}]}