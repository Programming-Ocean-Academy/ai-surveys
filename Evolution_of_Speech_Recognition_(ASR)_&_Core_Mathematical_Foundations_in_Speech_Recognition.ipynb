{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evolution of Speech Recognition (ASR)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Acoustic → Text Conversion (Signal → Symbols)\n",
        "\n",
        "**Input:** Raw audio waveform  \n",
        "**Goal:** Map continuous acoustic signals → discrete textual units (phonemes, subwords, words)\n",
        "\n",
        "### Techniques\n",
        "\n",
        "- **Signal processing (early era):** MFCCs, spectrograms  \n",
        "- **HMM/GMM era:** Statistical mapping of frames to phonemes  \n",
        "  - Hidden Markov Models:  \n",
        "    $$\n",
        "    P(O,S) = \\pi_{s_1} \\prod_{t=2}^{T} a_{s_{t-1}, s_t} \\prod_{t=1}^{T} b_{s_t}(o_t)\n",
        "    $$\n",
        "  - Gaussian Mixture Models:  \n",
        "    $$\n",
        "    P(x) = \\sum_{m=1}^{M} c_m \\, \\mathcal{N}(x \\mid \\mu_m, \\Sigma_m)\n",
        "    $$\n",
        "\n",
        "- **Neural acoustic models:** CNNs, RNNs, GRUs, LSTMs replacing GMMs  \n",
        "- **End-to-end models:**\n",
        "  - **CTC:** Connectionist Temporal Classification  \n",
        "    $$\n",
        "    P(y \\mid x) = \\sum_{\\pi \\in B^{-1}(y)} \\prod_{t=1}^T P(\\pi_t \\mid x_t)\n",
        "    $$\n",
        "  - **Attention-based Seq2Seq:** Aligns acoustic frames with output tokens  \n",
        "  - **Transformers (Conformer, Whisper):** Direct mapping from features → text without hand-crafted phoneme models  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Text → Understanding (NLP on Recognized Text)\n",
        "\n",
        "**Input:** Transcribed text from ASR  \n",
        "**Goal:** Extract meaning, context, and semantic structure  \n",
        "\n",
        "### Techniques\n",
        "\n",
        "- **Statistical Language Models (LMs):**  \n",
        "  $$\n",
        "  P(W) \\approx \\prod_{t=1}^T P(w_t \\mid w_{t-n+1}, \\dots, w_{t-1})\n",
        "  $$\n",
        "\n",
        "- **Neural sequence models:** RNN, LSTM, GRU  \n",
        "- **Attention & Transformers:**\n",
        "  - **BERT:** Deep bidirectional contextual embeddings  \n",
        "  - **GPT-family:** Autoregressive large language models  \n",
        "  - **Whisper:** Joint ASR + NLP end-to-end system  \n",
        "\n",
        "---\n",
        "\n",
        "##  Summary\n",
        "\n",
        "- **ASR = Audio → Text**\n",
        "  - Old way: HMM/GMM  \n",
        "  - Modern way: Neural acoustic models + CTC / Attention / Transformers  \n",
        "\n",
        "- **NLP = Text → Meaning**\n",
        "  - Old way: n-gram statistical models  \n",
        "  - Modern way: RNNs, LSTMs, Transformers (BERT, GPT, etc.)  \n",
        "\n",
        "---\n",
        "\n",
        "##  Integration in Modern Systems\n",
        "\n",
        "In classic pipelines, ASR and NLP were separate:  \n",
        "1. **ASR:** Convert audio → text  \n",
        "2. **NLP:** Process text for meaning  \n",
        "\n",
        "In **modern end-to-end architectures** (DeepSpeech, Conformer, Whisper), the acoustic and language modeling are **trained jointly** within a single neural system, blurring the line between ASR and NLP.\n"
      ],
      "metadata": {
        "id": "ysk3CShsxw0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Core Mathematical Foundations in Speech Recognition\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Acoustic Modeling with Hidden Markov Models (HMMs)\n",
        "\n",
        "Speech is modeled as a sequence of hidden states (phonemes) generating observable acoustic signals.\n",
        "\n",
        "**Markov assumption:**\n",
        "\n",
        "$$\n",
        "P(s_t \\mid s_1, s_2, \\dots, s_{t-1}) \\approx P(s_t \\mid s_{t-1})\n",
        "$$\n",
        "\n",
        "**Observation likelihood:**\n",
        "\n",
        "$$\n",
        "P(O \\mid S) = \\prod_{t=1}^{T} P(o_t \\mid s_t)\n",
        "$$\n",
        "\n",
        "**Total probability (joint):**\n",
        "\n",
        "$$\n",
        "P(O, S) = \\pi_{s_1} \\prod_{t=2}^{T} a_{s_{t-1}, s_t} \\prod_{t=1}^{T} b_{s_t}(o_t)\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- \\( \\pi_{s_1} \\) = initial state distribution  \n",
        "- \\( a_{ij} \\) = transition probability between states  \n",
        "- \\( b_j(o_t) \\) = emission probability  \n",
        "\n",
        "**Key algorithms:**  \n",
        "- Forward algorithm: Computes likelihood efficiently.  \n",
        "- Viterbi algorithm (1973): Finds most likely state sequence.  \n",
        "- Baum–Welch (EM algorithm): Learns transition and emission probabilities.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Probabilistic Acoustic Features\n",
        "\n",
        "Speech features = continuous signals → modeled by Gaussian Mixture Models (GMMs).  \n",
        "\n",
        "**Likelihood of acoustic vector \\(x\\):**\n",
        "\n",
        "$$\n",
        "P(x) = \\sum_{m=1}^{M} c_m \\, \\mathcal{N}(x \\mid \\mu_m, \\Sigma_m)\n",
        "$$\n",
        "\n",
        "where \\( \\mathcal{N} \\) is a Gaussian density with mean \\( \\mu_m \\) and covariance \\( \\Sigma_m \\).  \n",
        "\n",
        "Later, GMMs were replaced by neural networks estimating posterior probabilities.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Bayesian Decoding (Speech as Noisy Channel)\n",
        "\n",
        "**Central ASR decoding formula:**\n",
        "\n",
        "$$\n",
        "\\hat{W} = \\arg\\max_W P(W \\mid O) = \\arg\\max_W P(O \\mid W) P(W)\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- \\( W \\) = word sequence  \n",
        "- \\( O \\) = observed acoustic sequence  \n",
        "\n",
        "This separates acoustic model \\( P(O \\mid W) \\) and language model \\( P(W) \\).  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Language Models (LMs)\n",
        "\n",
        "**n-gram models:**\n",
        "\n",
        "$$\n",
        "P(W) \\approx \\prod_{t=1}^T P(w_t \\mid w_{t-1}, \\dots, w_{t-n+1})\n",
        "$$\n",
        "\n",
        "Estimated with Maximum Likelihood Estimation (MLE) and smoothing.  \n",
        "\n",
        "**Neural LMs:** (feedforward, RNNs, Transformers) approximate the same probability via embeddings + parameterized networks.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Neural Network Acoustic Models\n",
        "\n",
        "**Connectionist models (late 1980s, 1990s):**  \n",
        "Feedforward NN outputs posterior over states.  \n",
        "\n",
        "**Cross-entropy training:**\n",
        "\n",
        "$$\n",
        "L = - \\sum_{t=1}^{T} \\sum_i y_{t,i} \\log \\hat{y}_{t,i}\n",
        "$$\n",
        "\n",
        "**Recurrent neural networks (RNN, LSTM, GRU):**\n",
        "\n",
        "$$\n",
        "h_t = f(W_{xh} x_t + W_{hh} h_{t-1})\n",
        "$$\n",
        "\n",
        "**CTC (Connectionist Temporal Classification, 2006):**  \n",
        "\n",
        "Loss function for unaligned sequences:\n",
        "\n",
        "$$\n",
        "P(y \\mid x) = \\sum_{\\pi \\in B^{-1}(y)} \\prod_{t=1}^{T} P(\\pi_t \\mid x_t)\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- \\( \\pi \\) = alignment paths  \n",
        "- \\( B \\) = collapsing function (merging repeats and blanks)  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. Attention and Transformers in ASR\n",
        "\n",
        "**Seq2Seq with Attention:**\n",
        "\n",
        "$$\n",
        "\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_j \\exp(e_{t,j})}, \\quad\n",
        "e_{t,i} = v^T \\tanh(W_1 h_i + W_2 s_{t-1})\n",
        "$$\n",
        "\n",
        "**Context vector:**\n",
        "\n",
        "$$\n",
        "c_t = \\sum_i \\alpha_{t,i} h_i\n",
        "$$\n",
        "\n",
        "**Transformer (Vaswani 2017) self-attention:**\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
        "$$\n",
        "\n",
        "Used in Conformer and Whisper models for end-to-end ASR.  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Statistical and Physics-Inspired Links\n",
        "\n",
        "- **Ising / Hopfield nets:** Associative memory → robust recall with noisy input.  \n",
        "- **Boltzmann Machines / RBMs:** Early generative pretraining (2000s, e.g. DBNs for phoneme recognition).  \n",
        "- **Spin glass mathematics (EA/SK models):** Framework for rugged probability landscapes, analogous to speech state decoding with HMMs.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Summary of Key Equations in Speech Recognition\n",
        "\n",
        "**Markov chain probability:**  \n",
        "\n",
        "$$\n",
        "P(s_t \\mid s_{t-1})\n",
        "$$\n",
        "\n",
        "**Joint HMM probability:**  \n",
        "\n",
        "$$\n",
        "P(O, S) = \\pi_{s_1} \\prod a_{ij} b_j(o_t)\n",
        "$$\n",
        "\n",
        "**GMM acoustic model:**  \n",
        "\n",
        "$$\n",
        "P(x) = \\sum c_m \\, \\mathcal{N}(x \\mid \\mu_m, \\Sigma_m)\n",
        "$$\n",
        "\n",
        "**Bayesian decoding (noisy channel):**  \n",
        "\n",
        "$$\n",
        "\\hat{W} = \\arg\\max_W P(O \\mid W) P(W)\n",
        "$$\n",
        "\n",
        "**n-gram LM:**  \n",
        "\n",
        "$$\n",
        "P(W) \\approx \\prod P(w_t \\mid w_{t-n+1}^{t-1})\n",
        "$$\n",
        "\n",
        "**NN cross-entropy loss:**  \n",
        "\n",
        "$$\n",
        "L = - \\sum y \\log \\hat{y}\n",
        "$$\n",
        "\n",
        "**CTC loss:**  \n",
        "\n",
        "$$\n",
        "P(y \\mid x) = \\sum_{\\pi \\in B^{-1}(y)} \\prod P(\\pi_t \\mid x_t)\n",
        "$$\n",
        "\n",
        "**Attention weights:**  \n",
        "\n",
        "$$\n",
        "\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_j \\exp(e_{t,j})}\n",
        "$$\n",
        "\n",
        "**Transformer self-attention:**  \n",
        "\n",
        "$$\n",
        "\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        " **In other words:**  \n",
        "- Early ASR = Markov + Gaussian statistics (HMM/GMM).  \n",
        "- Middle era = Neural + statistical physics (RBM, DBN).  \n",
        "- Modern era = Deep end-to-end models (CTC, Attention, Transformer).  \n"
      ],
      "metadata": {
        "id": "BpL7KH7xxbru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Landmark Papers in Speech Recognition (ASR)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Foundations (1950s–1970s)\n",
        "\n",
        "- **Baum, L. E. & Eagon, J. A. (1967).**  \n",
        "  *An inequality with applications to statistical estimation for probabilistic functions of Markov processes.*  \n",
        "  → Mathematical basis for Hidden Markov Models (HMMs).\n",
        "\n",
        "- **Forney, G. D. (1973).**  \n",
        "  *The Viterbi Algorithm.*  \n",
        "  → Introduced Viterbi decoding, crucial for sequence alignment in HMM-based ASR.\n",
        "\n",
        "- **Baker, J. K. (1975).**  \n",
        "  *The DRAGON system — An overview.*  \n",
        "  → First large-scale speech recognition system using HMMs.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Statistical HMM/GMM Era (1980s–1990s)\n",
        "\n",
        "- **Bahl, L. R., Jelinek, F., & Mercer, R. L. (1983).**  \n",
        "  *A maximum likelihood approach to continuous speech recognition.*  \n",
        "  → Pioneering statistical ASR with n-gram language models.\n",
        "\n",
        "- **Jelinek, F. (1985).**  \n",
        "  *Markov source modeling of text generation.*  \n",
        "  → Advanced statistical language modeling for ASR.\n",
        "\n",
        "- **Rabiner, L. R. (1989).**  \n",
        "  *A tutorial on Hidden Markov Models and selected applications in speech recognition.*  \n",
        "  → The canonical tutorial on HMMs in ASR.\n",
        "\n",
        "- **Young, S., et al. (1993–1995).**  \n",
        "  *The HTK Book.*  \n",
        "  → Toolkit that standardized HMM-based ASR research and practice.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Neural Network Era (1990s–2000s)\n",
        "\n",
        "- **Bourlard, H., & Morgan, N. (1994).**  \n",
        "  *Connectionist Speech Recognition: A Hybrid Approach.*  \n",
        "  → Early integration of neural networks with HMMs.\n",
        "\n",
        "- **Bengio, Y., et al. (1994).**  \n",
        "  *Learning long-term dependencies with gradient descent is difficult.*  \n",
        "  → Identified vanishing gradients, motivating LSTM development.\n",
        "\n",
        "- **Hochreiter, S., & Schmidhuber, J. (1997).**  \n",
        "  *Long Short-Term Memory.*  \n",
        "  → Introduced LSTM, later a cornerstone in ASR sequence modeling.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Deep Learning Revolution (2010s)\n",
        "\n",
        "- **Graves, A., Fernández, S., Gomez, F., & Schmidhuber, J. (2006).**  \n",
        "  *Connectionist Temporal Classification (CTC).*  \n",
        "  → Enabled end-to-end sequence training without explicit alignment.\n",
        "\n",
        "- **Mohamed, A., Dahl, G. E., & Hinton, G. (2012).**  \n",
        "  *Acoustic modeling using deep belief networks.*  \n",
        "  → Introduced deep neural networks (DNNs) to ASR.\n",
        "\n",
        "- **Graves, A., Mohamed, A., & Hinton, G. (2013).**  \n",
        "  *Speech recognition with deep recurrent neural networks.*  \n",
        "  → First successful RNN-based ASR system.\n",
        "\n",
        "- **Chorowski, J., Bahdanau, D., Serdyuk, D., Cho, K., & Bengio, Y. (2015).**  \n",
        "  *Attention-based models for speech recognition.*  \n",
        "  → Brought attention mechanisms into ASR.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. End-to-End & Transformer Era (2016–Present)\n",
        "\n",
        "- **Chan, W., Jaitly, N., Le, Q. V., & Vinyals, O. (2016).**  \n",
        "  *Listen, Attend and Spell (LAS).*  \n",
        "  → End-to-end attention-based ASR.\n",
        "\n",
        "- **Amodei, D., et al. (2016).**  \n",
        "  *Deep Speech 2: End-to-End Speech Recognition in English and Mandarin.*  \n",
        "  → Large-scale RNN-based end-to-end ASR from Baidu.\n",
        "\n",
        "- **Vaswani, A., et al. (2017).**  \n",
        "  *Attention Is All You Need.*  \n",
        "  → Introduced the Transformer; later adopted in ASR.\n",
        "\n",
        "- **Gulati, A., et al. (2020).**  \n",
        "  *Conformer: Convolution-augmented Transformer for Speech Recognition.*  \n",
        "  → State-of-the-art acoustic modeling with Transformer + CNN synergy.\n",
        "\n",
        "- **Radford, A., et al. (2023).**  \n",
        "  *Whisper: Robust Speech Recognition via Large-Scale Weak Supervision.*  \n",
        "  → OpenAI’s multilingual, multitask Transformer-based ASR model.\n",
        "\n",
        "---\n",
        "\n",
        "##  Conclusion\n",
        "\n",
        "- **HMM/GMM statistical era:** Baum, Forney, Rabiner → foundations of probabilistic ASR.  \n",
        "- **Neural hybrid era:** Bourlard, Morgan, Hochreiter → neural nets + HMMs, LSTM breakthroughs.  \n",
        "- **Deep learning revolution:** Hinton, Graves, Mohamed → DNNs, RNNs, CTC.  \n",
        "- **End-to-end era:** LAS, DeepSpeech, Conformer, Whisper → attention & Transformer architectures dominating modern ASR.\n"
      ],
      "metadata": {
        "id": "78QZl4pXzZlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Important Real-World Projects in Speech Recognition\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Early Pioneering Systems (1950s–1970s)\n",
        "\n",
        "- **Audrey (Bell Labs, 1952)**  \n",
        "  Recognized digits 0–9 from a single speaker.  \n",
        "  ➝ First working ASR prototype.\n",
        "\n",
        "- **Harpy (CMU, 1976)**  \n",
        "  Recognized ~1,000 words using a finite-state network.  \n",
        "  ➝ Introduced **beam search decoding** → still core in ASR today.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Statistical HMM-Based Systems (1980s–1990s)\n",
        "\n",
        "- **DRAGON (1975–1980s, CMU / Dragon Systems)**  \n",
        "  First large-vocabulary dictation system.  \n",
        "  ➝ Evolved into **Dragon NaturallySpeaking (1997)** → commercial success.\n",
        "\n",
        "- **IBM Tangora (1980s)**  \n",
        "  Large-vocabulary continuous speech recognition (20,000 words).  \n",
        "  ➝ Based on **Hidden Markov Models (HMMs).**\n",
        "\n",
        "- **AT&T Voice Recognition Call Routing (1990s)**  \n",
        "  Deployed in call centers for IVR.  \n",
        "  ➝ First **commercial speech-enabled customer service.**\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Consumer Applications Begin (2000s)\n",
        "\n",
        "- **Nuance Dragon NaturallySpeaking (1997 → 2000s)**  \n",
        "  Dictation software, widely used in law, medicine, transcription.\n",
        "\n",
        "- **Google Voice Search (2008)**  \n",
        "  First **cloud-based ASR** for consumers.  \n",
        "  ➝ Demonstrated power of internet-scale statistical models.\n",
        "\n",
        "- **Microsoft Cortana / Speech API (2000s)**  \n",
        "  Integrated ASR into **Windows, Xbox, enterprise tools.**\n",
        "\n",
        "- **Apple Siri (2011)**  \n",
        "  First **mainstream voice assistant**, combining ASR + NLP.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Deep Learning Deployment (2010s)\n",
        "\n",
        "- **Google Voice Search / Google Now (2012)**  \n",
        "  Replaced GMMs with **deep neural networks (DNNs).**  \n",
        "  ➝ Achieved ≈30% error reduction.\n",
        "\n",
        "- **Baidu DeepSpeech (2014, 2016)**  \n",
        "  End-to-end **RNN-based ASR.**  \n",
        "  ➝ Deployed in Mandarin and English at scale.\n",
        "\n",
        "- **Amazon Alexa (2014)**  \n",
        "  Smart speaker ecosystem.  \n",
        "  ➝ Used **LSTM acoustic models + large-scale LMs.**\n",
        "\n",
        "- **Microsoft Skype Translator (2014)**  \n",
        "  Real-time **speech-to-speech translation** with ASR + MT + TTS.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Transformer and End-to-End Era (2020s)\n",
        "\n",
        "- **Google Assistant (2016 → now)**  \n",
        "  Progressed from **RNN-T** to **Transformer/Conformer models.**  \n",
        "  ➝ Real-time, multilingual ASR.\n",
        "\n",
        "- **Apple Siri (updated, 2021)**  \n",
        "  On-device ASR with **Transformer models.**  \n",
        "  ➝ Privacy-preserving, low-latency inference.\n",
        "\n",
        "- **Meta AI – wav2vec 2.0 (2020)**  \n",
        "  Self-supervised ASR.  \n",
        "  ➝ Deployed in **low-resource languages.**\n",
        "\n",
        "- **OpenAI Whisper (2022)**  \n",
        "  Multilingual, multitask **Transformer-based ASR** trained on 680k hours.  \n",
        "  ➝ Open-sourced, widely used for **transcription & subtitling.**\n",
        "\n",
        "- **Anthropic, Microsoft, Google Cloud Speech-to-Text APIs (2020s)**  \n",
        "  Industrial-grade ASR deployed as **cloud services.**\n",
        "\n",
        "---\n",
        "\n",
        "##  Conclusion\n",
        "\n",
        "- **Prototypes (1950s–70s):** Audrey, Harpy.  \n",
        "- **HMM Era (1980s–90s):** DRAGON, IBM Tangora, AT&T IVR.  \n",
        "- **Consumer Cloud Era (2000s):** Dragon, Google Voice Search, Siri, Cortana.  \n",
        "- **Deep Learning Breakthrough (2010s):** Google DNNs, Baidu DeepSpeech, Alexa.  \n",
        "- **Transformer Era (2020s):** Google Assistant (Conformer), Siri (on-device Transformer), OpenAI Whisper (multilingual end-to-end).\n"
      ],
      "metadata": {
        "id": "KW8s4dGAz2QV"
      }
    }
  ]
}