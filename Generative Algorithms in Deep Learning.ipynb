{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyODJem9/AUhXJSrzAUNPnTY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 📜 Generative Algorithms in Deep Learning\n","\n","---\n","\n","## 🔹 Energy-Based Models (Precursors)\n","- **Boltzmann Machines** – Ackley, Hinton & Sejnowski (1985)  \n","  *“A Learning Algorithm for Boltzmann Machines.”* Cognitive Science, 1985.  \n","\n","- **Restricted Boltzmann Machines (RBM)** – Smolensky (1986)  \n","  *“Information Processing in Dynamical Systems: Foundations of Harmony Theory.”*  \n","\n","---\n","\n","## 🔹 Autoencoders & Variants\n","- **Autoencoder (basic)** – Rumelhart, Hinton & Williams (1986)  \n","  *“Learning Representations by Back-Propagating Errors.”*  \n","\n","- **Deep Autoencoder** – Hinton & Salakhutdinov (2006)  \n","  *“Reducing the Dimensionality of Data with Neural Networks.”* Science, 2006.  \n","\n","- **Variational Autoencoder (VAE)** – Kingma & Welling (2013)  \n","  *“Auto-Encoding Variational Bayes.”* arXiv:1312.6114.  \n","\n","- **β-VAE** – Higgins et al. (2017, DeepMind)  \n","  *“beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.”*  \n","\n","- **VQ-VAE** – van den Oord, Vinyals & Kavukcuoglu (2017)  \n","  *“Neural Discrete Representation Learning.”*  \n","\n","---\n","\n","## 🔹 Autoregressive Models\n","- **Neural Probabilistic Language Model** – Bengio et al. (2003)  \n","  *“A Neural Probabilistic Language Model.”* JMLR, 2003.  \n","\n","- **NADE (Neural Autoregressive Distribution Estimator)** – Larochelle & Murray (2011)  \n","  *“The Neural Autoregressive Distribution Estimator.”* AISTATS 2011.  \n","\n","- **PixelRNN / PixelCNN** – van den Oord et al. (2016, DeepMind)  \n","  *“Pixel Recurrent Neural Networks.”* ICML 2016.  \n","  *“Conditional Image Generation with PixelCNN Decoders.”* NeurIPS 2016.  \n","\n","---\n","\n","## 🔹 GAN Family\n","- **GAN (Generative Adversarial Network)** – Goodfellow et al. (2014)  \n","  *“Generative Adversarial Nets.”* NeurIPS 2014.  \n","\n","- **DCGAN (Deep Convolutional GAN)** – Radford, Metz & Chintala (2015)  \n","  *“Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.”*  \n","\n","- **WGAN (Wasserstein GAN)** – Arjovsky, Chintala & Bottou (2017)  \n","  *“Wasserstein GAN.”* ICML 2017.  \n","\n","- **StyleGAN** – Karras et al. (2018, NVIDIA)  \n","  *“A Style-Based Generator Architecture for Generative Adversarial Networks.”* CVPR 2019.  \n","\n","---\n","\n","## 🔹 Flow-Based Models\n","- **NICE (Nonlinear Independent Components Estimation)** – Dinh, Krueger & Bengio (2014)  \n","  *“NICE: Non-linear Independent Components Estimation.”* arXiv:1410.8516.  \n","\n","- **Real NVP** – Dinh, Sohl-Dickstein & Bengio (2016)  \n","  *“Density Estimation using Real NVP.”* ICLR 2017.  \n","\n","- **Glow** – Kingma & Dhariwal (2018, OpenAI)  \n","  *“Glow: Generative Flow with Invertible 1×1 Convolutions.”* NeurIPS 2018.  \n","\n","---\n","\n","## 🔹 Diffusion Models\n","- **Diffusion Probabilistic Models (DPMs)** – Sohl-Dickstein et al. (2015)  \n","  *“Deep Unsupervised Learning using Nonequilibrium Thermodynamics.”* ICML 2015.  \n","\n","- **DDPM (Denoising Diffusion Probabilistic Models)** – Ho, Jain & Abbeel (2020, Google Brain)  \n","  *“Denoising Diffusion Probabilistic Models.”* NeurIPS 2020.  \n","\n","- **DDIM** – Song, Meng & Ermon (2020)  \n","  *“Denoising Diffusion Implicit Models.”* ICLR 2021.  \n","\n","- **Latent Diffusion / Stable Diffusion** – Rombach et al. (2022)  \n","  *“High-Resolution Image Synthesis with Latent Diffusion Models.”* CVPR 2022.  \n","\n","---\n","\n","## ✅ Summary\n","- **Autoencoders:** AE (1986) → Deep AE (2006) → VAE (2013) → VQ-VAE (2017).  \n","- **Autoregressive:** Bengio’s NLM (2003) → NADE (2011) → PixelRNN/PixelCNN (2016).  \n","- **GANs:** GAN (2014) → DCGAN (2015) → WGAN (2017) → StyleGAN (2018).  \n","- **Flows:** NICE (2014) → RealNVP (2016) → Glow (2018).  \n","- **Diffusions:** DPM (2015) → DDPM (2020) → DDIM (2021) → Stable Diffusion (2022).  \n"],"metadata":{"id":"JLfKnQYh2GuD"}},{"cell_type":"markdown","source":["# 📜 Generative Algorithms in Deep Learning\n","\n","| **Family** | **Model** | **Year** | **Authors** | **Paper** |\n","|------------|-----------|----------|-------------|-----------|\n","| 🔹 Energy-Based | Boltzmann Machines | 1985 | Ackley, Hinton & Sejnowski | *A Learning Algorithm for Boltzmann Machines* (Cognitive Science) |\n","| | Restricted Boltzmann Machines (RBM) | 1986 | Smolensky | *Foundations of Harmony Theory* |\n","| 🔹 Autoencoders | Autoencoder (basic) | 1986 | Rumelhart, Hinton & Williams | *Learning Representations by Back-Propagating Errors* |\n","| | Deep Autoencoder | 2006 | Hinton & Salakhutdinov | *Reducing the Dimensionality of Data with Neural Networks* (Science) |\n","| | Variational Autoencoder (VAE) | 2013 | Kingma & Welling | *Auto-Encoding Variational Bayes* |\n","| | β-VAE | 2017 | Higgins et al. (DeepMind) | *beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework* |\n","| | VQ-VAE | 2017 | van den Oord, Vinyals & Kavukcuoglu | *Neural Discrete Representation Learning* |\n","| 🔹 Autoregressive | Neural Probabilistic Language Model | 2003 | Bengio et al. | *A Neural Probabilistic Language Model* (JMLR) |\n","| | NADE (Neural Autoregressive Distribution Estimator) | 2011 | Larochelle & Murray | *The Neural Autoregressive Distribution Estimator* (AISTATS) |\n","| | PixelRNN | 2016 | van den Oord et al. (DeepMind) | *Pixel Recurrent Neural Networks* (ICML) |\n","| | PixelCNN | 2016 | van den Oord et al. (DeepMind) | *Conditional Image Generation with PixelCNN Decoders* (NeurIPS) |\n","| 🔹 GANs | GAN | 2014 | Goodfellow et al. | *Generative Adversarial Nets* (NeurIPS) |\n","| | DCGAN | 2015 | Radford, Metz & Chintala | *Unsupervised Representation Learning with DCGANs* |\n","| | WGAN | 2017 | Arjovsky, Chintala & Bottou | *Wasserstein GAN* (ICML) |\n","| | StyleGAN | 2018 | Karras et al. (NVIDIA) | *A Style-Based Generator Architecture for GANs* (CVPR 2019) |\n","| 🔹 Flow-Based | NICE | 2014 | Dinh, Krueger & Bengio | *NICE: Non-linear Independent Components Estimation* |\n","| | Real NVP | 2016 | Dinh, Sohl-Dickstein & Bengio | *Density Estimation using Real NVP* (ICLR 2017) |\n","| | Glow | 2018 | Kingma & Dhariwal (OpenAI) | *Glow: Generative Flow with Invertible 1×1 Convolutions* (NeurIPS) |\n","| 🔹 Diffusions | DPM | 2015 | Sohl-Dickstein et al. | *Deep Unsupervised Learning using Nonequilibrium Thermodynamics* (ICML) |\n","| | DDPM | 2020 | Ho, Jain & Abbeel (Google Brain) | *Denoising Diffusion Probabilistic Models* (NeurIPS) |\n","| | DDIM | 2020 | Song, Meng & Ermon | *Denoising Diffusion Implicit Models* (ICLR 2021) |\n","| | LDM / Stable Diffusion | 2022 | Rombach et al. | *High-Resolution Image Synthesis with Latent Diffusion Models* (CVPR) |\n","\n","---\n","\n","## ✅ Summary\n","- **Autoencoders:** AE (1986) → Deep AE (2006) → VAE (2013) → VQ-VAE (2017).  \n","- **Autoregressive:** NLM (2003) → NADE (2011) → PixelRNN/PixelCNN (2016).  \n","- **GANs:** GAN (2014) → DCGAN (2015) → WGAN (2017) → StyleGAN (2018).  \n","- **Flows:** NICE (2014) → RealNVP (2016) → Glow (2018).  \n","- **Diffusions:** DPM (2015) → DDPM (2020) → DDIM (2021) → LDM (2022).  \n"],"metadata":{"id":"djlvMqrN2Xmv"}}]}