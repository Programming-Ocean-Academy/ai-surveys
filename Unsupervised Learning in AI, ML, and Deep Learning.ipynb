{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMxRUCi6flT34pgkyXBJ712"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üìú Unsupervised Learning in AI, ML, and Deep Learning\n","\n","---\n","\n","## üîπ Definition\n","- **Unsupervised learning** = algorithms discover patterns, structures, or representations from **unlabeled data**.  \n","- **Goal:** Learn the underlying distribution, cluster data, or compress it.  \n","- **Key Tasks:** Clustering, dimensionality reduction, density estimation, generative modeling.  \n","\n","---\n","\n","## üîπ Unsupervised Learning in Classical ML\n","\n","Before deep nets, unsupervised AI/ML relied on **statistical methods**:\n","\n","| **Category** | **Algorithm / Concept** | **Year** | **Authors** | **Key Idea** |\n","|--------------|--------------------------|----------|-------------|--------------|\n","| **Clustering** | k-Means | 1967 | MacQueen | Iterative centroid-based clustering. |\n","| | Gaussian Mixture Models (GMMs) | 1960s | Dempster et al. (EM algorithm, 1977) | Probabilistic mixture modeling. |\n","| **Dimensionality Reduction** | PCA | 1901 | Pearson | Orthogonal projection for variance maximization. |\n","| | ICA | 1994 | Comon | Separation of independent components. |\n","| | t-SNE | 2008 | van der Maaten | Nonlinear embedding for visualization. |\n","| **Association Rules** | Apriori | 1994 | Agrawal & Srikant | Market basket analysis via frequent itemsets. |\n","\n","‚û°Ô∏è These methods dominated **exploratory data analysis** in AI/ML from the 1960s‚Äì2000s.  \n","\n","---\n","\n","## üîπ Unsupervised Learning in Deep Learning\n","\n","Deep learning extended unsupervised learning into **representation learning** and **generative modeling**:\n","\n","### 1. Autoencoders (AEs)\n","- **Early Autoencoders** ‚Äì Rumelhart, Hinton & Williams (1986).  \n","- **Deep Autoencoders** ‚Äì Hinton & Salakhutdinov (2006, *Science*): Dimensionality reduction with deep nets.  \n","\n","### 2. Probabilistic Models\n","- **Boltzmann Machines** ‚Äì Ackley, Hinton & Sejnowski (1985).  \n","- **Restricted Boltzmann Machines (RBM)** ‚Äì Smolensky (1986).  \n","- **Deep Belief Nets (DBNs)** ‚Äì Hinton, Osindero & Teh (2006): Stacked RBMs for unsupervised pretraining.  \n","\n","### 3. Generative Models\n","- **Variational Autoencoder (VAE)** ‚Äì Kingma & Welling (2013).  \n","- **GANs** ‚Äì Goodfellow et al. (2014): Adversarial training, unsupervised sample synthesis.  \n","- **Flow-based Models** ‚Äì NICE (2014), RealNVP (2016), Glow (2018).  \n","- **Diffusion Models** ‚Äì Sohl-Dickstein (2015) ‚Üí Stable Diffusion (2022).  \n","\n","### 4. Clustering & Representation Learning with Deep Nets\n","- **Deep Embedded Clustering (DEC)** ‚Äì Xie et al. (2016).  \n","- **Contrastive Learning** ‚Äì SimCLR (2020, Google Brain), MoCo (2020, Facebook AI), BYOL (2020, DeepMind).  \n","\n","---\n","\n","## üîπ Applications\n","- **Computer Vision:** Image clustering, anomaly detection, unsupervised pretraining.  \n","- **NLP:** Word embeddings (Word2Vec, 2013; FastText, 2016).  \n","- **Speech:** Self-supervised audio embeddings (Wav2Vec 2.0, 2020).  \n","- **Sciences:** Protein structure prediction (AlphaFold leverages unsupervised embeddings).  \n","\n","---\n","\n","## ‚úÖ Key Insights\n","- **Classical ML (pre-deep):** Unsupervised = clustering & dimensionality reduction.  \n","- **Deep Learning (modern):** Unsupervised = generative modeling (VAE, GAN, Diffusion) + deep representation learning.  \n","- **AI as a whole:** Unsupervised learning is **indispensable when labels are scarce** ‚Üí it underpins **self-supervised methods** and today‚Äôs **foundation models**.  \n"],"metadata":{"id":"zf5RTeM8D5Il"}}]}