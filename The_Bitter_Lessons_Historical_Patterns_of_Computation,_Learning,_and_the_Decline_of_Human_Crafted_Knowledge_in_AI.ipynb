{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Bitter Lesson\n",
        "\n",
        "# https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf\n",
        "\n",
        "# Learning the Bitter Lesson: Empirical Evidence from 20 Years of CVPR Proceedings\n",
        "\n",
        "# https://arxiv.org/html/2410.09649v1\n"
      ],
      "metadata": {
        "id": "EdsKYYI4ax3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📊 Comparison: Feature Engineering vs. Network Engineering\n",
        "\n",
        "| **Aspect**            | **Feature Engineering**                                                                 | **Network Engineering**                                                                 |\n",
        "|------------------------|------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|\n",
        "| **Definition**         | Manually designing, selecting, and transforming input variables to improve model performance. | Designing and optimizing neural network architectures (layers, connections, modules) to learn features automatically. |\n",
        "| **Era of Dominance**   | Pre-deep learning era (1950s–2010s), especially with classical ML models (SVMs, logistic regression, decision trees). | Deep learning era (2010s–present), where models learn features end-to-end. |\n",
        "| **Who Designs Features** | Human experts (domain knowledge crucial).                                               | Neural networks (architectures + training automatically discover features). |\n",
        "| **Techniques**         | PCA, normalization, polynomial features, embeddings, handcrafted filters (e.g., HOG, SIFT in vision). | CNNs (convolutions), RNNs (temporal patterns), Transformers (attention), ResNet (residuals). |\n",
        "| **Advantages**         | - Exploits domain expertise. <br> - Works well with small data. <br> - Lower computational demand. | - Scales to large, unstructured data. <br> - Learns hierarchical representations. <br> - Reduces reliance on human-crafted features. |\n",
        "| **Limitations**        | - Time-consuming & labor-intensive. <br> - May miss hidden patterns. <br> - Hard to generalize. | - Data-hungry & compute-intensive. <br> - Risk of overfitting without regularization. <br> - Requires architecture search expertise. |\n",
        "| **Examples**           | - TF-IDF in NLP. <br> - Handcrafted medical features (e.g., tumor size). <br> - Statistical moments in finance. | - AlexNet (2012) replacing handcrafted vision features. <br> - BERT/GPT learning embeddings from raw text. <br> - AlphaFold predicting protein structures. |\n",
        "| **Philosophy**         | *“Features are engineered by humans, models are simple.”*                                | *“Networks engineer the features, humans design the architectures.”* |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Summary Insight\n",
        "* **Feature Engineering** dominated early AI by encoding **human knowledge** into models.  \n",
        "* **Network Engineering** defines modern deep learning by building **architectures that learn representations automatically**.  \n",
        "* Today, both coexist: preprocessing and normalization remain useful, but deep architectures largely replaced manual feature crafting in **vision, NLP, and multimodal AI**.  \n"
      ],
      "metadata": {
        "id": "uYRDlQBpZulw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🕰️ Timeline: From Feature Engineering to Network Engineering in AI\n",
        "\n",
        "---\n",
        "\n",
        "| **Era**        | **Field**   | **Dominant Approach**          | **Key Techniques / Models**                                       | **Landmark Papers & Authors** |\n",
        "|----------------|-------------|--------------------------------|-------------------------------------------------------------------|--------------------------------|\n",
        "| **1950s–1970s** | General AI  | Handcrafted Features           | Symbolic AI, rule-based logic, perceptrons (limited capacity)     | McCarthy (1955, AI term), Rosenblatt (1958, Perceptron) |\n",
        "| **1980s**      | Vision      | Early Feature Extraction       | Edge detection (Sobel, Canny), handcrafted filters                | Canny (1986, Edge Detection) |\n",
        "|                | Speech      | Manual Features                | LPC (Linear Predictive Coding), MFCC (Mel-Frequency Cepstral Coefficients) | Davis & Mermelstein (1980, MFCC) |\n",
        "|                | NLP         | Statistical Features           | n-grams, POS tagging, curated lexicons                           | Jelinek (1980s, Statistical LM) |\n",
        "| **1990s**      | Vision      | Feature Engineering Peak       | SIFT (Scale-Invariant Feature Transform), HOG (Histograms of Oriented Gradients) | Lowe (1999, SIFT); Dalal & Triggs (2005, HOG) |\n",
        "|                | NLP         | Feature-Based ML               | TF-IDF, bag-of-words, feature-rich linear models                  | Salton (1993, TF-IDF) |\n",
        "|                | Speech      | Engineered Features + HMMs     | MFCC + Hidden Markov Models (HMMs)                               | Rabiner (1989, HMM Tutorial) |\n",
        "| **2000s**      | Vision      | Sophisticated Features         | SURF, Gabor filters, handcrafted descriptors                     | Bay et al. (2006, SURF) |\n",
        "|                | NLP         | Embeddings (transition phase)  | Neural LMs, Word2Vec (representation learning)                   | Bengio (2003, Neural LM); Mikolov (2013, Word2Vec) |\n",
        "|                | Speech      | Hybrid ML                      | GMM-HMM, DNN-HMM hybrids                                          | Hinton et al. (2012, Speech DNNs) |\n",
        "| **2010–2015**  | Vision      | Network Engineering Takes Over | CNNs replace handcrafted features (end-to-end learning)           | Krizhevsky et al. (2012, AlexNet) |\n",
        "|                | NLP         | Neural Feature Learning        | RNNs, LSTMs, Seq2Seq, Attention                                  | Cho et al. (2014, Seq2Seq); Bahdanau et al. (2014, Attention) |\n",
        "|                | Speech      | Deep Neural Features           | End-to-end DNNs for recognition                                  | Hinton et al. (2012, Deep Speech) |\n",
        "| **2016–2020**  | Vision      | Deep Architectures             | ResNet, Inception, EfficientNet                                  | He et al. (2015, ResNet) |\n",
        "|                | NLP         | Transformer Era                | BERT, GPT series                                                 | Vaswani et al. (2017, Attention); Devlin et al. (2019, BERT) |\n",
        "|                | Multi-Modal | Joint Feature Learning         | CLIP (image+text), Video Transformers                            | Radford et al. (2021, CLIP) |\n",
        "| **2021–2025**  | Vision & NLP| Foundation Models              | ViTs, multimodal LLMs, diffusion models                          | Dosovitskiy et al. (2020, ViT); Ho et al. (2020, Diffusion) |\n",
        "|                | Multi-Modal | Unified Architectures          | GPT-4, Gemini, LLaVA, Flamingo                                   | OpenAI (2023, GPT-4); DeepMind (2022, Flamingo) |\n",
        "|                | General AI  | AutoML / NAS                   | Automated neural architecture search                             | Zoph & Le (2017, NASNet) |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 Evolution Insight\n",
        "\n",
        "- **1950s–1990s → Feature Engineering Era**  \n",
        "  Domain experts manually designed features (edges, MFCCs, TF-IDF).  \n",
        "  Models were shallow (SVMs, HMMs, linear regression).  \n",
        "\n",
        "- **2000s → Transition**  \n",
        "  Shift toward **learned embeddings** (Word2Vec, deep speech).  \n",
        "  Hybrid approaches: handcrafted features + shallow nets.  \n",
        "\n",
        "- **2010s → Network Engineering Era**  \n",
        "  CNNs (vision), RNNs (sequence), Transformers (attention).  \n",
        "  Networks learned hierarchical features end-to-end.  \n",
        "\n",
        "- **2020s → Foundation Models & AutoML**  \n",
        "  Large multimodal models unify representation learning (LLMs, ViTs, diffusion).  \n",
        "  Network engineering increasingly **automated** via NAS/AutoML.  \n",
        "  Feature engineering reduced to **preprocessing & normalization**.  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1Jjg54NOaDH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚀 Summary in Key Points: The Bitter Lesson in Action (CVPR 2005–2024)\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 The Bitter Lesson\n",
        "- Rich Sutton (2019): AI progress comes from **general-purpose, compute-heavy methods**, not handcrafted heuristics.  \n",
        "- Computer Vision (CV) exemplifies this trajectory: **SIFT/HOG → CNNs → Transformers → Foundation Models**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Study Design\n",
        "- Dataset: **20 years of CVPR papers (2005–2024)**.  \n",
        "- Method: Large Language Models (LLMs) rated titles & abstracts on 5 “Bitter Lesson” dimensions:  \n",
        "  1. **Learning > Engineering**  \n",
        "  2. **Search > Heuristics**  \n",
        "  3. **Scalability with Computation**  \n",
        "  4. **Generality > Specificity**  \n",
        "  5. **Fundamental Principles > Tricks**  \n",
        "- Analysis: Correlated alignment scores with **citation counts**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 📈 Core Findings\n",
        "- **Learning over Engineering**: Strong upward trend post-2012 (deep learning era).  \n",
        "- **Scalability with Computation**: Rapid rise with AlexNet, ResNet, scaling laws.  \n",
        "- **Generality**: Growth accelerates with foundation models.  \n",
        "- **Search over Heuristics**: Stagnant — explicit search rarely adopted.  \n",
        "- **Citation Impact**: Post-2015, papers aligning with scalability & learning gained **more citations**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🌊 Impactful Era (2015–2020)\n",
        "- Sharpest increase in **bitter lesson alignment**.  \n",
        "- Predictive power of alignment ↔ citations peaked.  \n",
        "- Coincides with **AlexNet, ResNet, ImageNet breakthroughs**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔮 Future Outlook\n",
        "- **Inference-time search** (e.g., OpenAI o1 models, AlphaGo’s MCTS) = next frontier.  \n",
        "- Expect CV to further embrace **scalable, compute-driven learning**, reducing reliance on heuristics.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Meta Contribution\n",
        "- Methodology itself embodies Sutton’s vision:  \n",
        "  - Used **LLMs (fruits of Bitter Lesson)** to analyze research trends.  \n",
        "  - Demonstrates automation accelerating **scientific meta-analysis**.  \n",
        "\n",
        "---\n",
        "\n",
        "## ✨ One-Line Takeaway\n",
        "The last two decades of CVPR confirm Sutton’s Bitter Lesson: **the more computer vision embraced scalable, general, compute-driven learning, the more impactful the research became** — and the future leans toward even deeper reliance on computation and search.  \n"
      ],
      "metadata": {
        "id": "kpNzwxdYbKc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📊 Evolution of Computer Vision Through the Lens of *The Bitter Lesson*\n",
        "\n",
        "| Era | Dominant Approach | Key Traits | Landmark Examples | Alignment with Bitter Lesson |\n",
        "|-----|------------------|------------|-------------------|------------------------------|\n",
        "| **Pre-2012: Handcrafted Era** | Feature Engineering | - Manual design (SIFT, HOG, Haar cascades) <br> - Strong reliance on domain expertise <br> - Limited scalability with compute | SIFT (Lowe, 1999), HOG (Dalal & Triggs, 2005) | **Low**: Engineering > Learning, little scalability |\n",
        "| **2012–2015: Deep Learning Breakthrough** | CNN Revolution | - Data-driven hierarchical features <br> - GPU-accelerated training <br> - ImageNet-scale datasets | AlexNet (2012), VGG (2014), ResNet (2015) | **Medium–High**: Clear shift to learning & compute |\n",
        "| **2016–2020: Scaling Deep Nets** | Large CNNs & Early Transformers | - Rapid architecture innovations (ResNet, Inception, EfficientNet) <br> - Scaling depth, width, and data <br> - Early multimodal exploration | ResNet (2015), Transformer (2017), EfficientNet (2019) | **High**: Scalability & generality dominate |\n",
        "| **2020–2024: Foundation Models Era** | General Models + Multimodality | - Vision–Language models (CLIP, ALIGN, Florence) <br> - Few-shot & zero-shot generalization <br> - Universal transferable features | CLIP (2021), Florence (2021), ViT (2020) | **Very High**: Maximum scalability & generality |\n",
        "| **2024–Future: Inference-Time Scaling** | Compute-as-Search | - Dynamic compute at inference (o1 models, AlphaGo’s MCTS) <br> - Simulating strategies & test-time optimization <br> - Less human heuristics, more automated exploration | OpenAI o1 (2024), AlphaGo (2016, precursor) | **Emerging Frontier**: Search over heuristics returns |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 Teaching Insight\n",
        "\n",
        "- **Handcrafted Era** → Engineers wrote features.  \n",
        "- **Deep Learning Era** → Networks learned features.  \n",
        "- **Foundation Models** → Networks learn **generalizable representations**.  \n",
        "- **Inference-Time Scaling** → Networks may also **learn to search dynamically**.  \n"
      ],
      "metadata": {
        "id": "AjA3X62jb2BS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📌 The Bitter Lesson — Simplified Summary\n",
        "\n",
        "---\n",
        "\n",
        "## 🌍 Core Idea\n",
        "The biggest lesson from **70+ years of AI**:  \n",
        "👉 **General methods that scale with computation beat human-crafted knowledge in the long run.**\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ Why?\n",
        "- **Moore’s Law** → computing power grows exponentially.  \n",
        "- **Short-term:** Human knowledge helps.  \n",
        "- **Long-term:** Scalable methods + computation always win.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Examples Across Fields\n",
        "- **Chess (1997, Deep Blue vs Kasparov):**  \n",
        "  Human-like reasoning failed → brute-force deep search + hardware won.  \n",
        "\n",
        "- **Go (2016, AlphaGo):**  \n",
        "  Handcrafted tricks failed → success via massive search + self-play learning.  \n",
        "\n",
        "- **Speech Recognition (1970s–today):**  \n",
        "  Phoneme/vocal tract rules failed → statistical HMMs → deep learning won.  \n",
        "\n",
        "- **Computer Vision:**  \n",
        "  Handcrafted features (edges, SIFT, HOG) plateaued → CNNs & deep nets dominate.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🚫 Why Human Knowledge Hurts Long-Term\n",
        "- Feels natural to bake in human reasoning.  \n",
        "- Works in the short term, gives researchers pride.  \n",
        "- But **plateaus** and blocks scalability.  \n",
        "\n",
        "---\n",
        "\n",
        "## ✅ What Actually Works\n",
        "Two families of methods **scale with compute**:\n",
        "1. **Search** (tree search, optimization, brute force).  \n",
        "2. **Learning** (statistical models, deep nets, self-play).  \n",
        "\n",
        "➡️ These improve automatically as compute grows.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Big Takeaway\n",
        "- The world is **too complex to predefine** with rules.  \n",
        "- Don’t hard-code intelligence (objects, grammar, reasoning).  \n",
        "- Instead: build systems that **learn and discover for themselves**.  \n",
        "\n",
        "---\n",
        "\n",
        "## ✨ In One Sentence\n",
        "**The Bitter Lesson:**  \n",
        "Stop hand-coding intelligence.  \n",
        "Let **search + learning + computation** discover solutions — they always win in the long run.  \n"
      ],
      "metadata": {
        "id": "1qmlmhTPb_Rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚡ The Bitter Lessons of AI\n",
        "\n",
        "---\n",
        "\n",
        "## 🎲 Games & Search\n",
        "- **Chess (1997, Deep Blue)**  \n",
        "  Handcrafted chess knowledge lost.  \n",
        "  👉 Brute-force search + compute won.  \n",
        "\n",
        "- **Go (2016, AlphaGo)**  \n",
        "  Human-inspired heuristics failed.  \n",
        "  👉 Massive search + self-play learning won.  \n",
        "\n",
        "- **Checkers (1950s–2007, Chinook)**  \n",
        "  Decades of strategy modeling plateaued.  \n",
        "  👉 Exhaustive search with compute solved the game.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🎤 Speech Recognition\n",
        "- **1970s–2010s**  \n",
        "  Phoneme models + vocal-tract physics failed.  \n",
        "  👉 Statistical HMMs → Deep learning won.  \n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Natural Language Processing\n",
        "- Rule-based grammar systems collapsed.  \n",
        "- 👉 Statistical models → embeddings → transformers won.  \n",
        "\n",
        "---\n",
        "\n",
        "## 👁️ Computer Vision\n",
        "- Edges, SIFT, HOG, handcrafted filters died.  \n",
        "- 👉 CNNs → Vision Transformers → Foundation models won.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🌍 Machine Translation\n",
        "- Symbolic, grammar-based translators failed.  \n",
        "- 👉 Data-driven Seq2Seq & attention models won.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🤖 Robotics & Control\n",
        "- Hard-coded locomotion (e.g., walking rules) unstable.  \n",
        "- 👉 Reinforcement learning + large-scale simulation won.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🎵 Recommendation Systems\n",
        "- Human-coded “taste ontologies” failed.  \n",
        "- 👉 Collaborative filtering + deep models on massive data won.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🚗 Autonomous Driving\n",
        "- Expert-crafted “if–then” rules collapsed in complexity.  \n",
        "- 👉 End-to-end perception + deep learning policies progressing.  \n",
        "\n",
        "---\n",
        "\n",
        "## 💊 Drug Discovery & Bioinformatics\n",
        "- Human-designed molecular rules plateaued.  \n",
        "- 👉 Deep generative models + scaling laws win.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🕹️ General Game AI\n",
        "- Hand-coded strategies weak.  \n",
        "- 👉 Self-play (AlphaZero, MuZero) + search dominate.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Knowledge Representation (GOFAI, 1960s–80s)\n",
        "- Symbolic expert systems collapsed.  \n",
        "- 👉 Data-driven statistical learning took over.  \n",
        "\n",
        "---\n",
        "\n",
        "## 📈 Scaling Laws (2020s)\n",
        "- Carefully tuned small models saturate.  \n",
        "- 👉 Bigger models + more data = predictable gains.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🧬 Neuroscience-Inspired AI\n",
        "- Direct brain mimicry (symbolic neurons, cognitive architectures) underperformed.  \n",
        "- 👉 Abstract, scalable learning (Transformers, RL) succeed.  \n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ Optimization\n",
        "- Hand-tuned heuristics limited.  \n",
        "- 👉 Gradient descent + large compute optimization won.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔮 Search at Inference (Emerging Frontier)\n",
        "- Static models plateau.  \n",
        "- 👉 Inference-time compute (AlphaGo’s MCTS, OpenAI o1) rising.  \n",
        "\n",
        "---\n",
        "\n",
        "## ✨ Meta-Lesson\n",
        "Across all domains, the Bitter Lesson repeats:  \n",
        "👉 Human-designed tricks help briefly but **hit walls**.  \n",
        "👉 **Scalable, compute-hungry methods — search + learning — always overtake them.**\n"
      ],
      "metadata": {
        "id": "WT1IgS3NdC-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚡ The 30 Bitter Lessons of AI\n",
        "\n",
        "---\n",
        "\n",
        "## 🎲 Games\n",
        "1. **Chess (1997, Deep Blue)**  \n",
        "   Human strategy models failed.  \n",
        "   👉 Brute-force search + compute won.  \n",
        "\n",
        "2. **Go (2016, AlphaGo)**  \n",
        "   Hand-crafted heuristics collapsed.  \n",
        "   👉 Search + self-play learning won.  \n",
        "\n",
        "3. **Checkers (2007, Chinook)**  \n",
        "   Human opening books obsolete.  \n",
        "   👉 Exhaustive computation solved the game.  \n",
        "\n",
        "4. **Backgammon (TD-Gammon, 1990s)**  \n",
        "   Expert rules dominated early.  \n",
        "   👉 Reinforcement learning + self-play beat pros.  \n",
        "\n",
        "5. **Poker (2017, Libratus, Pluribus)**  \n",
        "   Hand-coded bluffing rules failed.  \n",
        "   👉 Game-theoretic RL with large compute beat humans.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🎤 Speech & Language\n",
        "6. **Speech Recognition (DARPA 1970s → 2010s)**  \n",
        "   Phoneme/vocal-tract models weak.  \n",
        "   👉 HMMs, then deep neural nets scaled.  \n",
        "\n",
        "7. **Text-to-Speech**  \n",
        "   Rule-based phonetic synthesis unnatural.  \n",
        "   👉 Neural TTS (WaveNet, Tacotron) won.  \n",
        "\n",
        "8. **Machine Translation**  \n",
        "   Grammar-based symbolic MT stalled.  \n",
        "   👉 Statistical → Seq2Seq → Transformers won.  \n",
        "\n",
        "9. **Natural Language Understanding (NLP)**  \n",
        "   Hand-coded semantic ontologies failed.  \n",
        "   👉 Word embeddings → LLMs succeeded.  \n",
        "\n",
        "10. **Information Retrieval / Search Engines**  \n",
        "    Human taxonomies (Yahoo directories) collapsed.  \n",
        "    👉 Statistical ranking + large-scale indexing won.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🎵 Recommendations & Vision\n",
        "11. **Recommendation Systems**  \n",
        "    Expert-coded “taste graphs” failed.  \n",
        "    👉 Collaborative filtering + deep learning scaled.  \n",
        "\n",
        "12. **Computer Vision (Edges, SIFT, HOG)**  \n",
        "    Feature engineering plateaued.  \n",
        "    👉 CNNs, ViTs, multimodal models surpassed.  \n",
        "\n",
        "13. **Image Classification (ILSVRC 2012)**  \n",
        "    Decades of descriptors obsolete.  \n",
        "    👉 AlexNet with GPUs crushed error rates.  \n",
        "\n",
        "14. **Object Detection**  \n",
        "    Rule-based Haar cascades limited.  \n",
        "    👉 Deep detectors (R-CNN, YOLO, DETR) won.  \n",
        "\n",
        "15. **Medical Imaging**  \n",
        "    Handcrafted features underwhelmed.  \n",
        "    👉 CNNs on massive datasets outperform.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🤖 Robotics & Control\n",
        "16. **Reinforcement Learning in Robotics**  \n",
        "    Hard-coded walking/gait rules brittle.  \n",
        "    👉 Policy learning + simulation scaling wins.  \n",
        "\n",
        "17. **Autonomous Driving**  \n",
        "    Rule-based pipelines brittle in open world.  \n",
        "    👉 End-to-end deep policies + sensor fusion more robust.  \n",
        "\n",
        "18. **Control Systems (Classic AI Robotics)**  \n",
        "    Symbolic planners (STRIPS, 1970s) failed at scale.  \n",
        "    👉 RL + model-based learning scale better.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔬 Science & Discovery\n",
        "19. **Drug Discovery**  \n",
        "    Rule-driven chemistry “expert systems” plateaued.  \n",
        "    👉 Generative models + deep RL accelerating progress.  \n",
        "\n",
        "20. **Protein Folding (AlphaFold, 2020)**  \n",
        "    Decades of handcrafted bio-physics.  \n",
        "    👉 Deep learning + compute solved structure.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Knowledge & Reasoning\n",
        "21. **Knowledge Representation (GOFAI, 1960s–80s)**  \n",
        "    Expert systems collapsed.  \n",
        "    👉 Statistical & neural learning dominated.  \n",
        "\n",
        "22. **Cognitive Architectures**  \n",
        "    Human reasoning mimics (SOAR, ACT-R) failed to generalize.  \n",
        "    👉 Scalable ML methods overtook.  \n",
        "\n",
        "23. **Optimization**  \n",
        "    Hand-tuned search heuristics fragile.  \n",
        "    👉 Gradient descent + stochastic optimization scalable.  \n",
        "\n",
        "24. **Hyperparameter Tuning**  \n",
        "    Manual trial-and-error slow.  \n",
        "    👉 Bayesian optimization + AutoML won.  \n",
        "\n",
        "25. **Game AI (General)**  \n",
        "    Scripted behavior brittle.  \n",
        "    👉 Self-play RL + search (AlphaZero, MuZero) generalize.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔎 Paradigms & Scaling\n",
        "26. **Neuroscience-Inspired AI**  \n",
        "    Direct mimicry of neurons/brains didn’t scale.  \n",
        "    👉 Abstract scalable math (attention, backprop) did.  \n",
        "\n",
        "27. **Symbolic Logic & Planning**  \n",
        "    Logic-based AI stagnated.  \n",
        "    👉 Learning-based statistical models scaled better.  \n",
        "\n",
        "28. **Scaling Laws (2020s)**  \n",
        "    Carefully tuned small models plateau.  \n",
        "    👉 Bigger models + more data → predictable gains.  \n",
        "\n",
        "29. **Inference-Time Compute (New Frontier)**  \n",
        "    Static frozen models plateau.  \n",
        "    👉 Dynamic search at inference (AlphaGo’s MCTS, OpenAI o1) rising.  \n",
        "\n",
        "30. **General AI Outlook**  \n",
        "    Human-designed rules/knowledge always appealing but brittle.  \n",
        "    👉 Search + learning + compute scaling are the universal winners.  \n",
        "\n",
        "---\n",
        "\n",
        "## ✨ Master Insight\n",
        "Across all domains — games, language, vision, robotics, science — the pattern repeats:  \n",
        "👉 Hand-engineered knowledge feels right but **stalls**.  \n",
        "👉 **Scalable, compute-driven methods (search + learning) always break through and win.**\n"
      ],
      "metadata": {
        "id": "K6i4D6GydaNV"
      }
    }
  ]
}