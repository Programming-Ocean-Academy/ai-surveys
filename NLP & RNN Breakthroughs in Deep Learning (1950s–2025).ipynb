{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9u6tMc+XJE0E0xC/zpDJd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ“œ NLP & RNN Breakthroughs in Deep Learning (1950sâ€“2025)\n","\n","---\n","\n","## ðŸ“š Key Milestones\n","\n","| **Era** | **Model / Concept** | **Year** | **Authors / Org** | **Key Contributions** |\n","|---------|---------------------|----------|-------------------|-----------------------|\n","| **Early Concepts** | **n-gram Models** | 1948 | Claude Shannon | First probabilistic language models (word/character sequences). |\n","| | **ELIZA** | 1966 | Weizenbaum (MIT) | Rule-based conversational system; early NLP milestone. |\n","| **Foundations of Recurrent Nets** | **Hopfield Networks** | 1982 | John Hopfield | First widely known recurrent model, associative memory. |\n","| | **Backpropagation Through Time (BPTT)** | 1990 | Paul Werbos | Formalized unfolding RNNs through time for training. |\n","| | **LSTM** | 1997 | Hochreiter & Schmidhuber | Solved vanishing gradients, enabled long-sequence learning. |\n","| **Neural NLP Era** | **Neural Probabilistic Language Model** | 2003 | Bengio et al. (U. MontrÃ©al) | Introduced neural embeddings + feedforward autoregressive LM. |\n","| | **Word2Vec** | 2013 | Mikolov et al. (Google) | Scalable word embeddings; revolutionized NLP representation. |\n","| | **Seq2Seq + Attention** | 2014 | Sutskever, Vinyals & Le (Google Brain) / Bahdanau et al. | Encoderâ€“decoder RNNs with attention; breakthrough in MT. |\n","| **RNN Variants & Scaling** | **GRU (Gated Recurrent Unit)** | 2014 | Cho et al. | Simplified gating, efficient alternative to LSTM. |\n","| | **Google Neural Machine Translation (NMT)** | 2016 | Wu et al. (Google) | Large-scale LSTM-based NMT replacing phrase-based MT. |\n","| **Transformers Take Over NLP** | **Transformer** | 2017 | Vaswani et al. (Google Brain) | Self-attention replaces recurrence; scalable NLP backbone. |\n","| | **BERT** | 2018 | Devlin et al. (Google AI) | Bidirectional masked LM, pretrain + finetune paradigm. |\n","| | **GPT** | 2018 | Radford et al. (OpenAI) | Decoder-only autoregressive LM; generative pretraining. |\n","| | **T5** | 2020 | Raffel et al. (Google) | Unified text-to-text Transformer across NLP tasks. |\n","| **Modern NLP & Multimodality** | **ChatGPT (GPT-3.5/4)** | 2022â€“2023 | OpenAI | Conversational fine-tuned LLMs, global adoption. |\n","| | **PaLM (Pathways LM)** | 2022 | Google Research | Scaled Transformers to hundreds of billions of params. |\n","| | **LLaMA** | 2023 | Meta AI | Open foundation models democratizing research access. |\n","\n","---\n","\n","## âœ… Summary Families\n","- **RNN Core:** Hopfield (1982) â†’ LSTM (1997) â†’ GRU (2014).  \n","- **Neural NLP Foundations:** Bengio NLM (2003) â†’ Word2Vec (2013) â†’ Seq2Seq + Attention (2014).  \n","- **Scaling with RNNs:** Google NMT (2016).  \n","- **Transformer Era:** Transformer (2017) â†’ BERT (2018) â†’ GPT (2018+) â†’ T5 (2020).  \n","- **Modern LLMs:** GPT-4, PaLM, LLaMA, ChatGPT (2022â€“2025).  \n"],"metadata":{"id":"LlcVXnaU5wZs"}}]}