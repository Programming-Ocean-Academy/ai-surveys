{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Multimodal Deep Learning ‚Äî A Field Guide (2014‚Äì2025)\n",
        "\n",
        "---\n",
        "\n",
        "## What ‚Äúmultimodal‚Äù means\n",
        "\n",
        "Multimodal models jointly learn from two or more data modalities‚Äîe.g., text+image, audio+video, text+speech, image+tabular‚Äîso they can align, reason across, or generate one modality from another.\n",
        "\n",
        "---\n",
        "\n",
        "## Core paradigms\n",
        "\n",
        "### Contrastive dual encoders (alignment)\n",
        "\n",
        "- Learn a shared space where paired modalities are close (e.g., image‚Äìcaption).  \n",
        "- Enables zero-shot retrieval/classification by comparing embeddings.  \n",
        "\n",
        "**Exemplars:** CLIP (OpenAI) and ALIGN (Google).  \n",
        "*Proceedings of Machine Learning Research +3*  \n",
        "*arXiv +3*  \n",
        "*Proceedings of Machine Learning Research +3*\n",
        "\n",
        "---\n",
        "\n",
        "### Generative vision‚Äìlanguage models (instruction & few-shot)\n",
        "\n",
        "- Connect a visual encoder to a language model (frozen or trainable) so the LM can condition on visual tokens and generate text (VQA, captioning, OCR-style tasks).  \n",
        "\n",
        "Two dominant recipes:  \n",
        "\n",
        "1. **Bridging / adapters into an LLM** (freeze vision & LLM; train a small connector): BLIP-2.  \n",
        "   *arXiv +2*  \n",
        "\n",
        "2. **Interleaved visual‚Äìtext sequences** on top of a strong LM: Flamingo.  \n",
        "   *arXiv +2*  \n",
        "   *NeurIPS Proceedings +2*\n",
        "\n",
        "---\n",
        "\n",
        "### Joint, scaled vision‚Äìlanguage models\n",
        "\n",
        "- One model trained on many VL tasks (captioning, VQA, OCR, multilingual understanding) with large mixtures of data: PaLI / PaLI-X / PaLI-3.  \n",
        "  *Ÿáÿßÿ±ŸÅÿßÿ±ÿØ ŸÑÿßŸÖÿØÿß +2*  \n",
        "  *arXiv +2*\n",
        "\n",
        "---\n",
        "\n",
        "### Domain-specific multimodal (speech/audio, medical, video)\n",
        "\n",
        "- **Speech & AV:** Conformer-based ASR, audio-visual speech recognition, AV representation learning.  \n",
        "- **Healthcare:** clinical notes + imaging (multimodal fusion, uncertainty). *(Good survey below.)*  \n",
        "  *arXiv*\n",
        "\n",
        "---\n",
        "\n",
        "### Robustness to missing modalities\n",
        "\n",
        "- Practical deployments often lose sensors/modalities; recent work surveys training/eval strategies for MLMM (Missing-Modality Multimodal).  \n",
        "  *arXiv +1*\n",
        "\n",
        "---\n",
        "\n",
        "## Landmark models & why they matter\n",
        "\n",
        "| Era     | Model     | Key idea | Why it mattered |\n",
        "|---------|-----------|----------|-----------------|\n",
        "| 2021    | CLIP (Radford et al.) | Contrastive training on web-scale image‚Äìtext; dual encoders | Kicked off robust zero-shot transfer and retrieval; established recipe for alignment at scale. <br>*arXiv +1* |\n",
        "| 2021    | ALIGN (Jia et al.) | Even larger, noisy pairs; scale beats label quality | Showed noisy but massive pairs + InfoNCE suffice for SOTA VL representations. <br>*arXiv +1* |\n",
        "| 2022    | Flamingo (DeepMind) | Frozen vision + LMs with gated cross-attention to handle interleaved image/video & text | Set SOTA in few-/in-context VL tasks; unlocked ‚Äúprompting with pictures.‚Äù <br>*arXiv +2* <br>*NeurIPS Proceedings +2* |\n",
        "| 2023    | BLIP-2 (Salesforce) | Train a lightweight Querying Transformer to bridge frozen encoders to an LLM | Orders-of-magnitude fewer trainable params vs end-to-end; strong zero-shot VQA/captioning. <br>*arXiv +1* |\n",
        "| 2022‚Äì23 | PaLI / PaLI-X / PaLI-3 (Google) | Jointly scaled multilingual VLMs; recipe comparisons (e.g., SigLIP vs. cls pretrain) | Practical recipe for multilingual OCR/VQA/captioning at scale; smaller PaLI-3 competes with larger peers. <br>*Ÿáÿßÿ±ŸÅÿßÿ±ÿØ ŸÑÿßŸÖÿØÿß +2* <br>*arXiv +2* |\n",
        "\n",
        "Very recent work pushes CLIP-style pretraining to 4K resolution efficiently, underscoring a current theme: **scaling context/resolution while controlling cost.**  \n",
        "*arXiv*\n",
        "\n",
        "---\n",
        "\n",
        "## Taxonomy of training objectives\n",
        "\n",
        "- **Contrastive:** InfoNCE on aligned pairs (image‚Äìtext, video‚Äìtext). (CLIP/ALIGN).  \n",
        "  *arXiv +1*  \n",
        "\n",
        "- **Matching / ITM:** Binary match vs mismatch (often combined with MLM).  \n",
        "\n",
        "- **Masked modeling:** Masked language/vision modeling (e.g., caption or region masking).  \n",
        "\n",
        "- **Generative:** Next-token prediction with visual context (Flamingo, BLIP-2 stage-2).  \n",
        "  *arXiv +1*\n",
        "\n",
        "---\n",
        "\n",
        "## Model architectures at a glance\n",
        "\n",
        "- **Dual encoders** (separate towers + contrastive loss): scalable retrieval/zero-shot; limited cross-modal reasoning without extra heads. (CLIP/ALIGN).  \n",
        "  *arXiv +1*\n",
        "\n",
        "- **Encoder‚Äìdecoder with cross-attention:** strong for question answering and captioning; higher training cost.  \n",
        "\n",
        "- **LLM-as-decoder with visual adapters:** frozen vision & LLM; train a small connector (BLIP-2).  \n",
        "  *arXiv*\n",
        "\n",
        "- **Interleaved token streams into an LLM (Flamingo):** images/videos interspersed with text tokens, enabling in-context multimodal reasoning.  \n",
        "  *arXiv*\n",
        "\n",
        "---\n",
        "\n",
        "## Where multimodal shines today\n",
        "\n",
        "- **Zero-/few-shot recognition & retrieval:** CLIP/ALIGN; PaLI-style OCR/VQA.  \n",
        "  *Proceedings of Machine Learning Research*  \n",
        "\n",
        "- **Instruction-following with images:** Flamingo/BLIP-2-style systems.  \n",
        "  *arXiv +1*  \n",
        "\n",
        "- **Specialized domains:** medical (notes+imaging), audio-visual speech, robotics‚Äîsee dedicated surveys and domain papers.  \n",
        "  *arXiv*\n",
        "\n",
        "---\n",
        "\n",
        "## Surveys & overviews (good starting points)\n",
        "\n",
        "- **A Survey on Multimodal Large Language Models (MLLMs):** architecture, training, evaluation of GPT-4V-class systems.  \n",
        "  *arXiv*  \n",
        "\n",
        "- **Deep Multimodal Learning with Missing Modality (MLMM):** robustness when some sensors/modalities are absent.  \n",
        "  *arXiv +1*  \n",
        "\n",
        "- **Medical multimodal survey:** clinical+imaging pipelines and deployment issues.  \n",
        "  *arXiv*\n",
        "\n",
        "---\n",
        "\n",
        "## Practical takeaways (if you‚Äôre building one)\n",
        "\n",
        "- **Start simple:** dual-encoder contrastive pretrain for alignment; add a small adapter to your LLM (BLIP-2 recipe) for generation.  \n",
        "  *arXiv*  \n",
        "\n",
        "- **Data > tricks:** scale and diversity of paired data still dominate performance (ALIGN/CLIP).  \n",
        "  *arXiv*  \n",
        "\n",
        "- **Choose by use-case:**  \n",
        "  - Retrieval/zero-shot: CLIP-style dual encoders.  \n",
        "    *arXiv*  \n",
        "  - VQA/instruction following: Flamingo/BLIP-2-style adapters.  \n",
        "    *arXiv +1*  \n",
        "  - Multilingual OCR/VQA: PaLI-family.  \n",
        "    *Harvard lambda*\n"
      ],
      "metadata": {
        "id": "JUkq2z4z0_p6"
      }
    }
  ]
}