{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOchsgd27EmDEmWY7PfEDCz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["📜 Unsupervised / Representation Learning Breakthroughs\n","\n","🔹 Foundational Ideas\n","\n","Self-Organizing Maps (SOMs) – Kohonen (1982)\n","“Self-Organized Formation of Topologically Correct Feature Maps.”\n","➝ Competitive learning algorithm mapping high-dimensional data onto a 2D grid; useful for clustering and visualization.\n","\n","Independent Component Analysis (ICA) / Sparse Coding – Comon (1994), Olshausen & Field (1996)\n","“Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images.” (Nature 1996).\n","➝ Learned sparse and independent features resembling visual cortex responses.\n","\n","🔹 Neural Feature Learning (2000s–2010s)\n","\n","Autoencoders – Rumelhart et al. (1986); extended in deep learning era.\n","\n","Stacked Autoencoders & RBMs – Hinton & Salakhutdinov (2006).\n","➝ First large-scale unsupervised pretraining, foundational to deep learning revival.\n","\n","Dictionary Learning / Sparse Representations – used in image denoising, compression.\n","\n","🔹 Modern Deep Representation Learning\n","\n","Contrastive Learning:\n","\n","SimCLR – Chen et al. (2020, Google Brain)\n","“A Simple Framework for Contrastive Learning of Visual Representations.”\n","➝ Positive vs. negative pair training with data augmentations.\n","\n","MoCo (Momentum Contrast) – He et al. (2020, Facebook AI)\n","“Momentum Contrast for Unsupervised Visual Representation Learning.”\n","➝ Memory bank + momentum encoder for scalable contrastive SSL.\n","\n","BYOL (Bootstrap Your Own Latent) – Grill et al. (2020, DeepMind)\n","➝ Contrastive-like training without negatives; teacher–student setup.\n","\n","SwAV (Swapping Assignments between Views) – Caron et al. (2020, Facebook AI)\n","➝ Online clustering + contrastive SSL, competitive with supervised ImageNet training.\n","\n","✅ Summary Families\n","\n","Neuro-inspired early methods: Self-Organizing Maps (1982), Sparse Coding/ICA (1990s).\n","\n","Deep unsupervised pretraining: Autoencoders, RBMs, DBNs (2006).\n","\n","Modern contrastive SSL: SimCLR, MoCo, BYOL, SwAV (2020).\n","\n","👉 Why it matters:\n","Representation learning provides the latent features that power:\n","\n","Generative models (VAEs, GANs, Diffusion).\n","\n","Foundation models in NLP (BERT embeddings, GPT pretraining).\n","\n","Vision models (SimCLR, MAE, SwAV).\n","\n","Multimodal learning (CLIP, DALL·E, PaLM-E)."],"metadata":{"id":"7qCIoU_VQbvE"}}]}