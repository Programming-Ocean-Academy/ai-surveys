{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizers in Deep Learning — Comprehensive Overview\n",
        "\n",
        "---\n",
        "\n",
        "## 1) What an Optimizer Actually Does (and Why It’s Hard)\n",
        "\n",
        "**Goal:** Minimize the expected risk  \n",
        "$$\n",
        "\\mathbb{E}_{(x,y)\\sim D}[\\ell(f_\\theta(x), y)]\n",
        "$$\n",
        "by iteratively updating parameters \\( \\theta \\).\n",
        "\n",
        "**Reality:**  \n",
        "We only see minibatches → noisy gradients.  \n",
        "The loss landscape is **non-convex**, **ill-conditioned**, and full of **saddles** and **valleys** of varying curvature.\n",
        "\n",
        "**Trade-offs:**  \n",
        "- Stability vs speed  \n",
        "- Memory vs accuracy  \n",
        "- Sharp vs flat minima  \n",
        "- Generalization vs training loss  \n",
        "\n",
        "---\n",
        "\n",
        "## 2) Vanilla → Momentum → Nesterov (the “Classics”)\n",
        "\n",
        "### 2.1 Gradient Descent (GD) / Stochastic Gradient Descent (SGD)\n",
        "\n",
        "**Update:**\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\, \\nabla_\\theta L_B(\\theta_t)\n",
        "$$\n",
        "where \\( L_B \\) is minibatch loss, \\( \\eta \\) the learning rate.\n",
        "\n",
        "**Pros:** Simple, strong generalization (SGD).  \n",
        "**Cons:** Sensitive to \\( \\eta \\); slow in narrow valleys.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2 Momentum (Polyak)\n",
        "\n",
        "**Idea:** EMA of gradients → accelerate in consistent directions, damp noise.\n",
        "\n",
        "**Update:**\n",
        "$$\n",
        "v_t = \\mu v_{t-1} + (1-\\mu)\\nabla L_B(\\theta_t), \\qquad\n",
        "\\theta_{t+1} = \\theta_t - \\eta v_t\n",
        "$$\n",
        "Typical \\( \\mu \\in [0.8,0.99] \\); higher = smoother but laggier.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3 Nesterov Accelerated Gradient (NAG)\n",
        "\n",
        "**Look-ahead gradient:**\n",
        "$$\n",
        "v_t = \\mu v_{t-1} + \\nabla L_B(\\theta_t - \\eta\\mu v_{t-1}), \\qquad\n",
        "\\theta_{t+1} = \\theta_t - \\eta v_t\n",
        "$$\n",
        "\n",
        "**Use when:** Need finer control vs momentum; small consistent gains for vision (SGD + Nesterov).\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Adaptive Gradient Methods (Per-Parameter Step Sizes)\n",
        "\n",
        "### 3.1 AdaGrad\n",
        "\n",
        "Accumulate squared gradients:\n",
        "$$\n",
        "G_t = \\sum_{\\tau\\le t} g_\\tau \\odot g_\\tau\n",
        "$$\n",
        "\n",
        "**Update:**\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\, g_t / (\\sqrt{G_t} + \\epsilon)\n",
        "$$\n",
        "\n",
        "**Intuition:** Infrequent features get larger steps → great for sparse problems.  \n",
        "**Con:** Learning rate decays to zero → training can stall.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2 RMSProp\n",
        "\n",
        "Fix AdaGrad’s decay:\n",
        "$$\n",
        "E[g^2]_t = \\rho E[g^2]_{t-1} + (1-\\rho) g_t^2\n",
        "$$\n",
        "\n",
        "**Update:**\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta g_t / (\\sqrt{E[g^2]_t} + \\epsilon)\n",
        "$$\n",
        "Historically strong for RNNs.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3 Adam (Adaptive Moment Estimation)\n",
        "\n",
        "First / second-moment EMAs:\n",
        "$$\n",
        "m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t, \\qquad\n",
        "v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n",
        "$$\n",
        "\n",
        "Bias correction:\n",
        "$$\n",
        "\\hat m_t = \\frac{m_t}{1-\\beta_1^t}, \\quad\n",
        "\\hat v_t = \\frac{v_t}{1-\\beta_2^t}\n",
        "$$\n",
        "\n",
        "**Update:**\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat m_t}{\\sqrt{\\hat v_t} + \\epsilon}\n",
        "$$\n",
        "\n",
        "**Pros:** Fast, scale-robust.  \n",
        "**Cons:** Can converge to sharper minima → weaker final generalization.\n",
        "\n",
        "**Adam Variants:**\n",
        "\n",
        "| Variant | Key Idea |\n",
        "|----------|-----------|\n",
        "| **AdamW** | Decoupled weight decay: \\( \\theta \\!\\leftarrow\\! (1-\\eta\\lambda)\\theta \\) — fixes L2-coupling. |\n",
        "| **AMSGrad** | Non-decreasing \\( \\hat v_t^{max} \\) for convergence proofs. |\n",
        "| **Nadam** | Adds Nesterov look-ahead. |\n",
        "| **AdaBelief** | Uses \\( (g_t - m_t)^2 \\) → better generalization. |\n",
        "| **AdamP / RAdam / Ranger** | Directionality & warmup improvements. |\n",
        "| **Lion** | Updates via sign(\\(m_t\\)) → low memory, effective for ViTs / LLMs. |\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Second-Order & Preconditioning (Curvature-Aware)\n",
        "\n",
        "### 4.1 Newton / Gauss–Newton\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - H^{-1}\\nabla L\n",
        "$$\n",
        "\\(H\\): Hessian approximation.  \n",
        "**Issue:** Intractable for deep nets.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2 Quasi-Newton (L-BFGS)\n",
        "\n",
        "Approximates \\(H^{-1}\\) from gradient history.  \n",
        "**Use:** Small-batch convex or fine-tuning; rare in large-scale deep training.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.3 Natural Gradient / K-FAC / Shampoo / Adafactor\n",
        "\n",
        "- **Natural Gradient:** Precondition by Fisher Information; parameterization-invariant.  \n",
        "- **K-FAC:** Kronecker-factored block approximation.  \n",
        "- **Shampoo:** Matrix-root preconditioning per dimension.  \n",
        "- **Adafactor:** Memory-efficient Adam (factored moments) for huge LMs.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Normalization, Signs & Robust Tricks\n",
        "\n",
        "- **Normalized SGD / Gradient Centralization:** Mean-center gradients for stability.  \n",
        "- **signSGD / QSGD:** Use only sign bits to reduce comms; robust but may plateau.\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Schedules & Warmup (The Silent Super-Power)\n",
        "\n",
        "**Why:** Schedule often matters more than optimizer choice.\n",
        "\n",
        "| Schedule | Description |\n",
        "|-----------|--------------|\n",
        "| **Step Decay** | Drop η by 10× at milestones. |\n",
        "| **Cosine Decay** | Smooth to 0; combine with linear warmup (1–10%). |\n",
        "| **Cyclical / OneCycle** | Ramp up then down; great for SGD. |\n",
        "| **Linear Decay** | Common in NLP (AdamW). |\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Practical Defaults (Per Domain)\n",
        "\n",
        "### Vision (CNNs / ViTs)\n",
        "\n",
        "| Setup | Optimizer | LR | Momentum / Betas | WD | Notes |\n",
        "|--------|------------|----|------------------|----|-------|\n",
        "| From scratch | SGD + Nesterov | 0.1–1.0 | 0.9 | 1e-4–5e-4 | Cosine + warmup |\n",
        "| Deep/ViT | AdamW | 1e-4–3e-3 | (0.9, 0.999) | 1e-4–1e-2 | Cosine + warmup |\n",
        "\n",
        "---\n",
        "\n",
        "### NLP (Transformers / LLMs)\n",
        "\n",
        "**AdamW default:**  \n",
        "η = 1e-5–5e-4, β = (0.9, 0.98 or 0.999), wd ≈ 0.01  \n",
        "Linear decay + warmup (1–10%).  \n",
        "Use **Adafactor** for extremely large models.\n",
        "\n",
        "---\n",
        "\n",
        "### Speech / Sequential (RNN / Conformer)\n",
        "\n",
        "Adam / RMSProp + warmup; always clip gradients (1.0 – 5.0).\n",
        "\n",
        "---\n",
        "\n",
        "### Reinforcement Learning\n",
        "\n",
        "Adam / AdamW; tune η carefully; clip + schedule essential; entropy bonuses interact with optimizer noise.\n",
        "\n",
        "---\n",
        "\n",
        "### Tabular / Small Data\n",
        "\n",
        "SGD + momentum or AdamW + early stopping.  \n",
        "Regularization > optimizer choice.\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Optimizer “Add-Ons”\n",
        "\n",
        "| Add-On | Function |\n",
        "|---------|-----------|\n",
        "| **Gradient Clipping** | Prevents explosion (esp. RNNs). |\n",
        "| **Lookahead** | Slow EMA of fast steps → smoother convergence. |\n",
        "| **SWA** | Average late checkpoints → flatter minima. |\n",
        "| **SAM** | Minimize worst-case loss in local neighborhood → flatness boost (~2× cost). |\n",
        "| **EMA / Polyak Averaging** | \\( \\tilde\\theta_t = \\alpha \\tilde\\theta_{t-1} + (1-\\alpha)\\theta_t \\), α ∈ [0.99, 0.9999]; stabilizes eval. |\n",
        "\n",
        "---\n",
        "\n",
        "## 9) Reading Curves & Tuning Playbook\n",
        "\n",
        "| Symptom | Likely Cause | Fix |\n",
        "|----------|---------------|-----|\n",
        "| Loss zig-zag / early stall | LR too high / small batch | Lower LR |\n",
        "| Val gap ↑ | Overfit | More WD / stronger aug / earlier schedule |\n",
        "| Adam → sharp minima | Poor generalization | Switch to SGD late / add SAM or SWA |\n",
        "| Plateau post LR drop | Schedule too late / shallow | Drop earlier or use cosine |\n",
        "| NaNs / explode | LR too high or mixed-precision error | Clip grad / lower LR / check scaling |\n",
        "\n",
        "---\n",
        "\n",
        "### Typical Sweep Ranges\n",
        "\n",
        "| Optimizer | η range | WD | Momentum / β₂ |\n",
        "|------------|----------|----|----------------|\n",
        "| SGD + Nest. | {0.05, 0.1, 0.2, 0.4} | 1e-4 – 1e-3 | 0.9 |\n",
        "| AdamW | {1e-4, 3e-4, 1e-3, 3e-3} | 1e-4 – 1e-2 | 0.98 / 0.999 |\n",
        "\n",
        "Warmup steps: 1–5 % of total.  \n",
        "\n",
        "---\n",
        "\n",
        "## 10) Concise Equations Cheat-Sheet\n",
        "\n",
        "| Optimizer | Update Rule |\n",
        "|------------|-------------|\n",
        "| **SGD** | \\( \\theta_{t+1} = \\theta_t - \\eta g_t \\) |\n",
        "| **Momentum** | \\( v_t = \\mu v_{t-1} + g_t,\\;\\theta_{t+1} = \\theta_t - \\eta v_t \\) |\n",
        "| **Nesterov** | \\( v_t = \\mu v_{t-1} + g(\\theta_t - \\eta\\mu v_{t-1}),\\;\\theta_{t+1} = \\theta_t - \\eta v_t \\) |\n",
        "| **RMSProp** | \\( s_t = \\rho s_{t-1} + (1-\\rho)g_t^2,\\;\\theta_{t+1} = \\theta_t - \\eta g_t / (\\sqrt{s_t}+\\epsilon) \\) |\n",
        "| **Adam** | see full moment formulas above |\n",
        "| **AdamW Decay** | \\( \\theta \\leftarrow (1-\\eta\\lambda)\\theta \\) |\n",
        "\n",
        "---\n",
        "\n",
        "## 11) Pros / Cons / When to Use\n",
        "\n",
        "| Optimizer | Pros | Cons | Where It Shines | Key Knobs |\n",
        "|------------|------|------|----------------|------------|\n",
        "| **SGD + Nesterov** | Great generalization · simple · low mem | Needs tuned LR & schedule | Vision (CNNs from scratch) | LR, momentum, WD, schedule |\n",
        "| **AdamW** | Fast · stable · decoupled WD | Sharper minima possible | Transformers / ViTs | LR, WD, β’s, warmup |\n",
        "| **RMSProp** | Handles non-stationary grads | Fewer modern wins | Older RNN / RL | LR, ρ |\n",
        "| **AdaGrad** | Sparse features | LR decays → stall | NLP sparse grads | LR |\n",
        "| **AMSGrad** | Theoretical guarantee | Usually same as Adam | Safety-critical | LR, β’s |\n",
        "| **AdaBelief** | Often better generalization | Inconsistent results | Vision/NLP | LR, β’s |\n",
        "| **Lion** | Low memory | Needs tuning | ViTs / LLMs | LR, β’s |\n",
        "| **L-BFGS** | Strong on smooth losses | Full-batch / mem heavy | Small models | Hist size |\n",
        "| **K-FAC / Shampoo** | Curvature speed-ups | Complex · memory heavy | Large models | Damping freq |\n",
        "| **Adafactor** | Memory saving | Stability quirks | Massive LMs | Factored moments on/off |\n",
        "\n",
        "---\n",
        "\n",
        "## 12) Common Pitfalls & Fixes\n",
        "\n",
        "- Use **AdamW**, not L2 inside Adam.  \n",
        "- Always use **warmup** for deep / transformer models.  \n",
        "- Don’t over-rely on Adam for vision → switch to SGD late or add SWA/SAM.  \n",
        "- Always **schedule** learning rate.  \n",
        "- Scale LR roughly linearly with batch size.\n",
        "\n",
        "---\n",
        "\n",
        "## 13) Quick Recipes (Drop-In Defaults)\n",
        "\n",
        "| Context | Recommended Setup |\n",
        "|----------|------------------|\n",
        "| **General (default)** | AdamW (η = 3e-4, β = (0.9, 0.999), wd = 0.01) + 5 % warmup + cosine decay. |\n",
        "| **Vision from scratch** | SGD + Nesterov (ηₘₐₓ = 0.4, momentum = 0.9, wd = 1e-4) + OneCycle schedule. |\n",
        "| **Transformer fine-tune** | AdamW (η = 2e-5 – 5e-5, β = (0.9, 0.98), wd = 0.01) + linear decay + 10 % warmup. |\n",
        "| **Very large models** | Adafactor / AdamW + memory optimizations + EMA for eval. |\n",
        "\n",
        "---\n",
        "\n",
        "**Summary Insight:**  \n",
        "Optimizers navigate a noisy, high-dimensional, curved landscape.  \n",
        "The interplay of **update rule**, **learning-rate schedule**, and **regularization** dictates whether training lands in a **flat, generalizing minimum** or a **sharp, brittle one**.  \n",
        "Mastery lies not in choosing “the best optimizer,” but in matching the **dynamics and schedule** to your model, data, and compute.\n"
      ],
      "metadata": {
        "id": "0NLxO9uGIy9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer Research Landscape — Chronological & Thematic Foundations\n",
        "\n",
        "---\n",
        "\n",
        "## **A) Foundations & Core Theory**\n",
        "\n",
        "| Theme | Key Work | Authors / Venue / Year | Contribution |\n",
        "|--------|-----------|------------------------|---------------|\n",
        "| **Stochastic Approximation (SA)** | *On the Stability of Inverse Problems* | **Tikhonov**, 1943 | Introduced regularization for ill-posed inverse problems. |\n",
        "|  | *A Stochastic Approximation Method* | **Robbins & Monro**, *Ann. Math. Stat.*, 1951 | First principled analysis of noisy gradient descent; foundation of SGD. |\n",
        "|  | *Kiefer–Wolfowitz SA* | **Kiefer & Wolfowitz**, *Ann. Math. Stat.*, 1952 | Finite-difference stochastic approximation. |\n",
        "| **Convex Optimization** | *Convex Optimization (book)* | **Boyd & Vandenberghe**, 2004 | Unified treatment of convex problems, KKT, proximal and dual methods. |\n",
        "| **Acceleration Theory** | *Introductory Lectures on Convex Optimization* | **Nesterov**, 2004 | Proved optimal first-order convergence; basis of NAG. |\n",
        "| **Mirror Descent** | *Problem Complexity and Method Efficiency in Optimization* | **Nemirovski & Yudin**, 1983 | Introduced geometry-aware first-order framework. |\n",
        "|  | *Mirror Descent & Proximal Analysis* | **Beck & Teboulle**, *SIAM J. Optim.*, 2003 | Modern convergence and composite extension. |\n",
        "| **Momentum & Step Rules** | *Polyak’s Heavy Ball* | **Polyak**, *USSR Comp. Math.*, 1964 | Introduced momentum & adaptive step-size principles. |\n",
        "| **Spectral Steps** | *Two-Point Step Size Gradient Methods* | **Barzilai & Borwein**, *IMA JNA*, 1988 | Proposed step-size using curvature (spectral rule). |\n",
        "| **Second-Order Frameworks** | *Trust-Region Methods (book)* | **Conn, Gould & Toint**, 2000 | Rigorous analysis of second-order optimization frameworks. |\n",
        "\n",
        "---\n",
        "\n",
        "## **B) SGD, Momentum, Nesterov**\n",
        "\n",
        "| Key Work | Authors / Venue / Year | Contribution |\n",
        "|-----------|------------------------|---------------|\n",
        "| *SGD for Large-Scale Learning* | **Bottou**, *Neurocomputing*, 2010 | Statistical view of SGD and convergence tradeoffs. |\n",
        "| *Optimization Methods for Large-Scale ML* | **Bottou et al.**, *NIPS LSS*, 2016 | Practical perspective bridging theory and implementation. |\n",
        "| *Nesterov Accelerated Gradient* | **Nesterov**, 1983; **Sutskever et al.**, *ICML*, 2013 | Theoretically optimal look-ahead acceleration adapted to DL. |\n",
        "| *Large-Batch Sharp Minima* | **Keskar et al.**, *ICLR*, 2017 | Showed large batches lead to sharper minima; inspired flatness research. |\n",
        "\n",
        "---\n",
        "\n",
        "## **C) Adaptive Gradient Methods (Per-Parameter Steps)**\n",
        "\n",
        "| Method | Authors / Venue / Year | Contribution |\n",
        "|---------|------------------------|---------------|\n",
        "| **AdaGrad** | **Duchi, Hazan & Singer**, *JMLR*, 2011 | Per-coordinate adaptivity via accumulated squared gradients. |\n",
        "| **RMSProp** | **Tieleman & Hinton**, Coursera Notes, 2012 | Exponential moving average of gradient magnitudes. |\n",
        "| **Adam** | **Kingma & Ba**, *ICLR (Best Paper)*, 2015 | Combines EMA of first and second moments + bias correction. |\n",
        "| **AMSGrad** | **Reddi et al.**, *ICLR*, 2018 | Fixed non-convergent behavior in Adam. |\n",
        "| **AdamW** | **Loshchilov & Hutter**, *ICLR*, 2019 | Decoupled weight decay; fixed L2 coupling bug. |\n",
        "| **RAdam** | **Liu et al.**, *NeurIPS*, 2019 | Variance-rectified warmup for stable early training. |\n",
        "| **AdaBound** | **Luo et al.**, *ICLR*, 2019 | Transition from Adam → SGD asymptotically. |\n",
        "| **AdaBelief** | **Zhuang et al.**, *NeurIPS*, 2020 | Uses (g−m)² variance tracking → smoother generalization. |\n",
        "| **NovoGrad** | **Ginsburg et al.**, *arXiv*, 2019 | Adam-like but cheaper; used in speech recognition. |\n",
        "| **Lion** | **Chen et al.**, *NeurIPS*, 2023 | Momentum-sign updates (L1-like); low memory; strong for ViTs/LLMs. |\n",
        "\n",
        "---\n",
        "\n",
        "## **D) Learning-Rate Schedules & Warmup**\n",
        "\n",
        "| Method | Authors / Venue / Year | Key Idea |\n",
        "|---------|------------------------|-----------|\n",
        "| **Cyclical Learning Rates** | **Smith**, *WACV*, 2017 | Periodic LR exploration; LR range test. |\n",
        "| **OneCycle Policy** | **Smith & Topin**, *arXiv*, 2019 | LR increases then decreases; momentum inverted. |\n",
        "| **Cosine Annealing (SGDR)** | **Loshchilov & Hutter**, *ICLR*, 2017 | Smooth cosine decay; inspired modern default. |\n",
        "| **Warmup** | **He et al.**, *CVPR*, 2016; **Vaswani et al.**, *NeurIPS*, 2017 | Linear warmup for stability in deep / transformer training. |\n",
        "\n",
        "---\n",
        "\n",
        "## **E) Second-Order, Natural Gradient & Preconditioning**\n",
        "\n",
        "| Method | Authors / Venue / Year | Contribution |\n",
        "|---------|------------------------|---------------|\n",
        "| **Hessian-Free Optimization** | **Martens**, *ICML*, 2010 | Practical curvature-based optimization for deep nets. |\n",
        "| **Natural Gradient** | **Amari**, *Neural Computation*, 1998 | Fisher information geometry for parameter-invariant descent. |\n",
        "| **K-FAC** | **Martens & Grosse**, *ICML*, 2015 | Kronecker-factored Fisher approximation; scalable NG. |\n",
        "| **Shampoo** | **Gupta et al.**, *ICML*, 2018; **Anil et al.**, *ICML*, 2021 | Matrix-root preconditioning; large-scale curvature. |\n",
        "| **Adafactor** | **Shazeer & Stern**, *ICML*, 2018 | Memory-efficient Adam; factored second moments. |\n",
        "| **AdaHessian** | **Yao et al.**, *AAAI*, 2021 | Curvature-aware adaptive optimizer. |\n",
        "\n",
        "---\n",
        "\n",
        "## **F) Variance Reduction (Finite-Sum Optimization)**\n",
        "\n",
        "| Method | Authors / Venue / Year | Key Concept |\n",
        "|---------|------------------------|-------------|\n",
        "| **SAG** | **Schmidt, Le Roux & Bach**, *NeurIPS*, 2013 | Memory of gradients for variance reduction. |\n",
        "| **SVRG** | **Johnson & Zhang**, *NeurIPS*, 2013 | Periodically computed full gradient snapshot. |\n",
        "| **SAGA** | **Defazio et al.**, *NeurIPS*, 2014 | Simplified SAG with unbiased updates. |\n",
        "| **Katyusha** | **Allen-Zhu**, *STOC*, 2017 | Accelerated variance reduction. |\n",
        "| **SARAH** | **Nguyen et al.**, *ICML*, 2017 | Recursive variance-reduced gradient estimator. |\n",
        "\n",
        "---\n",
        "\n",
        "## **G) Proximal, Composite, and Constraints**\n",
        "\n",
        "| Method | Authors / Venue / Year | Contribution |\n",
        "|---------|------------------------|---------------|\n",
        "| **ISTA / Proximal Gradient** | **Daubechies et al.**, *Comm. Pure Appl. Math.*, 2004 | Foundation of proximal methods. |\n",
        "| **FISTA** | **Beck & Teboulle**, *SIAM J. Imaging Sci.*, 2009 | Accelerated proximal convergence. |\n",
        "| **ADMM (survey)** | **Boyd et al.**, *Found. Trends ML*, 2011 | Unified constrained optimization via splitting. |\n",
        "| **Mirror-Prox** | **Nemirovski**, *SIAM J. Optim.*, 2004 | Saddle-point and constrained gradient frameworks. |\n",
        "\n",
        "---\n",
        "\n",
        "## **H) Nonconvex Landscapes, Saddles & Escape**\n",
        "\n",
        "| Work | Authors / Venue / Year | Contribution |\n",
        "|-------|------------------------|---------------|\n",
        "| **Strict Saddle Escape** | **Jin et al.**, *COLT*, 2017 | SGD with noise escapes saddles efficiently. |\n",
        "| **Overparameterized GD** | **Du et al.**, *ICLR*, 2019 | Why simple GD converges in NTK regime. |\n",
        "| **PL Condition** | **Karimi et al.**, *CDC*, 2016 | Linear rates possible beyond convexity. |\n",
        "\n",
        "---\n",
        "\n",
        "## **I) Sharpness, Flat Minima & Generalization-Aware Optimizers**\n",
        "\n",
        "| Method | Authors / Venue / Year | Contribution |\n",
        "|---------|------------------------|---------------|\n",
        "| **Flat Minima** | **Hochreiter & Schmidhuber**, *Neural Comput.*, 1997 | Linked flatness with generalization. |\n",
        "| **Sharp Minima (Batch Size)** | **Keskar et al.**, *ICLR*, 2017 | Large-batch → sharp minima discovery. |\n",
        "| **SWA** | **Izmailov et al.**, *UAI*, 2018 | Weight averaging → flatter basins. |\n",
        "| **SAM** | **Foret et al.**, *ICLR*, 2021 | Adversarial step in parameter space → flatness bias. |\n",
        "| **ASAM / GSAM** | **Kwon et al.**, *NeurIPS*, 2021; **Zhuang et al.**, *NeurIPS*, 2022 | Smoother, stable SAM refinements. |\n",
        "\n",
        "---\n",
        "\n",
        "## **J) Normalization as Optimization Aid**\n",
        "\n",
        "| Technique | Authors / Venue / Year | Effect |\n",
        "|------------|------------------------|--------|\n",
        "| **BatchNorm** | **Ioffe & Szegedy**, *ICML*, 2015 | Stabilizes optimization, boosts effective LR. |\n",
        "| **LayerNorm** | **Ba, Kiros & Hinton**, *arXiv*, 2016 | Crucial for transformers; scale-invariant. |\n",
        "| **GroupNorm** | **Wu & He**, *ECCV*, 2018 | Effective for small batches. |\n",
        "| **WeightNorm** | **Salimans & Kingma**, *NeurIPS*, 2016 | Reparameterization improving conditioning. |\n",
        "| **Gradient Centralization** | **Yong et al.**, *ECCV*, 2020 | Mean-centered gradients → smoother optimization. |\n",
        "\n",
        "---\n",
        "\n",
        "## **K) Large-Batch, Distributed & Communication-Efficient Training**\n",
        "\n",
        "| Technique | Authors / Venue / Year | Contribution |\n",
        "|------------|------------------------|--------------|\n",
        "| **Hogwild!** | **Recht et al.**, *NeurIPS*, 2011 | Lock-free asynchronous SGD. |\n",
        "| **Linear Scaling Rule (ImageNet 1h)** | **Goyal et al.**, *arXiv*, 2017 | Warmup + LR scaling → massive parallelism. |\n",
        "| **LARS** | **You et al.**, *arXiv*, 2017 | Layerwise rate scaling for large-batch CNNs. |\n",
        "| **LAMB** | **You et al.**, *NeurIPS*, 2019 | Large-batch BERT optimizer; layerwise adaptivity. |\n",
        "| **QSGD** | **Alistarh et al.**, *NeurIPS*, 2017 | Quantized gradients for low-bandwidth updates. |\n",
        "| **signSGD** | **Bernstein et al.**, *ICML*, 2018 | Sign-based distributed SGD with robustness. |\n",
        "\n",
        "---\n",
        "\n",
        "## **L) Classical Second-Order & Quasi-Newton**\n",
        "\n",
        "| Method | Authors / Venue / Year | Contribution |\n",
        "|---------|------------------------|---------------|\n",
        "| **L-BFGS** | **Liu & Nocedal**, *Math. Prog. B*, 1989 | Memory-limited quasi-Newton; strong small-scale performance. |\n",
        "| **Trust-Region / Line Search** | **Wolfe, Armijo**, 1960s | Conditions for step acceptance; classical backtracking. |\n",
        "| **Practical L-BFGS in DL** | **Byrd et al.**, *SIAM Review*, 1995 | Overview and limited deep-learning applications. |\n",
        "\n",
        "---\n",
        "\n",
        "## **M) Optimizer Hybrids & Meta-Optimizers**\n",
        "\n",
        "| Method | Authors / Venue / Year | Contribution |\n",
        "|---------|------------------------|---------------|\n",
        "| **Lookahead** | **Zhang et al.**, *NeurIPS*, 2019 | EMA of “fast” weights → smoother, better convergence. |\n",
        "| **Hypergradient Descent** | **Baydin et al.**, *AAAI*, 2018 | Learns learning rate online. |\n",
        "| **YellowFin** | **Zhang et al.**, *ICML*, 2017 | Auto-tunes LR and momentum for SGD. |\n",
        "| **GradNorm / Adaptive Loss Balancing** | **Chen et al.**, *ICML*, 2018 | Dynamic weighting for multi-task optimization. |\n",
        "\n",
        "---\n",
        "\n",
        "## **N) Optimization for Sequences & Stability Tricks**\n",
        "\n",
        "| Key Work | Authors / Venue / Year | Contribution |\n",
        "|-----------|------------------------|---------------|\n",
        "| **Training RNNs Difficulties** | **Pascanu, Mikolov & Bengio**, *ICML*, 2013 | Identified gradient explosion/vanish; formalized clipping. |\n",
        "| **Gradient Clipping (origin)** | **Mikolov**, *2012 thesis/tech notes* | Canonical practical clipping trick. |\n",
        "| **RMSProp in RNNs** | **Hinton Notes**, 2012 | Early use for stabilizing recurrent learning. |\n",
        "\n",
        "---\n",
        "\n",
        "## **O) Online Learning & Regret (Theoretical Roots of Adaptivity)**\n",
        "\n",
        "| Method | Authors / Venue / Year | Contribution |\n",
        "|---------|------------------------|---------------|\n",
        "| **Online Gradient Descent** | **Zinkevich**, *ICML*, 2003 | Regret bounds for online convex optimization. |\n",
        "| **Dual Averaging / FTRL** | **McMahan**, *COLT*, 2011; **Nesterov**, *Math. Program.*, 2009 | Foundations behind AdaGrad & adaptive updates. |\n",
        "\n",
        "---\n",
        "\n",
        "## **P) Geometry, Invariances & Parameterization**\n",
        "\n",
        "| Method | Authors / Venue / Year | Contribution |\n",
        "|---------|------------------------|---------------|\n",
        "| **Path-SGD** | **Neyshabur et al.**, *NeurIPS*, 2015 | Scale-invariant optimization for ReLU nets. |\n",
        "| **Scaled Weight Standardization** | **Qiao et al.**, *ICLR*, 2021 | Normalization for very deep ViT/CvT architectures. |\n",
        "\n",
        "---\n",
        "\n",
        "## **Q) Surveys & Tutorials**\n",
        "\n",
        "| Title | Authors / Venue / Year | Contribution |\n",
        "|--------|------------------------|---------------|\n",
        "| **Optimization Methods for Large-Scale ML** | **Bottou, Curtis & Nocedal**, *SIAM Review*, 2018 | Comprehensive survey bridging convex and DL optimizers. |\n",
        "| **Deep Learning Optimization Landscape** | **Goodfellow, Vinyals & Saxe**, *ICML Workshop*, 2014 | Linear structure of loss surfaces. |\n",
        "| **Generalization & Optimization Series** | **Neyshabur et al.**, *NeurIPS*, 2017–2019 | Theoretical links between generalization and optimization. |\n",
        "| **A Modern Look at Momentum** | **Goh**, Blog Notes, 2017 | Popular and clear practitioner-friendly analysis. |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "T5Nx-LWiJlO-"
      }
    }
  ]
}