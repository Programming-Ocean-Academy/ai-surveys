{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“– Breakthrough Papers in Machine Translation\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŸ¤ Statistical Era (Before Neural MT)\n",
        "\n",
        "1. **Brown et al. (1993)** â€“ *The Mathematics of Statistical Machine Translation*  \n",
        "   - IBM Models 1â€“5 â†’ introduced **probabilistic word alignments**.  \n",
        "   - Foundation of **phrase-based SMT**.  \n",
        "\n",
        "2. **Koehn et al. (2003)** â€“ *Statistical Phrase-Based Translation*  \n",
        "   - Introduced **phrase-based SMT** (Moses toolkit).  \n",
        "   - Major improvement over word-based IBM models.  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”µ Early Neural MT\n",
        "\n",
        "3. **Kalchbrenner & Blunsom (2013)** â€“ *Recurrent Continuous Translation Models*  \n",
        "   - First **full neural MT system**.  \n",
        "   - CNN encoder + RNN decoder, **continuous sentence representations**.  \n",
        "\n",
        "4. **Sutskever, Vinyals & Le (2014)** â€“ *Sequence to Sequence Learning with Neural Networks*  \n",
        "   - Introduced **encoderâ€“decoder LSTMs**.  \n",
        "   - Showed **end-to-end NMT** works better than SMT.  \n",
        "\n",
        "5. **Bahdanau, Cho & Bengio (2015)** â€“ *Neural Machine Translation by Jointly Learning to Align and Translate*  \n",
        "   - Introduced the **attention mechanism**.  \n",
        "   - Solved **fixed bottleneck problem**, robust to long sequences.  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŸ¢ Scaling NMT\n",
        "\n",
        "6. **Luong et al. (2015)** â€“ *Effective Approaches to Attention-based Neural MT*  \n",
        "   - Refined attention (**global vs local**).  \n",
        "   - Widely adopted **practical improvements**.  \n",
        "\n",
        "7. **Jean et al. (2015)** â€“ *On Using Very Large Target Vocabulary for Neural MT*  \n",
        "   - Introduced **importance sampling** â†’ scalable vocabularies.  \n",
        "\n",
        "8. **Wu et al. (2016)** â€“ *Googleâ€™s Neural Machine Translation System (GNMT)*  \n",
        "   - Large-scale NMT deployment at **Google Translate**.  \n",
        "   - 8-layer LSTMs with **residuals + coverage model**.  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŸ£ Transformer Revolution\n",
        "\n",
        "9. **Vaswani et al. (2017)** â€“ *Attention Is All You Need*  \n",
        "   - Introduced the **Transformer**.  \n",
        "   - Fully attention-based, **no recurrence**.  \n",
        "   - Became the **new backbone** for MT and beyond.  \n",
        "\n",
        "10. **Ott et al. (2018)** â€“ *Scaling Neural Machine Translation*  \n",
        "    - Showed **large-batch training + Transformers** outperform GNMT.  \n",
        "    - Introduced **fairseq** toolkit.  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŸ¡ Multilingual & Pretrained MT\n",
        "\n",
        "11. **Johnson et al. (2017, Google)** â€“ *Zero-Shot Translation with a Multilingual NMT System*  \n",
        "    - Single model for **many languages**.  \n",
        "    - Introduced **zero-shot translation**.  \n",
        "\n",
        "12. **Edunov et al. (2018)** â€“ *Understanding Back-Translation at Scale*  \n",
        "    - Showed **back-translation** is critical for NMT performance.  \n",
        "\n",
        "13. **Lample & Conneau (2019)** â€“ *Cross-lingual Language Model Pretraining (XLM)*  \n",
        "    - Combined **pretraining + NMT** â†’ big BLEU improvements.  \n",
        "\n",
        "14. **Liu et al. (2020)** â€“ *Multilingual Denoising Pretraining for Neural MT (mBART)*  \n",
        "    - **Sequence-to-sequence pretraining** with denoising.  \n",
        "    - Foundation for many **multilingual MT systems**.  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”´ Latest Breakthroughs (Large-Scale & Generative)\n",
        "\n",
        "15. **NLLB Team, Meta AI (2022)** â€“ *No Language Left Behind*  \n",
        "    - Trained a single multilingual MT system for **200+ languages**.  \n",
        "\n",
        "16. **OpenAI GPT & ChatGPT (2022â€“2023)**  \n",
        "    - Not MT-specific, but showed **LLMs can outperform supervised MT** on many benchmarks via **emergent translation ability**.  \n",
        "\n",
        "17. **SeamlessM4T (Meta, 2023)** â€“ *Massively Multilingual & Multimodal Translation*  \n",
        "    - Unified model for **speech + text translation** across **100+ languages**.  \n",
        "\n",
        "---\n",
        "\n",
        "## âœ… In Short\n",
        "- **1990sâ€“2000s:** Statistical models â†’ word-based â†’ phrase-based SMT.  \n",
        "- **2013â€“2015:** Neural encoderâ€“decoder + attention.  \n",
        "- **2017â€“2020:** Transformers + scaling + multilingual pretraining.  \n",
        "- **2022â€“2023:** LLMs + massively multilingual, multimodal models.  "
      ],
      "metadata": {
        "id": "WFEGsrNtAjNU"
      }
    }
  ]
}