{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPued4nd8I5TlJD4NBWkcbh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ“œ Self-Supervised Learning (SSL) Breakthroughs\n","\n","---\n","\n","## ðŸ”¹ NLP Embeddings (Prediction from Context)\n","\n","**Word2Vec â€“ Mikolov et al. (2013, Google)**  \n","*\"Efficient Estimation of Word Representations in Vector Space.\"*  \n","- Learned **dense word embeddings** by predicting context words.  \n","- Models: **Skip-gram** (predict context from target) and **CBOW** (predict target from context).  \n","\n","**FastText â€“ Bojanowski et al. (2016, Facebook AI)**  \n","*\"Enriching Word Vectors with Subword Information.\"*  \n","- Improved embeddings by incorporating **subword character n-grams**.  \n","- Handles **rare/compound words** better than Word2Vec.  \n","\n","---\n","\n","## ðŸ”¹ Masked Modeling (Language & Vision)\n","\n","**BERT â€“ Devlin et al. (2018, Google AI)**  \n","*\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\"*  \n","- **Masked Language Modeling (MLM):** randomly masks tokens, predicts them using bidirectional context.  \n","- Revolutionized **NLP pretraining + fine-tuning** paradigm.  \n","\n","**MAE â€“ He et al. (2021, Meta AI)**  \n","*\"Masked Autoencoders Are Scalable Vision Learners.\"*  \n","- Extends **masked modeling** to vision.  \n","- Randomly masks image patches and trains the model to reconstruct them.  \n","- Efficient large-scale pretraining for vision tasks.  \n","\n","---\n","\n","## ðŸ”¹ Contrastive Learning\n","\n","**SimCLR â€“ Chen et al. (2020, Google Brain)**  \n","*\"A Simple Framework for Contrastive Learning of Visual Representations.\"* (ICML 2020)  \n","- Maximizes **agreement between augmented views** of the same image.  \n","- Uses contrastive loss (**InfoNCE**) to learn powerful image embeddings.  \n","\n","**MoCo â€“ He et al. (2020, Facebook AI)**  \n","*\"Momentum Contrast for Unsupervised Visual Representation Learning.\"* (CVPR 2020)  \n","- Introduces a **momentum encoder** + **memory bank**.  \n","- Scales contrastive learning to large datasets and long training schedules.  \n","\n","---\n","\n","## âœ… Summary Families\n","\n","- **Embeddings:** Word2Vec (2013), FastText (2016).  \n","- **Masked modeling:** BERT (2018), MAE (2021).  \n","- **Contrastive learning:** SimCLR (2020), MoCo (2020).  \n","\n","---\n","\n","## ðŸ‘‰ Why It Matters\n","\n","- **NLP:** Self-supervised learning (BERT, GPT-style pretraining) dominates modern language models.  \n","- **Computer Vision:** Masked autoencoders and contrastive methods now rival or surpass supervised CNNs.  \n","- **Speech/Audio:** SSL extended to acoustic data (e.g., **Wav2Vec 2.0, 2020**), enabling low-resource ASR.  \n","\n","ðŸ”‘ **Takeaway:** SSL shifted AI from **label-hungry supervised training** to **scalable pretraining on unlabeled data**, powering todayâ€™s **foundation models**.  \n"],"metadata":{"id":"xHDmEoqYI5IA"}},{"cell_type":"markdown","source":["# ðŸ“œ Self-Supervised Learning (SSL) Breakthroughs\n","\n","---\n","\n","## ðŸ“š Key Milestones\n","\n","| **Era** | **Model / Concept** | **Year** | **Authors / Org** | **Key Contributions** |\n","|---------|----------------------|----------|-------------------|-----------------------|\n","| **NLP Embeddings (Prediction from Context)** | **Word2Vec** | 2013 | Mikolov et al., Google | Skip-gram & CBOW objectives; learned embeddings from raw text. |\n","| | **FastText** | 2016 | Bojanowski et al., Facebook AI | Word representations as bags of n-grams; improved rare/compound word handling. |\n","| **Masked Language / Representation Modeling** | **BERT** | 2018 | Devlin et al., Google AI | Masked tokens â†’ deep bidirectional context learning. |\n","| | **XLNet** | 2019 | Yang et al. | Permutation-based autoregression; alternative to masking. |\n","| | **MAE (Masked Autoencoders)** | 2021 | He et al., Meta AI | Extended masked modeling to vision; scalable pretraining for images. |\n","| **Contrastive Learning (Positive vs Negative Pairs)** | **SimCLR** | 2020 | Chen et al., Google Brain | Contrastive loss with augmented image views; new SSL vision benchmarks. |\n","| | **MoCo (Momentum Contrast)** | 2020 | He et al., Facebook AI | Queue-based memory bank for scalable contrastive learning. |\n","| | **BYOL (Bootstrap Your Own Latent)** | 2020 | Grill et al., DeepMind | Contrastive-free SSL; stable emergent representations without negatives. |\n","| **Unified & Multimodal SSL** | **Wav2Vec 2.0** | 2020 | Baevski et al., Facebook AI | SSL breakthrough in speech/audio â†’ low-resource ASR. |\n","| | **CLIP** | 2021 | Radford et al., OpenAI | Trained on 400M imageâ€“text pairs; aligned multimodal embeddings. |\n","| | **DINO** | 2021 | Caron et al., Facebook AI | Teacherâ€“student SSL for Vision Transformers; no labels required. |\n","\n","---\n","\n","## âœ… Summary Families\n","- **Embeddings:** Word2Vec (2013), FastText (2016).  \n","- **Masked modeling:** BERT (2018), XLNet (2019), MAE (2021).  \n","- **Contrastive learning:** SimCLR (2020), MoCo (2020), BYOL (2020).  \n","- **Multimodal SSL:** Wav2Vec 2.0 (2020), CLIP (2021), DINO (2021).  \n"],"metadata":{"id":"5Hmj9GsqduDN"}}]}