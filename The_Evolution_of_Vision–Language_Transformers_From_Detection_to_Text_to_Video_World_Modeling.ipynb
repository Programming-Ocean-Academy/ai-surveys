{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Evolution of Vision–Language Transformers: From Detection to Text-to-Video World Modeling\n",
        "\n",
        "# https://arxiv.org/abs/2203.03605\n",
        "\n",
        "# https://arxiv.org/abs/2405.10300\n",
        "\n",
        "# https://arxiv.org/abs/2403.07944"
      ],
      "metadata": {
        "id": "5nXhSqJhd4j4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text-to-Video Generation: The Emergence of “World Models”\n",
        "\n",
        "## 1. Introduction\n",
        "Text-to-video (T2V) generation is the process of creating coherent, realistic videos directly from natural language.  \n",
        "To do this effectively, a model must integrate three abilities:\n",
        "\n",
        "- **Spatial grounding** – identifying what objects exist and where they are.  \n",
        "- **Semantic grounding** – understanding what the text means.  \n",
        "- **Temporal generation** – learning how scenes evolve over time.\n",
        "\n",
        "Three major milestones illustrate this evolution:\n",
        "- **DINO** built reliable spatial perception.  \n",
        "- **Grounding DINO 1.5** merged text and vision for open-set semantic grounding.  \n",
        "- **WorldGPT** unified perception and language with diffusion-based temporal synthesis.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Spatial Foundations: DINO and End-to-End Detection\n",
        "**DINO (DETR with Improved DeNoising Anchor Boxes)** marked a turning point in visual perception.  \n",
        "It improved transformer detectors through:\n",
        "- Contrastive denoising training for stable learning.  \n",
        "- Mixed query selection to better initialize anchors.  \n",
        "- Look-forward-twice optimization to speed convergence.  \n",
        "\n",
        "DINO achieved record performance (63.3 AP on COCO) while reducing training cost.  \n",
        "Crucially, its “dynamic anchor boxes” became reusable **scene priors** for generative tasks — allowing video systems to track objects consistently across frames.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Semantic Grounding: From Vision to Language with Grounding DINO 1.5\n",
        "**Grounding DINO 1.5** built upon DINO’s perception by adding language understanding.  \n",
        "It combined image and text features in a dual-encoder transformer, trained on 20 million image–text pairs, achieving state-of-the-art open-vocabulary detection.\n",
        "\n",
        "Key innovations:\n",
        "- **Early vision-language fusion** – improved recall but introduced mild hallucination risks.  \n",
        "- **Edge optimization** – lighter attention modules enabled real-time inference (~75 FPS).  \n",
        "\n",
        "In the T2V pipeline, this stage links text descriptions to spatial constraints — ensuring that generated objects match the prompt and remain consistent over time.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Temporal Generation: WorldGPT and Diffusion-Based “World Models”\n",
        "**WorldGPT**, inspired by OpenAI’s Sora, integrates perception and grounding into a full video-generation system.  \n",
        "It operates in three steps:\n",
        "\n",
        "1. **Prompt Enhancer (LLM)** – uses ChatGPT to expand a text input into detailed, structured sub-prompts.  \n",
        "2. **Key-frame Synthesis** – employs Grounding DINO for object detection and Stable Diffusion for key-frame creation.  \n",
        "3. **Video Translation** – uses DynamiCrafter, a diffusion model guided by motion fields, to interpolate between frames.  \n",
        "\n",
        "Results on **AIGCBench** show superior temporal consistency (CLIP ≈ 0.992) and semantic alignment compared to prior models such as DynamiCrafter and I2VGen-XL.  \n",
        "Although per-frame quality is slightly lower, the videos display improved **narrative flow** and **continuity**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. A Unified Three-Layer Perspective\n",
        "These systems form a layered architecture for T2V research:\n",
        "\n",
        "| Layer | Core Function | Representative Model | Output |\n",
        "|:--|:--|:--|:--|\n",
        "| Perception | Detect what and where | DINO | Spatial priors and object identity |\n",
        "| Grounding | Link text to scene | Grounding DINO 1.5 | Semantic alignment and constraints |\n",
        "| Generation | Model how things move | WorldGPT | Temporally consistent video synthesis |\n",
        "\n",
        "Together, they transform text into structured worlds that move, rather than isolated frames — embodying the idea of **world modeling**.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Key Challenges\n",
        "- **Temporal vs. visual fidelity** – maintaining long-term motion without blurring single frames.  \n",
        "- **Hallucination control** – balancing early fusion’s recall with precision.  \n",
        "- **Object persistence** – ensuring entities remain stable through time (potentially via “temporal denoising”).  \n",
        "- **Efficiency and deployment** – optimizing for edge hardware while preserving accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Evaluation Benchmarks\n",
        "- **Detection & grounding:** COCO, LVIS, ODinW (for open-vocabulary performance).  \n",
        "- **Video generation:** AIGCBench (measuring alignment, motion, temporal consistency, and frame quality).\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Future Outlook\n",
        "- Unified training of perception, grounding, and generation could minimize error propagation.  \n",
        "- Physics-informed priors may improve long-range motion realism.  \n",
        "- Controllable editing will require tighter fusion of grounding and diffusion layers.  \n",
        "- Better evaluation metrics should jointly reflect coherence, quality, and realism.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Conclusion\n",
        "Text-to-video research is evolving from **static image generation** toward **dynamic world modeling**.  \n",
        "By combining DINO’s spatial reasoning, Grounding DINO’s semantic alignment, and WorldGPT’s diffusion-based temporal synthesis, the field is converging on a single goal:\n",
        "\n",
        "**Building AI systems that understand, simulate, and narrate the world from text.**\n"
      ],
      "metadata": {
        "id": "lADipX3jeC2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Related Work\n",
        "\n",
        "| **Title / Authors** | **Year** | **Key Contribution** | **Relevance to DINO → Grounding DINO → WorldGPT Evolution** |\n",
        "|:--|:--:|:--|:--|\n",
        "| **DETR: End-to-End Object Detection with Transformers (Carion et al.)** | 2020 | Introduced transformer architecture for object detection without anchors or NMS; reframed detection as set prediction. | Foundation for all DETR-like models including DINO and Grounding DINO. |\n",
        "| **Deformable DETR (Zhu et al.)** | 2020 | Improved DETR’s slow convergence via deformable attention on sparse key points. | Backbone mechanism for DINO’s efficiency. |\n",
        "| **DAB-DETR: Dynamic Anchor Boxes for DETR (Liu et al.)** | 2022 | Added anchor-based query formulation for better convergence. | DINO builds directly upon this for mixed query initialization. |\n",
        "| **DN-DETR: Denoising Training for Faster DETR Convergence (Li et al.)** | 2022 | Stabilized bipartite matching with denoising ground-truth boxes. | DINO’s “Improved DeNoising Anchor Boxes” derives from this. |\n",
        "| **Swin Transformer / Swin V2 (Liu et al.)** | 2021–2022 | Hierarchical vision transformer with shifted windows; improved scalability. | Used as DINO’s and Grounding DINO’s visual backbone. |\n",
        "| **HTC++ / Hybrid Task Cascade (Chen et al.)** | 2019 | Multi-stage detector combining region proposals and segmentation. | Baseline comparison for DINO in detection benchmarks. |\n",
        "| **DyHead (Dai et al.)** | 2021 | Dynamic head architecture for joint detection and segmentation. | Compared in COCO leaderboard with DINO. |\n",
        "| **Objects365 Dataset (Shao et al.)** | 2019 | Large-scale dataset (1.7M images) for object detection. | DINO and Grounding DINO pre-training dataset. |\n",
        "| **COCO Dataset (Lin et al.)** | 2014 | Benchmark for detection and segmentation tasks. | Standard benchmark across all models. |\n",
        "| **Conditional DETR (Meng et al.)** | 2021 | Conditioned cross-attention queries to improve DETR’s learning. | Cited in DINO’s query design. |\n",
        "| **Efficient DETR (Yao et al.)** | 2021 | Optimized DETR with sparse encoder-decoder attention and top-K query selection. | Influenced DINO’s mixed query selection. |\n",
        "| **Florence (Yuan et al.)** | 2021 | Multimodal foundation model trained on image–text pairs. | Compared with DINO and Grounding DINO as a large-scale vision–language baseline. |\n",
        "| **ViT: Vision Transformer (Dosovitskiy et al.)** | 2020 | Showed transformers can outperform CNNs on vision tasks. | Backbone for Grounding DINO 1.5 Pro. |\n",
        "| **GLIP: Grounded Language-Image Pretraining (Li et al.)** | 2022 | Unified detection and phrase grounding through large-scale image–text training. | Precursor to Grounding DINO’s open-set detection paradigm. |\n",
        "| **OWL-ViT (Minderer et al.)** | 2022 | Zero-shot object detection using CLIP text–vision alignment. | Compared with Grounding DINO for zero-shot transfer. |\n",
        "| **DetCLIP / DetCLIP v3 (Zhou et al.)** | 2023 | CLIP-based detector bridging grounding and detection benchmarks. | Baseline for Grounding DINO 1.5’s improvement claims. |\n",
        "| **OmDet-Turbo (Zhang et al.)** | 2023 | Efficient multi-dataset open-vocabulary detection model. | Alternative zero-shot detector compared in Grounding DINO 1.5. |\n",
        "| **OpenSeeD / UniDetector** | 2023 | Open-world detectors integrating CLIP and DETR-style backbones. | Baselines for LVIS and ODinW benchmarks. |\n",
        "| **ODinW (Object Detection in the Wild) (Li et al.)** | 2022 | Benchmark for evaluating generalization of detectors across 35 datasets. | Used by Grounding DINO 1.5 for zero-shot evaluation. |\n",
        "| **MDETR (Kamath et al.)** | 2021 | Multimodal DETR combining image and text inputs for grounded reasoning. | The conceptual bridge between DETR and language-aware detection. |\n",
        "| **GLIPv2 (Li et al.)** | 2023 | Enhanced grounded pretraining with larger multimodal datasets. | Direct comparison model in Grounding DINO 1.5. |\n",
        "| **APE (Any-Prompt Evaluation)** | 2023 | Prompt-based open-vocabulary object detection. | Evaluated alongside Grounding DINO 1.5. |\n",
        "| **GLEE-Pro** | 2023 | ViT-based grounding model scaling to 10M merged datasets. | Another comparison in LVIS benchmark tables. |\n",
        "| **T-Rex2** | 2023 | Large vision–language detection system with text and visual prompts. | Compared in ODinW benchmarks. |\n",
        "| **Lite-DETR** | 2023 | Reduced-complexity DETR variant optimized for mobile inference. | Influenced Grounding DINO 1.5 Edge’s efficiency design. |\n",
        "| **EfficientViT-L1** | 2023 | Lightweight vision transformer for mobile/edge computing. | Backbone for Grounding DINO 1.5 Edge. |\n",
        "| **Stable Diffusion (Rombach et al.)** | 2022 | Latent diffusion model for efficient text-to-image generation. | Core image generator in WorldGPT’s key-frame synthesis. |\n",
        "| **CLIP (Radford et al.)** | 2021 | Unified vision–language model for zero-shot recognition. | Provides text embeddings for Stable Diffusion and Grounding DINO. |\n",
        "| **DALL-E / DALL-E 2 (Ramesh et al.)** | 2021–2022 | Transformer-based text-to-image model using discrete VAE. | Referenced in WorldGPT as inspiration for visual synthesis. |\n",
        "| **Imagen (Saharia et al.)** | 2022 | High-fidelity text-to-image diffusion using large T5 text encoders. | Baseline for diffusion quality. |\n",
        "| **CogView 2** | 2021 | Chinese multimodal text-to-image model. | Compared in background of WorldGPT. |\n",
        "| **DynamiCrafter** | 2023 | Image-to-video generation using optical flow & appearance modeling. | Core engine for temporal consistency in WorldGPT. |\n",
        "| **I2VGen-XL** | 2023 | Image-to-video generation via large diffusion backbone. | Benchmark compared to WorldGPT in AIGCBench results. |\n",
        "| **Sora (OpenAI)** | 2024 | Closed-source diffusion transformer for text-to-video. | Conceptual blueprint for WorldGPT’s design. |\n",
        "| **U-Net (Ronneberger et al.)** | 2015 | Encoder–decoder CNN for segmentation, later repurposed for diffusion denoising. | Backbone in Stable Diffusion pipeline. |\n",
        "| **Optical Flow Models (Horn & Schunck; RAFT)** | 1981–2020 | Compute pixel-level motion across frames. | Basis for DynamiCrafter’s motion field modeling in WorldGPT. |\n",
        "| **Large Language Models (GPT, BERT, etc.)** | 2018–2023 | Deep transformer architectures for generative text understanding. | Used in WorldGPT’s “Prompt Enhancer” to refine video instructions. |\n",
        "\n",
        "---\n",
        "\n",
        "## Observational Summary\n",
        "**Detection era (2014–2022):** COCO, DETR, and DINO established the transformer detection paradigm.  \n",
        "**Grounding era (2022–2024):** GLIP → Grounding DINO integrated language grounding and open-vocabulary detection.  \n",
        "**Generative era (2022–2024):** Stable Diffusion → DynamiCrafter → WorldGPT → Sora extended these concepts into temporal generative models.\n",
        "\n",
        "The intellectual lineage shows a continuous shift:  \n",
        "**Static vision (DETR)** → **Text–vision fusion (Grounding DINO)** → **Dynamic world generation (WorldGPT / Sora)**.\n"
      ],
      "metadata": {
        "id": "s0wbVzO5d9as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        " ┌─────────────────────────────────────────────────────────────────────────────┐\n",
        " │                THE EVOLUTION OF VISION–LANGUAGE TRANSFORMERS               │\n",
        " │                   From Detection → Grounding → World Modeling              │\n",
        " └─────────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "        ┌─────────────────────────────────────────────────────────────┐\n",
        "        │  DETR (Carion et al., 2020)                                 │\n",
        "        │  • Transformer-based object detection                       │\n",
        "        │  • Replaced anchors/NMS with set prediction                 │\n",
        "        │  → FOUNDATION: Unified attention for perception             │\n",
        "        └──────────────┬──────────────────────────────────────────────┘\n",
        "                       │\n",
        "                       ▼\n",
        "        ┌─────────────────────────────────────────────────────────────┐\n",
        "        │  DINO (Zhang et al., 2022)                                 │\n",
        "        │  • Improved DETR via de-noising anchors & mixed queries     │\n",
        "        │  • Contrastive training for robust feature learning         │\n",
        "        │  • Outputs reliable spatial priors                         │\n",
        "        │  → LAYER 1: Spatial Perception                             │\n",
        "        └──────────────┬──────────────────────────────────────────────┘\n",
        "                       │\n",
        "                       ▼\n",
        "        ┌─────────────────────────────────────────────────────────────┐\n",
        "        │  Grounding DINO 1.5 (Ren et al., 2024)                     │\n",
        "        │  • Dual-encoder Transformer (vision + text)                 │\n",
        "        │  • Trained on 20 M image-text pairs (Grounding-20M)        │\n",
        "        │  • Early fusion + Edge variant (75 FPS)                    │\n",
        "        │  → LAYER 2: Semantic Grounding (open-vocabulary detection)  │\n",
        "        └──────────────┬──────────────────────────────────────────────┘\n",
        "                       │\n",
        "                       ▼\n",
        "        ┌─────────────────────────────────────────────────────────────┐\n",
        "        │  WorldGPT (Yang et al., 2024)                               │\n",
        "        │  • LLM-enhanced prompt structuring (ChatGPT)                │\n",
        "        │  • Uses Grounding DINO + Stable Diffusion + DynamiCrafter   │\n",
        "        │  • Generates videos with temporal consistency (CLIP≈0.992)  │\n",
        "        │  → LAYER 3: Temporal Generation (World Modeling)            │\n",
        "        └──────────────┬──────────────────────────────────────────────┘\n",
        "                       │\n",
        "                       ▼\n",
        "        ┌─────────────────────────────────────────────────────────────┐\n",
        "        │  Sora (OpenAI, 2024 conceptual inspiration)                 │\n",
        "        │  • Diffusion Transformer for text-to-video simulation        │\n",
        "        │  • Unified spatiotemporal world understanding                │\n",
        "        │  → FUTURE: End-to-End World Models (Perceive→Imagine→Act)   │\n",
        "        └─────────────────────────────────────────────────────────────┘\n",
        "\n",
        "Key:  \n",
        "   ─►   Conceptual/temporal progression  \n",
        "   LAYER 1 = Spatial Perception LAYER 2 = Semantic Grounding LAYER 3 = Temporal Generation\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "iSzbR-LDfIG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Terminology, Acronyms, and Symbols — *DINO + Grounding DINO Frameworks*\n",
        "\n",
        "| **Term / Symbol** | **Full Form or Definition** | **Role / Explanation in Context** |\n",
        "|:--|:--|:--|\n",
        "| **DINO** | DETR with Improved DeNoising Anchor Boxes | Main proposed model; end-to-end transformer detector improving DETR’s convergence via contrastive denoising and mixed query selection. |\n",
        "| **DETR** | DEtection TRansformer | Foundational transformer-based detector replacing region proposals with bipartite matching; baseline for DINO and successors. |\n",
        "| **DN-DETR** | Denoising DETR | Predecessor introducing denoising (noisy box augmentation) to stabilize DETR training; DINO enhances this with contrastive learning. |\n",
        "| **DAB-DETR** | Dynamic Anchor Boxes DETR | Uses anchor-shaped queries to improve spatial initialization; DINO integrates and refines this mechanism. |\n",
        "| **DETR-like Models** | – | General family of transformer-based detectors treating detection as set prediction, eliminating NMS post-processing. |\n",
        "| **Anchor Boxes** | – | Bounding box priors or learned initializations for detection queries; dynamically refined in DINO’s decoder. |\n",
        "| **Query Embedding (Q)** | \\( Q \\) | Latent query vectors representing object hypotheses; attend to spatial features for localization. |\n",
        "| **Key Embedding (K)** | \\( K \\) | Encoder-derived spatial features that serve as reference keys in attention computation. |\n",
        "| **Value Embedding (V)** | \\( V \\) | Visual feature content associated with each key, attended to by queries for refinement. |\n",
        "| **Mixed Query Selection (MQS)** | – | Hybrid query initialization combining learned embeddings with top-K anchor features for faster convergence and better recall. |\n",
        "| **Contrastive DeNoising (CDN)** | – | Adds positive/negative noise around ground-truth boxes, teaching the model to distinguish true objects from near-duplicates. |\n",
        "| **Positive / Hard Negative Samples** | – | Perturbed box samples used for contrastive denoising, enhancing robustness and spatial discrimination. |\n",
        "| **Look-Forward-Twice (LFT)** | – | Two-pass refinement mechanism in DINO’s decoder improving gradient flow and box accuracy. |\n",
        "| **Swin Transformer** | Shifted Window Transformer | Hierarchical backbone using windowed self-attention; used in DINO and Grounding DINO (Pro). |\n",
        "| **ResNet-50 / ResNet-101** | Residual Networks | CNN backbones providing baseline comparisons for DETR variants. |\n",
        "| **COCO** | Common Objects in Context | Standard detection benchmark; used to evaluate DINO and Grounding DINO via Average Precision. |\n",
        "| **AP** | Average Precision | Detection metric summarizing precision–recall trade-off across IoU thresholds. |\n",
        "| **IoU** | Intersection over Union | Overlap ratio between predicted and ground-truth boxes; key for AP and denoising losses. |\n",
        "| **Objects365** | – | Large-scale detection dataset (1.7M images) used for pretraining DINO variants. |\n",
        "| **LVIS** | Large Vocabulary Instance Segmentation | Benchmark for open-vocabulary and fine-grained detection evaluation. |\n",
        "| **ODinW** | Object Detection in the Wild | Cross-domain benchmark testing zero-shot generalization. |\n",
        "| **DN Loss** | Denoising Loss | Auxiliary loss penalizing deviation on noisy boxes; contrastively reformulated in DINO. |\n",
        "| **Hungarian Matching** | – | One-to-one assignment algorithm aligning predicted and GT boxes; core to DETR’s set prediction. |\n",
        "| **Grounding DINO** | – | Extends DINO to **open-vocabulary detection** by integrating text embeddings into visual queries. |\n",
        "| **Vision–Language Fusion** | – | Mechanism combining text and visual features; enables cross-modal reasoning and grounding. |\n",
        "| **Text Encoder** | – | Transforms input phrases into embeddings (often CLIP/GLIP pretrained) for conditioning detection queries. |\n",
        "| **Image Encoder** | – | Extracts spatial feature maps from images (Swin Transformer or CNN backbone). |\n",
        "| **Dual-Encoder Architecture** | – | Separate text and image encoders fused downstream via cross-attention. |\n",
        "| **Fusion Module** | – | Integrates textual and visual representations; typically implemented as cross-attention layers. |\n",
        "| **Open-Vocabulary Detection (OVD)** | – | Task of detecting objects described by arbitrary natural-language labels, not limited to fixed categories. |\n",
        "| **Grounded Pretraining** | – | Aligning textual phrases and visual regions during large-scale pretraining to enable zero-shot grounding. |\n",
        "| **Pro Model** | – | High-capacity Grounding DINO model using **ViT-L** backbone; optimized for accuracy. |\n",
        "| **Edge Model** | – | Lightweight Grounding DINO variant using **EfficientViT-L1**; optimized for real-time deployment (~75 FPS). |\n",
        "| **ViT-L / ViT-B** | Vision Transformer (Large / Base) | Transformer backbones for scalable visual encoding. |\n",
        "| **CLIP** | Contrastive Language–Image Pretraining | Foundation model for shared text–image embeddings; used for pretraining and evaluation alignment. |\n",
        "| **GLIP** | Grounded Language–Image Pretraining | Earlier model unifying grounding and detection; conceptual predecessor to Grounding DINO. |\n",
        "| **Zero-Shot Detection** | – | Evaluation of unseen categories using text-based queries. |\n",
        "| **Prompt (p)** | \\( p \\) | Text input describing target objects; conditions the visual grounding module. |\n",
        "| **Embedding Space (E)** | \\( E_{text}, E_{image} \\) | Shared latent space where visual and textual embeddings interact for alignment. |\n",
        "| **Attention Mechanism** | – | Core transformer operation linking queries, keys, and values; enables spatial and semantic refinement. |\n",
        "| **Cross-Attention** | – | Connects visual queries to textual keys/values; crucial for multimodal grounding. |\n",
        "| **Decoder Layer (L)** | \\( L_d \\) | Transformer layer refining query embeddings iteratively in decoding. |\n",
        "| **Feature Pyramid (FPN)** | – | Multi-scale feature extractor aiding detection of small/large objects. |\n",
        "| **Deformable Attention** | – | Sparse attention variant reducing computational load by sampling key positions. |\n",
        "| **Multi-Scale Features** | – | Visual representations across resolutions to improve localization granularity. |\n",
        "| **Backbone (B)** | \\( B(x) \\) | Base feature extractor applied to image \\( x \\); outputs encoder tokens. |\n",
        "| **Embedding Dimension (d)** | \\( d \\) | Dimensionality of latent token vectors; typically 256–1024 in DETR-family models. |\n",
        "| **Output Logits** | \\( \\hat{y} \\) | Raw model predictions for class scores and bounding box coordinates. |\n",
        "| **Box Regression Head** | – | Predicts bounding box coordinates from decoder outputs. |\n",
        "| **Classification Head** | – | Predicts class logits from decoder query embeddings. |\n",
        "| **Contrastive Matching** | – | Compares positive and negative samples in embedding space to enhance discriminative learning. |\n",
        "| **Tokenization** | – | Converts text into token sequences for embedding. |\n",
        "| **Positional Encoding** | – | Adds spatial or sequential order to transformer tokens. |\n",
        "| **Training Epoch (E)** | \\( E \\) | One full training pass over data; DINO typically trained for 12–36 epochs. |\n",
        "| **Learning Rate (lr)** | \\( \\eta \\) | Step size in optimization; critical for stable DETR-family convergence. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "The **DINO → Grounding DINO** progression illustrates the evolution from **spatially grounded detection** to **semantically aligned open-vocabulary perception**.  \n",
        "Through innovations like **Contrastive DeNoising**, **Mixed Query Selection**, and **vision–language fusion**, these models collectively redefine the transformer detection paradigm — moving from closed-set object recognition to **generalized, multimodal scene understanding**.\n"
      ],
      "metadata": {
        "id": "RAx01t1jkkTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text-to-Video Models Overview\n",
        "\n",
        "| **Model Name** | **Year** | **Backbone** | **Task** | **Group** |\n",
        "|:--|:--:|:--|:--|:--|\n",
        "| Imagen Video | 2022 | Diffusion | Generation | Google |\n",
        "| Pix2Seq-D | 2022 | Diffusion | Segmentation | Google Deepmind |\n",
        "| FDIM | 2022 | Diffusion | Prediction | Google |\n",
        "| MaskViT | 2022 | Masked Vision Models | Prediction | Stanford, Salesforce |\n",
        "| CogVideo | 2022 | Auto-regressive | Generation | THU |\n",
        "| Make-a-Video | 2022 | Diffusion | Generation | Meta |\n",
        "| MagicVideo | 2022 | Diffusion | Generation | ByteDance |\n",
        "| TATS | 2022 | Auto-regressive | Generation | University of Maryland, Meta |\n",
        "| Phenaki | 2022 | Masked Vision Models | Generation | Google Brain |\n",
        "| Gen-1 | 2023 | Diffusion | Generation | RunwayML |\n",
        "| LFDMM | 2023 | Diffusion | Prediction | PSU, UCSD |\n",
        "| Text2Video-Zero | 2023 | Diffusion | Generation | Picsart |\n",
        "| Video Fusion | 2023 | Diffusion | Generation | USAC, Alibaba |\n",
        "| PyCo | 2023 | Diffusion | Generation | Nvidia |\n",
        "| Video LDM | 2023 | Diffusion | Generation | University of Maryland, Nvidia |\n",
        "| RIN | 2023 | Diffusion | Generation | Google Brain |\n",
        "| LVD | 2023 | Diffusion | Generation | UCB |\n",
        "| Dreamix | 2023 | Diffusion | Generation | Google Brain |\n",
        "| MagicEdit | 2023 | Diffusion | Editing | ByteDance |\n",
        "| Control-A-Video | 2023 | Diffusion | Editing | Sun Yat-Sen University |\n",
        "| Stable-Video | 2023 | Diffusion | Generation | ZJU, MSRA |\n",
        "| Tune-A-Video | 2023 | Diffusion | Editing | NUS |\n",
        "| Rerender-A-Video | 2023 | Diffusion | Editing | Adobe, UCL |\n",
        "| Pix2Video | 2023 | Diffusion | Editing | ZJU |\n",
        "| InstructVideo | 2023 | Diffusion | Editing | ZJU |\n",
        "| DiffAct | 2023 | Diffusion | Pose Estimation | Fudan University |\n",
        "| DiffPose | 2023 | Diffusion | Pose Estimation | Fudan University |\n",
        "| MAGVIT | 2023 | Masked Vision Models | Generation | Google |\n",
        "| AnimateDiff | 2023 | Diffusion | Generation | Anonymous |\n",
        "| MAGVIT V2 | 2023 | Masked Vision Models | Generation | Google |\n",
        "| Generative Dynamics | 2023 | Diffusion | Generation | EasyWithAI |\n",
        "| VideoCrafter | 2023 | Diffusion | Generation | Tencent |\n",
        "| Zeroscope | 2023 | - | Generation | RunwayML |\n",
        "| ModelScope | 2023 | - | Generation | RunwayML |\n",
        "| Gen-2 | 2023 | - | Generation | Pika Labs |\n",
        "| Pika | 2023 | Diffusion | Generation | Pika Labs |\n",
        "| Emu Video | 2023 | Diffusion | Generation | Meta |\n",
        "| PixelDance | 2023 | Diffusion | Generation | ByteDance |\n",
        "| Stable Video Diffusion | 2023 | Diffusion | Generation | StabilityAI |\n",
        "| W.A.L.T. | 2023 | Diffusion | Generation | Stanford, Google |\n",
        "| Fairy | 2023 | Diffusion | Generation, Editing | Huawei |\n",
        "| VideoPoet | 2023 | Auto-regressive | Generation | Google |\n",
        "| GenTron | 2024 | Diffusion | Generation | HKU & Meta |\n",
        "| LGV | 2024 | Diffusion | Generation | Adobe |\n",
        "| Lumiere | 2024 | Diffusion | Generation | Google |\n",
        "| Sora | 2024 | Diffusion | Generation, Editing | OpenAI |\n",
        "\n",
        " **Notes:**\n",
        "- All entries are transcribed exactly from the source table.  \n",
        "- Bracketed citation indices (e.g., [29]) have been removed.  \n",
        "- Column alignment and capitalization are preserved for clarity and academic readability.\n"
      ],
      "metadata": {
        "id": "hlz_xDFevQMX"
      }
    }
  ]
}