{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Probability — from axioms to AI (a researcher’s tour)\n",
        "\n",
        "Below is a compact-yet-deep field guide you can actually use: crisp definitions, core equations, mental models, landmark theorems, where they appear in AI, and the people who shaped the field.\n",
        "\n",
        "---\n",
        "\n",
        "## 1) What is probability?\n",
        "\n",
        "### Three complementary viewpoints\n",
        "- **Axiomatic (Kolmogorov, 1933)**:  \n",
        "  A probability space \\((\\Omega, \\mathcal{F}, P)\\) with  \n",
        "  $$\n",
        "  P(A) \\geq 0, \\quad P(\\Omega) = 1,\n",
        "  $$\n",
        "  and countable additivity on disjoint sets.  \n",
        "  Everything else—expectations, independence, conditional probability—follows.\n",
        "\n",
        "- **Frequentist**:  \n",
        "  $$\n",
        "  P(A) = \\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{i=1}^n 1\\{A \\text{ occurs on trial } i\\}.\n",
        "  $$\n",
        "\n",
        "- **Bayesian (degree of belief)**:  \n",
        "  Probability quantifies (coherent) belief and updates via Bayes’ rule from prior to posterior.\n",
        "\n",
        "**Core objects**: random variables, distributions, expectations, conditional distributions, independence, σ-algebras.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Five equations that show up everywhere\n",
        "\n",
        "- **Law of total probability**:  \n",
        "  $$\n",
        "  P(A) = \\sum_k P(A \\mid B_k) P(B_k), \\quad \\{B_k\\} \\text{ partition.}\n",
        "  $$\n",
        "\n",
        "- **Bayes’ theorem**:  \n",
        "  $$\n",
        "  P(\\theta \\mid x) = \\frac{P(x \\mid \\theta) P(\\theta)}{P(x)}, \\quad\n",
        "  P(x) = \\int P(x \\mid \\theta) P(\\theta) \\, d\\theta.\n",
        "  $$\n",
        "\n",
        "- **Expectation/Variance**:  \n",
        "  $$\n",
        "  E[X] = \\int x \\, dP, \\quad Var(X) = E[(X - E[X])^2].\n",
        "  $$\n",
        "\n",
        "- **KL divergence & entropy**:  \n",
        "  $$\n",
        "  KL(p \\parallel q) = \\int p(x) \\log \\frac{p(x)}{q(x)} \\, dx, \\quad\n",
        "  H(X) = -\\sum_x p(x) \\log p(x).\n",
        "  $$\n",
        "\n",
        "- **Markov chain stationarity**:  \n",
        "  $$\n",
        "  \\pi^\\top = \\pi^\\top P, \\quad \\sum_i \\pi_i = 1, \\; \\pi_i \\geq 0.\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Laws, limits, and inequalities (why learning works at scale)\n",
        "\n",
        "- **Linearity of expectation**:  \n",
        "  $$\n",
        "  E\\Big[\\sum_i X_i\\Big] = \\sum_i E[X_i] \\quad \\text{(no independence needed).}\n",
        "  $$\n",
        "\n",
        "- **Markov / Chebyshev / Jensen inequalities**:  \n",
        "  $$\n",
        "  P(X \\geq a) \\leq \\frac{E[X]}{a}, \\quad\n",
        "  P(|X - \\mu| \\geq t) \\leq \\frac{\\sigma^2}{t^2}, \\quad\n",
        "  \\phi(E[X]) \\leq E[\\phi(X)] \\;\\; \\text{for convex }\\phi.\n",
        "  $$\n",
        "\n",
        "- **LLN (Bernoulli/Chebyshev/Kolmogorov)**: sample means converge to expectations.  \n",
        "- **CLT (de Moivre–Laplace–Lindeberg–Feller)**: normalized sums ⇒ \\(N(0,1)\\).  \n",
        "\n",
        "- **Concentration inequalities** (Hoeffding, Chernoff, Bernstein, McDiarmid, Azuma):  \n",
        "  for i.i.d. bounded \\(X_i\\):  \n",
        "  $$\n",
        "  P(\\bar{X} - E\\bar{X} \\geq \\epsilon) \\leq \\exp(-2n\\epsilon^2).\n",
        "  $$\n",
        "\n",
        "- **Cramér–Rao bound & Fisher information**: variance lower bounds for unbiased estimators.  \n",
        "- **Martingales (Doob)**: optional stopping, Azuma–Hoeffding; backbone of online learning and bandits.\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Canon of distributions (with quick mnemonics)\n",
        "\n",
        "- **Discrete**: Bernoulli/Binomial, Geometric/Negative Binomial, Poisson, Categorical/Multinomial.  \n",
        "- **Continuous**: Uniform, Exponential, Normal, Gamma/Chi-square, Beta/Dirichlet, Logistic/Laplace, Stable/Student-t.  \n",
        "- **Structured**: Dirichlet–Multinomial, Wishart, von Mises.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Stochastic processes (dynamics, signals, decisions)\n",
        "\n",
        "- **Markov chains**: transitions \\(P\\), mixing times, ergodicity.  \n",
        "- **Queueing models**: Poisson, M/M/1.  \n",
        "- **Gaussian processes**: kernel \\(k(x,x')\\) encodes smoothness.  \n",
        "- **HMMs / CRFs**: sequence labeling; forward–backward, Viterbi.  \n",
        "- **Stochastic calculus**:  \n",
        "  $$\n",
        "  dX_t = \\mu(X_t,t)dt + \\sigma(X_t,t)dW_t\n",
        "  $$  \n",
        "  (Itô’s lemma, Fokker–Planck).  \n",
        "- **MDPs (Bellman equations)**:  \n",
        "  $$\n",
        "  V_\\pi(s) = E_\\pi\\!\\left[\\sum_t \\gamma^t r_t \\mid s_0=s\\right],\n",
        "  $$  \n",
        "  $$\n",
        "  V^*(s) = \\max_a \\{ r(s,a) + \\gamma \\sum_{s'} P(s' \\mid s,a)V^*(s') \\}.\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Inference paradigms & algorithms\n",
        "\n",
        "- **Frequentist**: MLE, likelihood ratio tests, Neyman–Pearson.  \n",
        "- **Bayesian**: Priors/posteriors, conjugacy, MCMC (MH, Gibbs, HMC/NUTS), Variational Inference.  \n",
        "- **Decision-theoretic**: minimize expected loss; Bayes risk.  \n",
        "- **Empirical Bayes** (Robbins/Efron), **Bootstrap** (Efron).  \n",
        "- **EM algorithm** (Dempster–Laird–Rubin).  \n",
        "- **Sequential methods**: Kalman/particle filters.\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Probability inside modern AI\n",
        "\n",
        "- **Graphical models**: Bayesian nets, factor graphs.  \n",
        "- **Normalizing flows**:  \n",
        "  $$\n",
        "  \\log p(x) = \\log p(z) + \\log \\Big|\\det \\frac{\\partial f}{\\partial x}\\Big|.\n",
        "  $$\n",
        "- **VAEs**:  \n",
        "  $$\n",
        "  \\mathbb{E}_{q_\\phi(z\\mid x)}[\\log p_\\theta(x\\mid z)] - KL(q_\\phi(z\\mid x)\\parallel p(z)).\n",
        "  $$\n",
        "- **GANs**: adversarial min-max learning; connects to f-divergences.  \n",
        "- **Diffusion models**: learn reverse SDEs, score \\(\\nabla_x \\log p_t(x)\\).  \n",
        "- **Reinforcement learning**: policy gradients, bandits, regret bounds.  \n",
        "- **Causality**: Pearl’s do-calculus, Rubin’s potential outcomes.  \n",
        "\n",
        "---\n",
        "\n",
        "## 8) Applications\n",
        "\n",
        "- **A/B testing**: power, sequential tests.  \n",
        "- **Signal processing & control**: Kalman filters, particle filters.  \n",
        "- **NLP/CV/Audio**: language modeling \\(p(x_t \\mid x_{<t})\\), diffusion.  \n",
        "- **Finance**: stochastic volatility, copulas.  \n",
        "- **Biostatistics**: survival analysis, causal inference.  \n",
        "- **Physics**: Ising models, Monte Carlo.  \n",
        "- **Security**: anomaly detection, differential privacy \\((\\epsilon,\\delta)\\).\n",
        "\n",
        "---\n",
        "\n",
        "## 9) Scientists & contributions (select highlights)\n",
        "\n",
        "- **Foundations**: Pascal, Fermat, Bernoulli, Bayes, Laplace, Gauss, Poisson, Kolmogorov, Lévy, Doob, Itô.  \n",
        "- **Statistics**: Fisher, Neyman–Pearson, Wald, Cramér–Rao, Tukey, Efron.  \n",
        "- **Information & learning**: Shannon, Jaynes, Vapnik–Chervonenkis, Valiant.  \n",
        "- **AI algorithms**: Metropolis–Hastings, Pearl, Ghahramani, Hinton, Kingma–Welling, Goodfellow, Sohl-Dickstein, Ho.\n",
        "\n",
        "---\n",
        "\n",
        "## 10) Minimal mental models\n",
        "- **Uncertainty as geometry**: distributions are shapes.  \n",
        "- **Learning = tradeoff**: fit vs uncertainty.  \n",
        "- **Sequences & control**: Markov structure = linear algebra.  \n",
        "- **Optimization is stochastic**: SGD ≈ noisy sampling.\n",
        "\n",
        "---\n",
        "\n",
        "## 11) Quick-reference formulas\n",
        "- **Law of total expectation**:  \n",
        "  $$\n",
        "  E[X] = E_Y[E[X\\mid Y]].\n",
        "  $$\n",
        "\n",
        "- **Law of total variance**:  \n",
        "  $$\n",
        "  Var(X) = E[Var(X\\mid Y)] + Var(E[X\\mid Y]).\n",
        "  $$\n",
        "\n",
        "- **CRLB**:  \n",
        "  $$\n",
        "  Var(\\hat{\\theta}) \\geq \\frac{1}{I(\\theta)}, \\quad I(\\theta) = E\\!\\left[-\\frac{\\partial^2}{\\partial\\theta^2}\\log p_\\theta(X)\\right].\n",
        "  $$\n",
        "\n",
        "- **Change of variables**:  \n",
        "  $$\n",
        "  p_X(x) = p_Z(f(x))\\left|\\det J_f(x)\\right|.\n",
        "  $$\n",
        "\n",
        "- **Hoeffding inequality**:  \n",
        "  $$\n",
        "  P(\\bar{X}-E\\bar{X}\\geq \\epsilon) \\leq \\exp\\!\\Big(-\\frac{2n\\epsilon^2}{(b-a)^2}\\Big).\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## 12) Workflows you can copy into practice\n",
        "- **Bayesian loop**: prior → likelihood → posterior (MCMC/VI) → posterior predictive check.  \n",
        "- **A/B testing**: power analysis → sequential monitoring → CUPED adjustment.  \n",
        "- **Uncertainty in deep nets**: ensembles, MC-dropout, conformal prediction.  \n",
        "- **Sequential state estimation**: Kalman/particle filters, EM tuning.\n",
        "\n",
        "---\n",
        "\n",
        "## 13) From math to code\n",
        "- **MAP estimation**:  \n",
        "  $$\n",
        "  \\max_\\theta \\log p(x\\mid \\theta) + \\log p(\\theta).\n",
        "  $$  \n",
        "  With Gaussian prior \\(\\sim N(0, \\lambda^{-1}I)\\), MAP = L2-regularized MLE.\n",
        "\n",
        "- **Variational Inference in one line**:  \n",
        "  $$\n",
        "  z = g_\\phi(\\epsilon, x), \\quad \\max_\\phi \\text{ELBO via SGD}.\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## 14) A lightning reading map\n",
        "- **Foundations**: Billingsley, Kallenberg.  \n",
        "- **Statistics**: Lehmann–Romano, Casella–Berger, Wasserman.  \n",
        "- **Information & learning**: Cover–Thomas, Vapnik, Shalev-Shwartz–Ben-David.  \n",
        "- **Probabilistic ML**: Bishop, Murphy, MacKay, Barber.  \n",
        "- **Bayesian computation**: Robert & Casella, Betancourt (HMC).  \n",
        "- **GPs**: Rasmussen–Williams.  \n",
        "- **Causality**: Pearl, Imbens–Rubin.\n",
        "\n",
        "---\n",
        "\n",
        "## 15) One-paragraph takeaway\n",
        "Probability is the calculus of uncertainty. Measure theory gives it bones, limit theorems give it stability, inequalities give it control, information theory gives it meaning, and algorithms (MCMC/VI/filters/SGD) make it computable. Modern AI is fundamentally probabilistic—whether you state a likelihood (VAEs), learn an implicit generator (GANs), evolve densities (diffusion), or reason about interventions (causality). **Master the axioms, the bounds, and the inference recipes—and you can reason rigorously from noisy data to reliable decisions.**\n"
      ],
      "metadata": {
        "id": "XXQyzv_4Vtw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probabilistic Distributions — A Creative, Complete Field Guide\n",
        "\n",
        "Below is a practical atlas you can use: a clean taxonomy, quick-reference tables (support, params, mean/var), core identities you’ll actually need (conjugacy, limits, transforms), and bite-size AI use-cases. **Bookmark-material.**\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Big picture: how to choose a distribution\n",
        "\n",
        "**Three questions unlock 80% of choices**\n",
        "\n",
        "- **What’s the support?**  \n",
        "  - Counts {0,1,2,…} → Poisson/NegBin  \n",
        "  - Proportions in [0,1] → Beta  \n",
        "  - Reals → Normal/Student-t/Laplace  \n",
        "  - Positives → Lognormal/Gamma/Weibull  \n",
        "  - Angles → von Mises  \n",
        "\n",
        "- **What shape/tails?**  \n",
        "  - Light tails (Normal) vs heavy tails (t, Cauchy)  \n",
        "  - Skewed (Lognormal, Gamma)  \n",
        "  - Bounded (Beta)  \n",
        "\n",
        "- **What data-gen mechanism?**  \n",
        "  - “success/failure per trial” → Bernoulli/Binomial  \n",
        "  - “arrivals over time” → Poisson/Exponential  \n",
        "  - “min of many risks” → Weibull  \n",
        "  - “sum/average” → Normal (CLT)  \n",
        "\n",
        "**Mnemonic**: CABS = Counts, Angles, Bounded, Signed reals.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) The exponential family (unifies the “greatest hits”)\n",
        "\n",
        "Many classics share one form:\n",
        "\n",
        "$$\n",
        "p(x\\mid \\eta) = h(x) \\exp\\{\\eta^\\top T(x) - A(\\eta)\\}\n",
        "$$\n",
        "\n",
        "Includes: Bernoulli, Binomial, Poisson, Exponential, Gamma, Beta, Dirichlet, Normal (known variance), Multinomial, Wishart…\n",
        "\n",
        "**Consequences**: sufficient statistics \\(T(x)\\), conjugacy, moment formulas via \\(A(\\eta)\\).\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Quick reference — Discrete distributions\n",
        "\n",
        "| Name        | Support      | Params   | Mean   | Var       | When to use (AI) |\n",
        "|-------------|--------------|----------|--------|-----------|------------------|\n",
        "| Bernoulli   | {0,1}        | \\(p\\)    | \\(p\\)  | \\(p(1-p)\\)| Binary labels/logits; BCE loss |\n",
        "| Binomial    | {0..n}       | \\(n,p\\)  | \\(np\\) | \\(np(1-p)\\)| #successes in fixed n; click-throughs |\n",
        "| Geometric   | {1,2,…}      | \\(p\\)    | 1/p    | (1−p)/p²  | Trials until 1st success |\n",
        "| Neg. Binomial | {0,1,…}    | \\(r,p\\)  | r(1−p)/p | r(1−p)/p² | Overdispersed counts; Poisson–Gamma mix |\n",
        "| Poisson     | {0,1,…}      | \\(\\lambda\\) | \\(\\lambda\\) | \\(\\lambda\\) | Event counts per interval |\n",
        "| Categorical | {1..K}       | \\(\\pi\\)  | —      | —         | Class labels; softmax output |\n",
        "| Multinomial | vectors sum n| \\(n,\\pi\\)| nπₖ    | nπₖ(1−πₖ) | Bag-of-words; token counts |\n",
        "\n",
        "**Workhorse conjugacies**:  \n",
        "- Beta–Binomial  \n",
        "- Gamma–Poisson  \n",
        "- Dirichlet–Multinomial (Polya urn)  \n",
        "\n",
        "Zero-inflation: use Zero-Inflated Poisson/NegBin when many zeros.\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Quick reference — Continuous on \\([0,\\infty)\\)\n",
        "\n",
        "| Name       | Support | Params     | Mean          | Var                      | Notes |\n",
        "|------------|---------|------------|---------------|--------------------------|-------|\n",
        "| Exponential| x>0     | λ          | 1/λ           | 1/λ²                     | Memoryless; inter-arrival |\n",
        "| Gamma      | x>0     | k,θ        | kθ            | kθ²                      | Sum of exponentials; priors |\n",
        "| Weibull    | x>0     | k,λ        | λΓ(1+1/k)     | —                        | Lifetimes; hazard shape |\n",
        "| Lognormal  | x>0     | μ,σ        | exp(μ+σ²/2)   | (e^{σ²}−1) e^{2μ+σ²}     | Multiplicative growth |\n",
        "| Chi-square | x>0     | ν          | ν             | 2ν                       | Sum of squares |\n",
        "| Inverse-Gamma | x>0 | α,β        | β/(α−1)       | β²/[(α−1)²(α−2)]         | Conjugate for Normal var |\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Quick reference — Continuous on ℝ\n",
        "\n",
        "| Name      | Support | Params      | Mean | Var                 | Why/When |\n",
        "|-----------|---------|-------------|------|---------------------|----------|\n",
        "| Normal    | ℝ       | μ,σ²        | μ    | σ²                  | CLT, linear models |\n",
        "| Student-t | ℝ       | ν,μ,σ       | μ    | (νσ²)/(ν−2)         | Heavy-tail robust |\n",
        "| Laplace   | ℝ       | b,μ         | μ    | 2b²                 | L1 errors; sparse priors |\n",
        "| Cauchy    | ℝ       | x₀,γ        | —    | —                   | Extreme outliers |\n",
        "| Logistic  | ℝ       | μ,s         | μ    | π²s²/3              | Sigmoid noise; choice models |\n",
        "| MoG       | ℝᵈ      | {w,μ,Σ}     | —    | —                   | Clustering; density modeling |\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Bounded and directional\n",
        "\n",
        "| Name        | Support    | Params | Mean            | Notes |\n",
        "|-------------|------------|--------|-----------------|-------|\n",
        "| Uniform     | [a,b]      | a,b    | (a+b)/2         | Baseline ignorance |\n",
        "| Beta        | [0,1]      | α,β    | α/(α+β)         | Conjugate to Bernoulli/Binomial |\n",
        "| Dirichlet   | simplex    | α      | αₖ/α₀           | Conjugate to Multinomial |\n",
        "| Kumaraswamy | [0,1]      | a,b    | —               | Beta-like, easy inverse-CDF |\n",
        "| von Mises   | circle     | μ,κ    | —               | “Circular Normal”; angles |\n",
        "| LKJ         | corr. mats | η      | —               | Prior over correlations |\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Matrix-variate & multivariate Gaussians\n",
        "\n",
        "- **Multivariate Normal** \\(N(\\mu,Σ)\\):  \n",
        "  $$\n",
        "  p(x) \\propto \\exp\\Big(-\\tfrac{1}{2}(x-\\mu)^\\top Σ^{-1}(x-\\mu)\\Big).\n",
        "  $$  \n",
        "  Marginals & conditionals stay Gaussian (→ GPs, Kalman filters).\n",
        "\n",
        "- **Wishart / Inv-Wishart**: priors for covariance/precision matrices.  \n",
        "- **Matrix-normal**: for \\(X \\in \\mathbb{R}^{m\\times n}\\) with row/col covariances.\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Essential identities & relationships\n",
        "\n",
        "- **Limits**:  \n",
        "  - Binomial(n,p), with np→λ ⇒ Poisson(λ).  \n",
        "  - Gamma(k=1) ⇒ Exponential.  \n",
        "  - Student-t(ν) → Normal as ν→∞.  \n",
        "\n",
        "- **Mixtures**:  \n",
        "  - Poisson with λ∼Gamma ⇒ NegBin.  \n",
        "  - Bernoulli with p∼Beta ⇒ Beta-Binomial.  \n",
        "\n",
        "- **Conjugacy crib sheet**:  \n",
        "  Bernoulli/Binomial ↔ Beta; Multinomial ↔ Dirichlet; Poisson ↔ Gamma;  \n",
        "  Normal (σ² known) ↔ Normal; Normal (σ² unknown) ↔ Normal–Inv-Gamma;  \n",
        "  Precision ↔ Wishart; Correlation ↔ LKJ.  \n",
        "\n",
        "- **Transforms**:  \n",
        "  If Y=logX, X∼Lognormal ⇒ Y∼N(μ,σ²).  \n",
        "  If X∼Beta(α,β) ⇒ X/(1−X) ∼ BetaPrime(α,β).  \n",
        "  Change of variables:  \n",
        "  $$\n",
        "  p_Y(y) = p_X(f^{-1}(y)) \\big|\\det J_{f^{-1}}(y)\\big|.\n",
        "  $$\n",
        "\n",
        "- **Order statistics**:  \n",
        "  For Uniform(0,1), the k-th order statistic ∼ Beta(k,n−k+1).  \n",
        "\n",
        "- **Copulas**: Gaussian copula, t-copula capture dependence.\n",
        "\n",
        "---\n",
        "\n",
        "## 9) Estimation notes (robust, stable)\n",
        "\n",
        "- Log-transform for positivity (fit logλ not λ).  \n",
        "- **Reparameterization trick**:  \n",
        "  - Normal: z=μ+σϵ  \n",
        "  - Gamma/Weibull: implicit approximations  \n",
        "- **Stable numerics**: log-sum-exp for mixtures, Cholesky for Σ≻0.  \n",
        "- **Tail-robust**: Student-t/Laplace for outliers.  \n",
        "- **Censoring/truncation**: use truncated versions with normalization.\n",
        "\n",
        "---\n",
        "\n",
        "## 10) Which one for which AI task?\n",
        "\n",
        "- **Classification logits** → Categorical; priors → Dirichlet.  \n",
        "- **Token counts** → Multinomial; topics → Dirichlet (LDA).  \n",
        "- **Event streams** → Poisson/NegBin.  \n",
        "- **Regression residuals** → Normal / Student-t / Laplace.  \n",
        "- **Time-to-event** → Exponential, Weibull, Lognormal.  \n",
        "- **Embeddings uncertainty** → Multivariate Normal (low-rank Σ).  \n",
        "- **Correlation matrices** → LKJ prior.  \n",
        "- **State-space/Kalman** → Gaussian; HMMs → categorical transitions.\n",
        "\n",
        "---\n",
        "\n",
        "## 11) Entropy & KL (closed forms)\n",
        "\n",
        "- **Normal**:  \n",
        "  $$\n",
        "  H = \\tfrac{1}{2}\\log\\big((2\\pi e)^d |Σ|\\big)\n",
        "  $$\n",
        "\n",
        "- **KL between Gaussians**:  \n",
        "  $$\n",
        "  KL(N_0\\parallel N_1) = \\tfrac{1}{2}\\Big(tr(Σ_1^{-1}Σ_0) + (μ_1-μ_0)^\\top Σ_1^{-1}(μ_1-μ_0) - d + \\log \\tfrac{|Σ_1|}{|Σ_0|}\\Big)\n",
        "  $$\n",
        "\n",
        "- **Bernoulli**:  \n",
        "  $$\n",
        "  H = -p\\log p - (1-p)\\log(1-p)\n",
        "  $$\n",
        "\n",
        "- **Categorical**:  \n",
        "  $$\n",
        "  H = -\\sum_k \\pi_k \\log \\pi_k\n",
        "  $$\n",
        "\n",
        "- **Dirichlet**: closed form via digamma functions (use library).\n",
        "\n",
        "---\n",
        "\n",
        "## 12) Tiny recipes (copy-paste logic)\n",
        "\n",
        "- Overdispersed counts? NegBin. If many zeros, Zero-Inflated NegBin.  \n",
        "- Probabilities on simplex? Dirichlet; with correlations → Logistic-Normal.  \n",
        "- Proportions near 0/1? Beta with α,β<1. Add inflation if exact 0/1 appear.  \n",
        "- Fat-tail regression residuals? Student-t with learned dof.  \n",
        "- Unknown variance in Normal? Normal–Inv-Gamma prior → Student-t predictive.  \n",
        "- Random effects? Hierarchical: Normal with hyper-priors; Poisson-Gamma for counts.\n",
        "\n",
        "---\n",
        "\n",
        "## 13) Visualization heuristics\n",
        "\n",
        "- **Skewness test**: right-skew → Lognormal vs Gamma.  \n",
        "- **Hazard shape**: Weibull k>1 = increasing; k<1 = decreasing hazard.  \n",
        "- **QQ-plots**: Normal QQ shows tail issues; log-QQ for Lognormal.\n",
        "\n",
        "---\n",
        "\n",
        "## 14) Advanced corners\n",
        "\n",
        "- **Generalized Pareto, GEV**: extremes.  \n",
        "- **Stable laws (Lévy α-stable)**: heavy tails, infinite variance.  \n",
        "- **Compound distributions**: Poisson-Lognormal for counts.  \n",
        "- **Hurdle models**: two-part (zero vs positive).  \n",
        "- **Dirichlet processes**: infinite mixtures.  \n",
        "- **Normalizing flows/diffusion**: flexible invertible transforms.\n",
        "\n",
        "---\n",
        "\n",
        "## 15) One-page “starter kit”\n",
        "\n",
        "- **Binary** → Bernoulli (Beta prior).  \n",
        "- **Counts** → Poisson (Gamma prior). Overdispersion? NegBin.  \n",
        "- **Proportion [0,1]** → Beta (Dirichlet for multi-class).  \n",
        "- **Real-valued** → Normal; outliers? Student-t; sparse? Laplace.  \n",
        "- **Positive** → Lognormal (multiplicative); Gamma/Weibull (rates).  \n",
        "- **Angles** → von Mises.  \n",
        "- **Covariances** → Wishart/Inv-Wishart; correlations → LKJ.  \n",
        "- **Mixtures** → multi-modal shapes.  \n"
      ],
      "metadata": {
        "id": "-4ig8UP6W1qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chronological Timeline of Probability  \n",
        "*(from games of chance → statistics → AI)*\n",
        "\n",
        "This is a story of centuries, showing how scattered ideas about dice and gambling evolved into the mathematical backbone of statistics and modern AI.\n",
        "\n",
        "---\n",
        "\n",
        "## 1600s: Birth of Probability\n",
        "\n",
        "- **Gerolamo Cardano (1501–1576)** – First to analyze gambling odds in *Liber de Ludo Aleae* (Book on Games of Chance).  \n",
        "- **Pierre de Fermat & Blaise Pascal (1654)** – Correspondence about a gambler’s problem → foundation of probability theory.  \n",
        "- **Christiaan Huygens (1657)** – First textbook: *De Ratiociniis in Ludo Aleae* (On Reasoning in Games of Chance). Introduced expectation.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1700s: Classical Foundations\n",
        "\n",
        "- **Jakob Bernoulli (1713)** – *Ars Conjectandi*. Introduced Law of Large Numbers (LLN).  \n",
        "- **Abraham de Moivre (1718, 1733)** – *Doctrine of Chances*. Derived normal approximation to binomial (early Central Limit Theorem).  \n",
        "- **Thomas Bayes (1763)** – *Essay on the Doctrine of Chances*. Introduced Bayes’ theorem.  \n",
        "- **Pierre-Simon Laplace (1774–1812)** – Generalized Bayes’ ideas; developed Bayesian probability, generating functions, and Laplace’s CLT.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1800s: Probability meets Statistics\n",
        "\n",
        "- **Carl Friedrich Gauss (1809)** – Normal distribution as error law; least squares for regression.  \n",
        "- **Siméon Poisson (1837)** – Poisson distribution for rare events.  \n",
        "- **Adolphe Quetelet (1835)** – Applied probability to social statistics (“average man”).  \n",
        "- **Pafnuty Chebyshev (1867)** – Inequalities, general Law of Large Numbers.  \n",
        "- **Francis Galton (1880s)** – Regression to the mean, correlation.  \n",
        "- **Karl Pearson (1890s)** – χ² tests, method of moments; founded biometrics and modern statistics.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1900–1930: Modern Probability Axioms\n",
        "\n",
        "- **Student (William Gosset, 1908)** – Student’s t-distribution for small-sample inference.  \n",
        "- **Andrey Markov (1906)** – Markov chains; stochastic processes.  \n",
        "- **Émile Borel (1909)** – Measure theory ideas applied to probability.  \n",
        "- **Norbert Wiener (1923)** – Wiener process (Brownian motion).  \n",
        "- **Andrey Kolmogorov (1933)** – *Foundations of the Theory of Probability*. Axiomatic probability using measure theory.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1930–1960: Statistics and Information\n",
        "\n",
        "- **Ronald Fisher (1920s–30s)** – Maximum likelihood estimation, ANOVA, Fisher Information.  \n",
        "- **Jerzy Neyman & Egon Pearson (1930s)** – Hypothesis testing framework (Neyman–Pearson lemma).  \n",
        "- **Abraham Wald (1940s)** – Decision theory, sequential analysis.  \n",
        "- **Claude Shannon (1948)** – *A Mathematical Theory of Communication*. Entropy, information as probability’s twin.  \n",
        "- **Harold Cramér & C.R. Rao (1940s)** – Cramér–Rao bound; efficiency of estimators.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1960–1990: Probability in AI & ML\n",
        "\n",
        "- **Norbert Wiener (1948)** – Cybernetics: feedback, control, stochastic processes in automation.  \n",
        "- **Solomonoff, Kolmogorov, Chaitin (1960s)** – Algorithmic probability, complexity.  \n",
        "- **Judea Pearl (1980s)** – Bayesian networks, causal reasoning.  \n",
        "- **Leonard Jimmie Savage** – Bayesian decision theory.  \n",
        "- **Hastings & Metropolis (1953), Geman & Geman (1984)** – MCMC methods for inference.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1990–2010: Probabilistic Machine Learning\n",
        "\n",
        "- **Michael Jordan, Zoubin Ghahramani, David MacKay, Chris Bishop** – Unified probabilistic graphical models.  \n",
        "- **Variational Inference (1990s–2000s)** – Approximate Bayesian inference (Jordan, Wainwright).  \n",
        "- **Ensemble methods (1990s)** – Bagging, boosting → probability of error.  \n",
        "- **Kernel methods (SVM, Gaussian Processes)** – Probabilistic view of functions (Rasmussen & Williams, 2006).  \n",
        "\n",
        "---\n",
        "\n",
        "## 2010–Present: Probability in Deep Learning & AI\n",
        "\n",
        "- **Geoffrey Hinton (2006–2012)** – Probabilistic generative models (Boltzmann Machines, DBNs).  \n",
        "- **Kingma & Welling (2013)** – Variational Autoencoders (VAEs): Bayesian inference in deep nets.  \n",
        "- **Ian Goodfellow (2014)** – GANs: adversarial probability game.  \n",
        "- **Sohl-Dickstein, Ho et al. (2015–2020)** – Diffusion models: probabilistic forward–reverse processes.  \n",
        "- **Yarin Gal & Zoubin Ghahramani (2016)** – Bayesian deep learning via dropout.  \n",
        "- **Judea Pearl (2009–2020)** – Causal inference bridges probability to reasoning in AI.  \n",
        "\n",
        "---\n",
        "\n",
        "## Creative View: Probability’s Journey\n",
        "\n",
        "**Dice → Distributions → Data → Decisions → Deep Nets**  \n",
        "\n",
        "Probability began as a gambling trick, became a mathematical science, turned into statistics for society, then powered information theory, and today fuels AI’s uncertainty-aware engines.  \n",
        "\n",
        "Every leap—from Pascal’s dice to VAEs—represents probability reimagined for a new age.  \n"
      ],
      "metadata": {
        "id": "DdKtB9SJXT5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probability as the Backbone of AI  \n",
        "*(Fields, concepts, and equations where probability rules the game)*\n",
        "\n",
        "Probability is not just a side tool in AI—it’s the invisible skeleton that holds together every learning algorithm. Let’s go field by field.\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Machine Learning Foundations\n",
        "\n",
        "**Statistical Learning Theory**  \n",
        "- PAC learning (Valiant) → probability of error.  \n",
        "- VC dimension, Rademacher complexity → generalization bounds.  \n",
        "\n",
        "**Key Equations**:\n",
        "\n",
        "$$\n",
        "R(f) = \\mathbb{E}_{(x,y)\\sim P}[L(f(x),y)], \\quad\n",
        "\\hat{R}(f) = \\frac{1}{n}\\sum_{i=1}^n L(f(x_i),y_i).\n",
        "$$  \n",
        "\n",
        "Concentration inequalities ensure:\n",
        "\n",
        "$$\n",
        "R(f) \\approx \\hat{R}(f).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Bayesian Learning\n",
        "\n",
        "**Concepts**: Prior, likelihood, posterior, predictive distribution.  \n",
        "\n",
        "**Equations**:\n",
        "\n",
        "$$\n",
        "p(\\theta\\mid D) = \\frac{p(D\\mid\\theta)p(\\theta)}{p(D)}, \\quad\n",
        "p(x\\mid D) = \\int p(x\\mid\\theta)p(\\theta\\mid D)\\,d\\theta.\n",
        "$$  \n",
        "\n",
        "**Applications**:  \n",
        "- Bayesian neural nets  \n",
        "- Thompson sampling in bandits  \n",
        "- Bayesian optimization (GPs)  \n",
        "\n",
        "---\n",
        "\n",
        "## 3) Supervised Learning\n",
        "\n",
        "- **Classification**:  \n",
        "  Logistic regression:  \n",
        "  $$\n",
        "  p(y=1\\mid x) = \\sigma(w^\\top x).\n",
        "  $$  \n",
        "\n",
        "  Softmax for multi-class probabilities.  \n",
        "\n",
        "- **Regression**:  \n",
        "  Gaussian likelihood:  \n",
        "  $$\n",
        "  y \\sim \\mathcal{N}(f(x), \\sigma^2).\n",
        "  $$  \n",
        "\n",
        "**Key Role**: Probabilistic assumptions define loss functions (e.g., cross-entropy = log-likelihood).\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Unsupervised Learning\n",
        "\n",
        "- **Clustering**:  \n",
        "  Gaussian Mixture Models (GMMs):  \n",
        "  $$\n",
        "  p(x) = \\sum_k \\pi_k \\,\\mathcal{N}(x\\mid \\mu_k, \\Sigma_k).\n",
        "  $$  \n",
        "\n",
        "- **Dimensionality Reduction**: Probabilistic PCA.  \n",
        "- **Density Estimation**: KDE, normalizing flows, diffusion.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5) Generative AI\n",
        "\n",
        "- **Variational Autoencoders (VAE)**:  \n",
        "  $$\n",
        "  \\log p(x) \\geq \\mathbb{E}_{q(z\\mid x)}[\\log p(x\\mid z)] - KL(q(z\\mid x)\\parallel p(z)).\n",
        "  $$  \n",
        "\n",
        "- **GANs**: Implicit probability matching.  \n",
        "- **Diffusion Models**: Reverse stochastic processes.  \n",
        "- **Energy-based Models**:  \n",
        "  $$\n",
        "  p(x) \\propto e^{-E(x)}.\n",
        "  $$  \n",
        "\n",
        "---\n",
        "\n",
        "## 6) Reinforcement Learning\n",
        "\n",
        "- **Markov Decision Processes (MDPs)**:  \n",
        "  Transition:  \n",
        "  $$\n",
        "  P(s' \\mid s,a).\n",
        "  $$  \n",
        "\n",
        "- **Bellman Equations**:  \n",
        "  $$\n",
        "  V^\\pi(s) = \\mathbb{E}[r + \\gamma V^\\pi(s')].\n",
        "  $$  \n",
        "\n",
        "- **Policy Gradients**:  \n",
        "  $$\n",
        "  \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s) Q^\\pi(s,a)].\n",
        "  $$  \n",
        "\n",
        "- Exploration vs exploitation: bandits, posterior sampling.\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Probabilistic Graphical Models\n",
        "\n",
        "- **Bayesian Networks**: factorization via conditional independence.  \n",
        "- **Markov Random Fields**: undirected probability structures.  \n",
        "\n",
        "**Equation**:\n",
        "\n",
        "$$\n",
        "p(x_1,\\ldots,x_n) = \\prod_i p(x_i \\mid Pa(x_i)).\n",
        "$$  \n",
        "\n",
        "**Applications**: NLP, speech, vision, bioinformatics.\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Natural Language Processing\n",
        "\n",
        "- **Language Models** (chain rule):  \n",
        "  $$\n",
        "  p(x_1,\\ldots,x_T) = \\prod_{t=1}^T p(x_t \\mid x_{<t}).\n",
        "  $$  \n",
        "\n",
        "- **Word Embeddings**: co-occurrence → probability ratios (PMI, GloVe).  \n",
        "- **Seq2Seq**: conditional distributions.  \n",
        "- **Transformers**: attention = probabilistic weighting.  \n",
        "\n",
        "---\n",
        "\n",
        "## 9) Computer Vision\n",
        "\n",
        "- **Bayesian filtering**: Kalman filter, particle filter.  \n",
        "- **Generative models**: VAEs, GANs, diffusion for images.  \n",
        "- **Uncertainty estimation**: dropout-as-Bayes, ensembles.  \n",
        "\n",
        "**Applications**: medical risk, autonomous driving.  \n",
        "\n",
        "---\n",
        "\n",
        "## 10) Speech & Signal Processing\n",
        "\n",
        "- **Hidden Markov Models (HMMs)**: probabilistic sequences.  \n",
        "- **Kalman Filters**: Gaussian hidden states.  \n",
        "\n",
        "**Applications**: speech recognition, radar, robotics.  \n",
        "\n",
        "---\n",
        "\n",
        "## 11) Causality\n",
        "\n",
        "- **Judea Pearl’s SCMs**: structural causal models.  \n",
        "- **Do-calculus**:  \n",
        "  $$\n",
        "  P(Y \\mid do(X=x)) \\neq P(Y \\mid X=x).\n",
        "  $$  \n",
        "\n",
        "**Counterfactual inference**: reasoning about “what if.”\n",
        "\n",
        "---\n",
        "\n",
        "## 12) Information Theory in AI\n",
        "\n",
        "- **Entropy**:  \n",
        "  $$\n",
        "  H(X) = -\\sum_x p(x)\\log p(x).\n",
        "  $$  \n",
        "\n",
        "- **Mutual Information**:  \n",
        "  $$\n",
        "  I(X;Y) = H(X) - H(X\\mid Y).\n",
        "  $$  \n",
        "\n",
        "**Applications**: InfoGAN, InfoMax, self-supervised learning.\n",
        "\n",
        "---\n",
        "\n",
        "## 13) Uncertainty & Robustness\n",
        "\n",
        "- Calibration: reliability diagrams, temperature scaling.  \n",
        "- Confidence/credible intervals.  \n",
        "- Adversarial robustness: probabilistic certification bounds.  \n",
        "\n",
        "---\n",
        "\n",
        "## 14) Optimization as Probability\n",
        "\n",
        "- SGD: noisy update ≈ stochastic sampling.  \n",
        "- Simulated annealing: Boltzmann distribution.  \n",
        "- Variational Inference: optimization of probability divergences.  \n",
        "\n",
        "---\n",
        "\n",
        "## 15) Privacy & Security\n",
        "\n",
        "- **Differential Privacy**: randomized mechanisms.  \n",
        "  $$\n",
        "  \\Pr[M(D)=o] \\leq e^\\epsilon \\Pr[M(D')=o] + \\delta.\n",
        "  $$  \n",
        "\n",
        "- Probabilistic anomaly detection: fraud, cybersecurity.\n",
        "\n",
        "---\n",
        "\n",
        "## 16) Applied Domains\n",
        "\n",
        "- **Healthcare AI**: survival analysis (Weibull, Cox models).  \n",
        "- **Finance AI**: stochastic processes, risk modeling.  \n",
        "- **Robotics**: SLAM via Bayesian filtering.  \n",
        "- **Recommender Systems**: probabilistic matrix factorization.  \n",
        "\n",
        "---\n",
        "\n",
        "## Creative Summary\n",
        "\n",
        "Probability is the **bloodstream of AI**:\n",
        "\n",
        "- In **learning theory**, it measures error & generalization.  \n",
        "- In **Bayesian inference**, it updates belief under uncertainty.  \n",
        "- In **generative AI**, it creates worlds via stochastic processes.  \n",
        "- In **reinforcement learning**, it balances chance & choice.  \n",
        "- In **causal AI**, it disentangles what is from what if.  \n",
        "\n",
        "From dice (Pascal) to diffusion models (Ho et al.), probability is the mathematical glue that makes intelligence under uncertainty possible.\n"
      ],
      "metadata": {
        "id": "VjGAk6Y9Y4Rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Hall of Fame: Probability Distributions & Their Scientists\n",
        "\n",
        "Probability distributions are not just mathematical objects—they are legacies. Each one carries the name of a scientist who wrestled with uncertainty and turned randomness into rigorous form.\n",
        "\n",
        "---\n",
        "\n",
        "##  Discrete Distributions\n",
        "\n",
        "- **Bernoulli Distribution – Jacob Bernoulli (1654–1705)**  \n",
        "  Father of the Law of Large Numbers; studied coin-flip style experiments.  \n",
        "\n",
        "- **Binomial Distribution – Jacob Bernoulli (1713, Ars Conjectandi)**  \n",
        "  Extension of Bernoulli trials to multiple successes.  \n",
        "\n",
        "- **Poisson Distribution – Siméon Denis Poisson (1781–1840)**  \n",
        "  Rare events, arrivals, accidents.  \n",
        "\n",
        "- **Markov Chains – Andrey Markov (1856–1922)**  \n",
        "  Memoryless stochastic processes.  \n",
        "\n",
        "- **Negative Binomial (Pascal Distribution) – Blaise Pascal (1623–1662)**  \n",
        "  Counts failures before success.  \n",
        "\n",
        "- **Geometric Distribution – Bernoulli trial process**  \n",
        "  Not tied to a single scientist; trials until first success.  \n",
        "\n",
        "- **Erlang Distribution (discrete-time queues) – Agner Krarup Erlang (1878–1929)**  \n",
        "  Father of queueing theory, telecom pioneer.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Continuous Distributions\n",
        "\n",
        "- **Normal (Gaussian) – Carl Friedrich Gauss (1777–1855)**  \n",
        "  Law of errors, least squares.  \n",
        "\n",
        "- **Cauchy – Augustin-Louis Cauchy (1789–1857)**  \n",
        "  Heavy-tailed; mean/variance undefined.  \n",
        "\n",
        "- **Laplace – Pierre-Simon Laplace (1749–1827)**  \n",
        "  Double exponential, least absolute deviations.  \n",
        "\n",
        "- **Student’s t – William Sealy Gosset (1876–1937)**  \n",
        "  Published under pseudonym “Student.”  \n",
        "\n",
        "- **F Distribution – Ronald A. Fisher (1890–1962) & George Snedecor (1881–1974)**  \n",
        "  Ratio of variances, ANOVA foundation.  \n",
        "\n",
        "- **Chi-Square – Karl Pearson (1857–1936)**  \n",
        "  Goodness-of-fit, hypothesis testing.  \n",
        "\n",
        "- **Gumbel – Emil Julius Gumbel (1891–1966)**  \n",
        "  Extreme value theory, risks, climate extremes.  \n",
        "\n",
        "- **Erlang (continuous) – A.K. Erlang**  \n",
        "  Lifetimes, reliability.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Families & Advanced\n",
        "\n",
        "- **Dirichlet – Johann Peter Gustav Lejeune Dirichlet (1805–1859)**  \n",
        "  Bayesian priors on probabilities.  \n",
        "\n",
        "- **Wishart – John Wishart (1898–1956)**  \n",
        "  Distribution of covariance matrices.  \n",
        "\n",
        "- **Fisher’s Z – Ronald A. Fisher (1890–1962)**  \n",
        "  Correlation inference.  \n",
        "\n",
        "- **Kolmogorov Distribution – Andrey Kolmogorov (1903–1987)**  \n",
        "  Basis of Kolmogorov–Smirnov test.  \n",
        "\n",
        "- **Lévy – Paul Lévy (1886–1971)**  \n",
        "  Stable distributions, infinite variance models.  \n",
        "\n",
        "- **Rényi Entropy/Distribution – Alfréd Rényi (1921–1970)**  \n",
        "  Hungarian pioneer in information theory.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Process Distributions\n",
        "\n",
        "- **Wiener Process – Norbert Wiener (1894–1964)**  \n",
        "  Mathematical Brownian motion.  \n",
        "\n",
        "- **Itô Processes – Kiyoshi Itô (1915–2008)**  \n",
        "  Stochastic calculus, SDEs.  \n",
        "\n",
        "- **Bessel Distribution – Friedrich Bessel (1784–1846)**  \n",
        "  Astronomical/statistical roots.  \n",
        "\n",
        "- **Pearson Distribution Family – Karl Pearson**  \n",
        "  Generalized family covering skewness/kurtosis.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Special Mentions\n",
        "\n",
        "- **Kolmogorov–Smirnov Distribution – Kolmogorov & Nikolai Smirnov**  \n",
        "  Probability of maximum deviation.  \n",
        "\n",
        "- **Rayleigh – Lord Rayleigh (John William Strutt, 1842–1919)**  \n",
        "  Wave physics, signal theory.  \n",
        "\n",
        "- **Maxwell–Boltzmann – James Clerk Maxwell (1831–1879), Ludwig Boltzmann (1844–1906)**  \n",
        "  Gas particles, kinetic theory.  \n",
        "\n",
        "- **Gibbs Distribution – Josiah Willard Gibbs (1839–1903)**  \n",
        "  Statistical mechanics; foundation of Boltzmann machines.  \n",
        "\n",
        "- **Boltzmann Distribution – Ludwig Boltzmann**  \n",
        "  Thermodynamic probability; entropy.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Distribution “Family Tree”\n",
        "\n",
        "- **Bernoulli → Binomial → Poisson → Normal (via CLT).**  \n",
        "- **Poisson + Gamma → Negative Binomial.**  \n",
        "- **Normal + ratios → t (Student), F (Fisher), χ² (Pearson).**  \n",
        "- **Dirichlet/Multinomial → foundation of Bayesian ML.**  \n",
        "- **Wishart → covariance priors for multivariate Gaussian models.**  \n",
        "- **Gumbel/Extreme Value → risk, climate, AI robustness.**  \n",
        "\n",
        "---\n",
        "\n",
        "##  Creative Closing\n",
        "\n",
        "Probability distributions are a **living hall of fame**.  \n",
        "Every time you call `scipy.stats.poisson` or `torch.distributions.Normal`, you’re invoking centuries of human genius.  \n",
        "\n",
        "They are not just functions but **historical fingerprints**—each named after mathematicians who faced uncertainty and gave it shape.  \n",
        "\n",
        "From Bernoulli’s coins to Gibbs’ ensembles to Boltzmann’s entropy, these names are the silent companions of every AI model we train today.\n"
      ],
      "metadata": {
        "id": "5W3WRCcbZpJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Timeline of Probability Distributions & Scientists\n",
        "\n",
        "A historical map of how probability evolved from games of chance into the backbone of statistics and AI.\n",
        "\n",
        "---\n",
        "\n",
        "## 1600–1700s: Foundations\n",
        "- Pascal (1623–1662) ────────────────┐  \n",
        "- Bernoulli (1654–1705) ────────────> Bernoulli, Binomial  \n",
        "- de Moivre (1667–1754) ────────────> Normal Approx (CLT beginnings)  \n",
        "- Bayes (1702–1761) ────────────────> Bayes' Theorem  \n",
        "- Laplace (1749–1827) ──────────────> Laplace Distribution, Bayesian Methods  \n",
        "\n",
        "---\n",
        "\n",
        "## 1800s: Classical Age\n",
        "- Gauss (1777–1855) ────────────────> Gaussian (Normal) Distribution  \n",
        "- Poisson (1781–1840) ─────────────> Poisson Distribution  \n",
        "- Bessel (1784–1846) ──────────────> Bessel Functions/Distribution  \n",
        "- Cauchy (1789–1857) ──────────────> Cauchy Distribution  \n",
        "- Dirichlet (1805–1859) ───────────> Dirichlet Distribution  \n",
        "- Quetelet (1796–1874) ────────────> \"Average Man\", Social Stats  \n",
        "- Chebyshev (1821–1894) ───────────> Inequalities, Foundations  \n",
        "- Pearson (1857–1936) ─────────────> χ², Pearson Distribution Family  \n",
        "- Galton (1822–1911) ──────────────> Correlation, Regression  \n",
        "\n",
        "---\n",
        "\n",
        "## 1900–1930s: Modern Stats & Axioms\n",
        "- Student / Gosset (1876–1937) ─────> Student’s t Distribution  \n",
        "- Fisher (1890–1962) ──────────────> F Distribution, Fisher Information  \n",
        "- Snedecor (1881–1974) ────────────> Fisher–Snedecor F Distribution  \n",
        "- Kolmogorov (1903–1987) ──────────> Probability Axioms, KS Test  \n",
        "- Neyman & Pearson (1930s) ─────────> Hypothesis Testing  \n",
        "\n",
        "---\n",
        "\n",
        "## 1940–1970: Processes & Information\n",
        "- Wiener (1894–1964) ──────────────> Wiener Process (Brownian motion)  \n",
        "- Itô (1915–2008) ─────────────────> Itô Processes (Stochastic calculus)  \n",
        "- Gibbs (1839–1903) ──────────────> Gibbs Distribution (Stat Mech → ML)  \n",
        "- Boltzmann (1844–1906) ──────────> Boltzmann Distribution  \n",
        "- Rayleigh (1842–1919) ───────────> Rayleigh Distribution  \n",
        "- Lévy (1886–1971) ───────────────> Lévy Stable Distributions  \n",
        "- Gumbel (1891–1966) ─────────────> Gumbel Extreme Value Distribution  \n",
        "- Wishart (1898–1956) ────────────> Wishart Distribution  \n",
        "- Rényi (1921–1970) ──────────────> Rényi entropy/distributions  \n",
        "\n",
        "---\n",
        "\n",
        "## 1980s–2000s: Probability in AI\n",
        "- Pearl (1936– ) ──────────────────> Bayesian Networks, Causal Models  \n",
        "- Jordan (1956– ) ─────────────────> Probabilistic Graphical Models  \n",
        "- MacKay (1962–2003) ─────────────> Bayesian ML, Information Theory  \n",
        "- Ghahramani (1970– ) ────────────> Probabilistic ML, Bayesian Deep Learning  \n",
        "- Wainwright (1975– ) ────────────> Variational Inference  \n",
        "\n",
        "---\n",
        "\n",
        "## 2010–Now: Generative AI & Beyond\n",
        "- Hinton (1947– ) ────────────────> Boltzmann Machines, DBNs  \n",
        "- Kingma & Welling (2013) ─────────> Variational Autoencoders (VAE)  \n",
        "- Goodfellow (2014) ──────────────> Generative Adversarial Networks (GANs)  \n",
        "- Sohl-Dickstein & Ho (2015–2020) ─> Diffusion Models  \n",
        "- Gal & Ghahramani (2016) ─────────> Bayesian Deep Learning via Dropout  \n",
        "- Pearl (continued) ──────────────> Causal AI, Counterfactuals  \n",
        "\n",
        "---\n",
        "\n",
        "## How it looks visually\n",
        "- Imagine a **horizontal scroll timeline** with eras as color-coded bands.  \n",
        "- Each scientist’s portrait appears with lifespan underneath.  \n",
        "- Below: their distribution, theorem, or method + a key formula.  \n",
        "- Arrows show the distribution family tree:  \n",
        "  - Bernoulli → Binomial → Poisson → Gaussian → t/F/Chi².  \n",
        "  - Dirichlet/Multinomial → Bayesian ML.  \n",
        "  - Gibbs/Boltzmann → Energy-based AI.  \n",
        "  - Extreme Value (Gumbel) → Robustness & risk models.  \n",
        "\n",
        "---\n",
        "\n",
        "## Creative Closing\n",
        "This timeline is more than mathematics—it is a **gallery of human curiosity**.  \n",
        "From Pascal’s dice to Ho’s diffusion models, probability has been the common language of uncertainty across centuries.  \n",
        "\n",
        "Every AI library today (`torch.distributions`, `scipy.stats`) is essentially a **digital museum** carrying forward these scientists’ legacies.  \n"
      ],
      "metadata": {
        "id": "s_VJMw1iaP2y"
      }
    }
  ]
}