{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM4h2/O4R3hsA+4+FCvx7jC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ“œ Semi-Supervised Learning (Semi-SL) in Deep Learning\n","\n","---\n","\n","## ðŸ“š Key Milestones\n","\n","| **Era** | **Model / Concept** | **Year** | **Authors / Org** | **Key Contributions** |\n","|---------|----------------------|----------|-------------------|-----------------------|\n","| **Early Foundations** | **Co-Training** | 1998 | Blum & Mitchell (Carnegie Mellon) | First influential algorithm combining small labeled + large unlabeled datasets. |\n","| | **Manifold Regularization** | 2006 | Belkin, Niyogi & Sindhwani | Laplacian regularization leveraging data geometry for Semi-SL. |\n","| **Classical Semi-SL with Deep Nets** | **Semi-Supervised Autoencoders** | 2008 | Ranzato & Szummer | Early autoencoder-based semi-supervised text classification. |\n","| | **Ladder Networks** | 2015 | Rasmus et al. (NeurIPS) | Combined supervised loss with unsupervised denoising autoencoder objectives. |\n","| | **Semi-Supervised GANs (SGAN)** | 2016 | Salimans et al., OpenAI | Used GAN discriminator as classifier with unlabeled data. |\n","| **Modern Semi-SL (Consistency & Pseudo-Labeling)** | **Pseudo-Labeling** | 2013 | Lee | High-confidence predictions treated as pseudo-labels for unlabeled data. |\n","| | **Temporal Ensembling & Mean Teacher** | 2017 | Laine & Aila (ICLR); Tarvainen & Valpola (NeurIPS) | Enforced consistency across time / teacherâ€“student setups. |\n","| | **MixMatch** | 2019 | Berthelot et al., Google Brain | Unified pseudo-labeling, consistency regularization, and augmentation. |\n","| | **FixMatch** | 2020 | Sohn et al., Google Brain | Simplified approach â†’ pseudo-labeling + strong augmentation; achieved SOTA with few labels. |\n","| **Applications** | **Computer Vision** | 2010sâ€“2020s | Medical imaging | Semi-SL critical where labeled data is scarce. |\n","| | **NLP** | 2010sâ€“2020s | Low-resource text tasks | Boosted classification & sentiment analysis. |\n","| | **Speech** | 2010sâ€“2020s | ASR models | Enabled scaling with small labeled + large unlabeled corpora. |\n","\n","---\n","\n","## âœ… Summary Families\n","- **Classical:** Co-Training (1998), Manifold Regularization (2006).  \n","- **Deep Semi-SL:** Semi-Supervised Autoencoders (2008), Ladder Networks (2015), SGAN (2016).  \n","- **Consistency & Augmentation Era:** Pseudo-Labeling (2013), Mean Teacher (2017), MixMatch (2019), FixMatch (2020).  \n","\n","---\n","\n","ðŸ‘‰ **Insight:**  \n","Semi-supervised learning acts as a **bridge** â€” linking supervised learning (with labels) and self-supervised learning (without labels). It paved the way for todayâ€™s **foundation models**, which scale training on massive unlabeled datasets.  \n"],"metadata":{"id":"pTvc2kRxBVHe"}}]}