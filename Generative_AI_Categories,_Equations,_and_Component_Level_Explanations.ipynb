{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI Models with Equations & Detailed Explanations\n",
        "\n",
        "Generative AI aims to model the data distribution \\(p(x)\\) and generate new samples that resemble the true data.  \n",
        "Below are the major categories with their mathematical foundations and detailed component breakdowns.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Generative Adversarial Networks (GANs)\n",
        "\n",
        "**Equation (Minimax Game):**\n",
        "\n",
        "$$\n",
        "\\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_\\text{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log (1 - D(G(z)))]\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- - Generator (G):  \n",
        "$$\n",
        "G(z; \\theta_g)\n",
        "$$  \n",
        "maps random noise  \n",
        "$$\n",
        "z \\sim p_z(z)\n",
        "$$  \n",
        "into data space.\n",
        "\n",
        "- Discriminator (D):  \n",
        "$$\n",
        "D(x; \\theta_d)\n",
        "$$  \n",
        "outputs probability that input is real.\n",
        "\n",
        "- Objective:  \n",
        "  -  \n",
        "  $$\n",
        "  D \\ \\text{maximizes correct classification (real vs fake)}\n",
        "  $$  \n",
        "  -  \n",
        "  $$\n",
        "  G \\ \\text{minimizes log-probability of being detected as fake}\n",
        "  $$\n",
        "\n",
        "Interpretation: A two-player game where \\(G\\) tries to fool \\(D\\), and \\(D\\) tries to detect fakes.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Variational Autoencoders (VAEs)\n",
        "\n",
        "**Evidence Lower Bound (ELBO):**\n",
        "\n",
        "$$\n",
        "\\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\text{KL}(q_\\phi(z|x) \\| p(z))\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Encoder: \\( q_\\phi(z|x) \\), approximates posterior distribution of latent \\(z\\).\n",
        "- Decoder: \\( p_\\theta(x|z) \\), reconstructs input data from latent samples.\n",
        "- Prior: \\( p(z) \\), often standard Gaussian \\(\\mathcal{N}(0,I)\\).\n",
        "- KL Divergence Term: Regularizes latent distribution to be close to prior.\n",
        "- Reconstruction Term: Ensures decoded samples resemble the input.\n",
        "\n",
        "Interpretation: VAEs balance reconstruction accuracy and latent regularization.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Autoregressive Models\n",
        "\n",
        "**Factorization of Probability:**\n",
        "\n",
        "$$\n",
        "p(x) = \\prod_{t=1}^T p(x_t \\mid x_{<t})\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Conditioning: Each token depends only on previous tokens \\(x_{<t}\\).\n",
        "- Neural Architectures: RNNs, LSTMs, Transformers.\n",
        "- Training: Teacher-forcing with maximum likelihood.\n",
        "\n",
        "Examples: GPT, PixelRNN, WaveNet.  \n",
        "Interpretation: Generate one element at a time in a sequence.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Normalizing Flows\n",
        "\n",
        "**Change of Variables Formula:**\n",
        "\n",
        "$$\n",
        "p_X(x) = p_Z(f(x)) \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right|\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Invertible Mapping: \\( f: X \\to Z \\) ensures reversibility.\n",
        "- Prior: Simple distribution (Gaussian).\n",
        "- Jacobian Determinant: Adjusts probability density when mapping between spaces.\n",
        "\n",
        "Interpretation: Learn exact likelihoods with flexible, invertible functions.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Diffusion Models\n",
        "\n",
        "**Forward (noising) process:**\n",
        "\n",
        "$$\n",
        "q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)\n",
        "$$\n",
        "\n",
        "**Reverse (denoising) process:**\n",
        "\n",
        "$$\n",
        "p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t,t), \\Sigma_\\theta(x_t,t))\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Forward Process: Gradually corrupt data with Gaussian noise.\n",
        "- Reverse Process: Learn denoising distribution parameterized by neural net (e.g., U-Net).\n",
        "- Scheduler: Controls step sizes \\(\\beta_t\\).\n",
        "- Sampling: Start from noise, iteratively denoise.\n",
        "\n",
        "Interpretation: Model learns to reverse noise injection and generate realistic samples.  \n",
        "Examples: DDPM, Stable Diffusion, Imagen.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Energy-Based Models (EBMs)\n",
        "\n",
        "**Boltzmann Distribution:**\n",
        "\n",
        "$$\n",
        "p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z_\\theta}, \\quad Z_\\theta = \\int e^{-E_\\theta(x)} dx\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Energy Function: \\(E_\\theta(x)\\) assigns low energy to real samples.\n",
        "- Partition Function: \\(Z_\\theta\\) ensures normalization (often intractable).\n",
        "- Learning: Contrastive divergence, sampling via MCMC.\n",
        "\n",
        "Interpretation: Learn an energy landscape where real samples lie in valleys.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Score-Based / Flow-Matching Models\n",
        "\n",
        "**Stochastic Differential Equation (SDE):**\n",
        "\n",
        "$$\n",
        "\\frac{dx}{dt} = \\nabla_x \\log p_\\theta(x) + \\sqrt{2} \\, dW_t\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Score Function: \\(\\nabla_x \\log p_\\theta(x)\\), gradient of log-density.\n",
        "- Sampling Dynamics: Langevin dynamics simulate motion in probability landscape.\n",
        "- Flow Matching: Aligning forward/backward ODE flows.\n",
        "\n",
        "Interpretation: Model generates data by following gradient fields of probability density.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Neural Autoregressive Density Estimators (NADE)\n",
        "\n",
        "**Density Factorization:**\n",
        "\n",
        "$$\n",
        "p(x) = \\prod_{i=1}^D p(x_i \\mid x_{<i}; \\theta)\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Neural Parametrization: Conditionals parameterized by NN.\n",
        "- Ordering: Sequence of features matters (fixed or random).\n",
        "- Training: Exact likelihood maximization.\n",
        "\n",
        "Interpretation: A fully tractable probabilistic model for high-dimensional data.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Transformer-based Generative Models\n",
        "\n",
        "**Self-Attention Mechanism:**\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Query (Q): Representation of the current token.\n",
        "- Key (K): Encodes context of past tokens.\n",
        "- Value (V): Carries the actual information.\n",
        "- Softmax Scaling: Normalizes attention scores.\n",
        "\n",
        "Interpretation: Each token attends to others for context, enabling long-range dependencies.  \n",
        "Examples: GPT-3/4/5, BERT (masked variant).\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Hybrid and Specialized Models\n",
        "\n",
        "- VAE-GAN: Combines reconstruction likelihood with adversarial loss.  \n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\mathcal{L}_\\text{VAE} + \\lambda \\mathcal{L}_\\text{GAN}\n",
        "$$\n",
        "\n",
        "- Diffusion + Transformers: Stable Diffusion uses U-Net with cross-attention.\n",
        "- Energy-Guided Diffusion: Injects energy-based priors into diffusion sampling.\n",
        "\n",
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "- GANs: Adversarial game → fake vs real.\n",
        "- VAEs: Latent variable model → ELBO maximization.\n",
        "- Autoregressive: Sequential factorization.\n",
        "- Normalizing Flows: Exact likelihood via invertible transforms.\n",
        "- Diffusion: Reverse noise → denoise to generate.\n",
        "- EBMs: Energy landscape.\n",
        "- Score Models: Gradient-based sampling.\n",
        "- NADE: Exact conditional factorization.\n",
        "- Transformers: Attention-powered autoregression.\n",
        "- Hybrids: Combine strengths of multiple models.\n"
      ],
      "metadata": {
        "id": "N_v9VrkS5vOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI Models: Probabilistic Essence\n",
        "\n",
        "---\n",
        "\n",
        "## 1. GANs\n",
        "\n",
        "**Essence**: The discriminator outputs a probability between 0 and 1 (real vs fake).\n",
        "\n",
        "**Why the logarithm?**  \n",
        "Logarithms turn multiplicative probabilities into additive terms, making optimization stable.  \n",
        "\n",
        "- For real data: maximize  \n",
        "$$\n",
        "\\log D(x)\n",
        "$$  \n",
        "which forces \\(D(x)\\) close to 1.  \n",
        "\n",
        "- For fake data: maximize  \n",
        "$$\n",
        "\\log (1 - D(G(z)))\n",
        "$$  \n",
        "which forces \\(D(G(z))\\) close to 0.  \n",
        "\n",
        "**Heart of idea**: The generator improves by making the discriminator’s prediction uncertain (pushing its probability close to 0.5). This back-and-forth ensures generated samples approximate the true distribution.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. VAEs\n",
        "\n",
        "**Essence**: Probability of data is often intractable, so we lower-bound it with ELBO.\n",
        "\n",
        "**Why the KL divergence?**  \n",
        "It measures how different the encoder’s distribution \\(q_\\phi(z|x)\\) is from a prior (usually Gaussian). By constraining it, the latent space becomes smooth and generalizable.\n",
        "\n",
        "**Why the log-likelihood term?**  \n",
        "$$\n",
        "\\log p_\\theta(x|z)\n",
        "$$  \n",
        "is like a “chance” of reconstructing the input from latent codes. Higher likelihood → better reconstructions.\n",
        "\n",
        "**Heart of idea**: Balance between two pressures:  \n",
        "1. Data reconstruction accuracy.  \n",
        "2. Keeping latent variables in a structured, normalized space.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Autoregressive Models\n",
        "\n",
        "**Essence**: Probability of a sequence is factorized step by step.\n",
        "\n",
        "Each token must lie in [0,1] probability space, and the full sentence probability is a product of these terms.\n",
        "\n",
        "**Factorization**:  \n",
        "$$\n",
        "p(x) = \\prod_{t=1}^T p(x_t \\mid x_{<t})\n",
        "$$\n",
        "\n",
        "**Why logs matter?**  \n",
        "Maximizing  \n",
        "$$\n",
        "\\log p(x)\n",
        "$$  \n",
        "avoids multiplying many small numbers (underflow) and instead adds them.\n",
        "\n",
        "**Heart of idea**: Predict the next token given history → chance of the sequence is just the product of these conditional chances.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Normalizing Flows\n",
        "\n",
        "**Essence**: Exact probability modeling using invertible transformations.\n",
        "\n",
        "**Change of Variables**:  \n",
        "$$\n",
        "p_X(x) = p_Z(f(x)) \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right|\n",
        "$$\n",
        "\n",
        "**Why determinant of Jacobian?**  \n",
        "When you stretch or compress space, the density changes accordingly. The determinant tells how much “volume” is scaled.\n",
        "\n",
        "**Heart of idea**: Start with a simple distribution (like Gaussian), apply reversible transformations, and compute the exact likelihood of data.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Diffusion Models\n",
        "\n",
        "**Essence**: Probability is modeled as a noisy process.\n",
        "\n",
        "- Forward process gradually destroys structure (adding Gaussian noise).  \n",
        "- Reverse process learns how likely it is to denoise step by step.  \n",
        "\n",
        "**Forward process**:  \n",
        "$$\n",
        "q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)\n",
        "$$\n",
        "\n",
        "**Reverse process**:  \n",
        "$$\n",
        "p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t,t), \\Sigma_\\theta(x_t,t))\n",
        "$$\n",
        "\n",
        "**Why Gaussian assumption?**  \n",
        "Normal distributions are mathematically convenient and stable under addition.\n",
        "\n",
        "**Heart of idea**: The chance of clean data is recovered by chaining probabilities of many denoising steps backward in time.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. EBMs\n",
        "\n",
        "**Essence**: Assigns an “energy” score to every configuration. Low energy = high probability.\n",
        "\n",
        "**Boltzmann Distribution**:  \n",
        "$$\n",
        "p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z_\\theta}, \\quad\n",
        "Z_\\theta = \\int e^{-E_\\theta(x)} dx\n",
        "$$\n",
        "\n",
        "**Why exponentials and partition function?**  \n",
        "The exponential ensures probabilities are always positive and between 0–1 once normalized by \\(Z\\).\n",
        "\n",
        "**Heart of idea**: The world is modeled as an energy surface; real data sits in valleys, and fake data in high-energy peaks.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Score-Based / Flow-Matching Models\n",
        "\n",
        "**Essence**: Instead of modeling probability directly, model its gradient (the score function).\n",
        "\n",
        "**Stochastic Differential Equation**:  \n",
        "$$\n",
        "\\frac{dx}{dt} = \\nabla_x \\log p_\\theta(x) + \\sqrt{2}\\, dW_t\n",
        "$$\n",
        "\n",
        "**Why gradient of log probability?**  \n",
        "It points in the direction of higher probability density — essentially, “where data is more likely.”\n",
        "\n",
        "**Heart of idea**: Generate by simulating dynamics that climb toward regions of higher data likelihood.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. NADE\n",
        "\n",
        "**Essence**: Like autoregressive models but for high-dimensional data (e.g., vectors).\n",
        "\n",
        "**Density Factorization**:  \n",
        "$$\n",
        "p(x) = \\prod_{i=1}^D p(x_i \\mid x_{<i}; \\theta)\n",
        "$$\n",
        "\n",
        "Every conditional probability is modeled explicitly so that the product remains a valid probability in [0,1].\n",
        "\n",
        "**Heart of idea**: Fully tractable density estimation with neural nets.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Transformers\n",
        "\n",
        "**Essence**: Attention mechanism computes a probability distribution over keys given a query.\n",
        "\n",
        "**Self-Attention**:  \n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "**Why softmax?**  \n",
        "Converts raw similarity scores into probabilities (0–1, summing to 1).\n",
        "\n",
        "**Heart of idea**: Each token “chooses” how much chance it gives to others when forming its new representation.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Hybrids\n",
        "\n",
        "**Essence**: Mix and match the probability tricks above.\n",
        "\n",
        "**Example (VAE-GAN)**:  \n",
        "$$\n",
        "\\mathcal{L} = \\mathcal{L}_\\text{VAE} + \\lambda \\mathcal{L}_\\text{GAN}\n",
        "$$\n",
        "\n",
        "**Heart of idea**: Multiple probability constraints together yield sharper, more realistic models.\n",
        "\n",
        "---\n",
        "\n",
        "# Core Intuition Across All Models\n",
        "\n",
        "- Probabilities must lie in [0,1] — hence the use of sigmoid, softmax, and normalization.  \n",
        "- Logarithms stabilize products of probabilities (turning them into sums).  \n",
        "- KL divergence / energy functions constrain distributions so they don’t collapse or spread arbitrarily.  \n",
        "- Noise vs. structure — most models balance the randomness of generation with the order of real data.  \n",
        "- Gradients guide learning — whether via adversarial games, variational bounds, or score matching, the key is optimizing probability surfaces.\n"
      ],
      "metadata": {
        "id": "MwfImXdgIyOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding GAN Training as a Game\n",
        "\n",
        "The GAN objective is formulated as:\n",
        "\n",
        "\\[\n",
        "\\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_{data}} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log (1 - D(G(z)))]\n",
        "\\]\n",
        "\n",
        "- **Discriminator (D):** judges whether an input is real (from data) or fake (from generator).  \n",
        "- **Generator (G):** takes random noise \\(z\\) and tries to produce samples \\(G(z)\\) that fool the discriminator.\n",
        "\n",
        "---\n",
        "\n",
        "## Step-by-Step Flow\n",
        "\n",
        "1. Draw real samples \\(x \\sim p_{data}(x)\\).  \n",
        "   - Feed into \\(D\\).  \n",
        "   - D’s goal: output probability close to 1 (real).  \n",
        "\n",
        "2. Draw random noise \\(z \\sim p_z(z)\\).  \n",
        "   - Feed into \\(G\\).  \n",
        "   - Generator outputs a fake sample \\(G(z)\\).  \n",
        "\n",
        "3. Feed \\(G(z)\\) into \\(D\\).  \n",
        "   - D’s goal: output probability close to 0 (fake).  \n",
        "   - G’s goal: push D’s output closer to 1 (make fake look real).  \n",
        "\n",
        "**Training alternates:**  \n",
        "- Fix \\(G\\), update \\(D\\) to better separate real/fake.  \n",
        "- Fix \\(D\\), update \\(G\\) to fool \\(D\\).  \n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "\n",
        "- If **D is perfect**: it outputs 1 for real, 0 for fake. Generator gets crushed.  \n",
        "- If **G is perfect**: its fakes are indistinguishable from real, so D outputs ~0.5 for everything. D is confused.  \n",
        "\n",
        "The training aims for a **balance point**: the generator learns the true data distribution.\n",
        "\n",
        "---\n",
        "\n",
        "## Truth Table of Cases\n",
        "\n",
        "| Input type   | D’s output = 1 (calls it Real) | D’s output = 0 (calls it Fake) | Who wins? |\n",
        "|--------------|--------------------------------|--------------------------------|-----------|\n",
        "| Real data \\(x\\) | ✅ Correct (good for D, \\(\\log D(x)\\) high) | ❌ Mistake (bad for D, \\(\\log D(x) \\to -\\infty\\)) | D wins if 1, loses if 0 |\n",
        "| Fake data \\(G(z)\\) | ❌ Mistake (bad for D, good for G, \\(\\log(1-D(G(z))) \\to -\\infty\\)) | ✅ Correct (good for D, bad for G, \\(\\log(1-D(G(z)))\\) high) | D wins if 0, G wins if 1 |\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "- **Discriminator’s job:** maximize correctness — say “1” for real, “0” for fake.  \n",
        "- **Generator’s job:** flip the second row of the table — try to force D to say “1” for its fakes.  \n",
        "\n",
        "**Training is a push–pull game:** each improvement by D pushes G to improve, and vice versa.\n"
      ],
      "metadata": {
        "id": "GYFRZP1686en"
      }
    }
  ]
}