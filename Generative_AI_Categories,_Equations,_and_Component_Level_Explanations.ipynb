{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI Models with Equations & Detailed Explanations\n",
        "\n",
        "Generative AI aims to model the data distribution \\(p(x)\\) and generate new samples that resemble the true data.  \n",
        "Below are the major categories with their mathematical foundations and detailed component breakdowns.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Generative Adversarial Networks (GANs)\n",
        "\n",
        "**Equation (Minimax Game):**\n",
        "\n",
        "$$\n",
        "\\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_\\text{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log (1 - D(G(z)))]\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- - Generator (G):  \n",
        "$$\n",
        "G(z; \\theta_g)\n",
        "$$  \n",
        "maps random noise  \n",
        "$$\n",
        "z \\sim p_z(z)\n",
        "$$  \n",
        "into data space.\n",
        "\n",
        "- Discriminator (D):  \n",
        "$$\n",
        "D(x; \\theta_d)\n",
        "$$  \n",
        "outputs probability that input is real.\n",
        "\n",
        "- Objective:  \n",
        "  -  \n",
        "  $$\n",
        "  D \\ \\text{maximizes correct classification (real vs fake)}\n",
        "  $$  \n",
        "  -  \n",
        "  $$\n",
        "  G \\ \\text{minimizes log-probability of being detected as fake}\n",
        "  $$\n",
        "\n",
        "Interpretation: A two-player game where \\(G\\) tries to fool \\(D\\), and \\(D\\) tries to detect fakes.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Variational Autoencoders (VAEs)\n",
        "\n",
        "**Evidence Lower Bound (ELBO):**\n",
        "\n",
        "$$\n",
        "\\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\text{KL}(q_\\phi(z|x) \\| p(z))\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Encoder: \\( q_\\phi(z|x) \\), approximates posterior distribution of latent \\(z\\).\n",
        "- Decoder: \\( p_\\theta(x|z) \\), reconstructs input data from latent samples.\n",
        "- Prior: \\( p(z) \\), often standard Gaussian \\(\\mathcal{N}(0,I)\\).\n",
        "- KL Divergence Term: Regularizes latent distribution to be close to prior.\n",
        "- Reconstruction Term: Ensures decoded samples resemble the input.\n",
        "\n",
        "Interpretation: VAEs balance reconstruction accuracy and latent regularization.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Autoregressive Models\n",
        "\n",
        "**Factorization of Probability:**\n",
        "\n",
        "$$\n",
        "p(x) = \\prod_{t=1}^T p(x_t \\mid x_{<t})\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Conditioning: Each token depends only on previous tokens \\(x_{<t}\\).\n",
        "- Neural Architectures: RNNs, LSTMs, Transformers.\n",
        "- Training: Teacher-forcing with maximum likelihood.\n",
        "\n",
        "Examples: GPT, PixelRNN, WaveNet.  \n",
        "Interpretation: Generate one element at a time in a sequence.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Normalizing Flows\n",
        "\n",
        "**Change of Variables Formula:**\n",
        "\n",
        "$$\n",
        "p_X(x) = p_Z(f(x)) \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right|\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Invertible Mapping: \\( f: X \\to Z \\) ensures reversibility.\n",
        "- Prior: Simple distribution (Gaussian).\n",
        "- Jacobian Determinant: Adjusts probability density when mapping between spaces.\n",
        "\n",
        "Interpretation: Learn exact likelihoods with flexible, invertible functions.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Diffusion Models\n",
        "\n",
        "**Forward (noising) process:**\n",
        "\n",
        "$$\n",
        "q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)\n",
        "$$\n",
        "\n",
        "**Reverse (denoising) process:**\n",
        "\n",
        "$$\n",
        "p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t,t), \\Sigma_\\theta(x_t,t))\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Forward Process: Gradually corrupt data with Gaussian noise.\n",
        "- Reverse Process: Learn denoising distribution parameterized by neural net (e.g., U-Net).\n",
        "- Scheduler: Controls step sizes \\(\\beta_t\\).\n",
        "- Sampling: Start from noise, iteratively denoise.\n",
        "\n",
        "Interpretation: Model learns to reverse noise injection and generate realistic samples.  \n",
        "Examples: DDPM, Stable Diffusion, Imagen.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Energy-Based Models (EBMs)\n",
        "\n",
        "**Boltzmann Distribution:**\n",
        "\n",
        "$$\n",
        "p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z_\\theta}, \\quad Z_\\theta = \\int e^{-E_\\theta(x)} dx\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Energy Function: \\(E_\\theta(x)\\) assigns low energy to real samples.\n",
        "- Partition Function: \\(Z_\\theta\\) ensures normalization (often intractable).\n",
        "- Learning: Contrastive divergence, sampling via MCMC.\n",
        "\n",
        "Interpretation: Learn an energy landscape where real samples lie in valleys.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Score-Based / Flow-Matching Models\n",
        "\n",
        "**Stochastic Differential Equation (SDE):**\n",
        "\n",
        "$$\n",
        "\\frac{dx}{dt} = \\nabla_x \\log p_\\theta(x) + \\sqrt{2} \\, dW_t\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Score Function: \\(\\nabla_x \\log p_\\theta(x)\\), gradient of log-density.\n",
        "- Sampling Dynamics: Langevin dynamics simulate motion in probability landscape.\n",
        "- Flow Matching: Aligning forward/backward ODE flows.\n",
        "\n",
        "Interpretation: Model generates data by following gradient fields of probability density.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Neural Autoregressive Density Estimators (NADE)\n",
        "\n",
        "**Density Factorization:**\n",
        "\n",
        "$$\n",
        "p(x) = \\prod_{i=1}^D p(x_i \\mid x_{<i}; \\theta)\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Neural Parametrization: Conditionals parameterized by NN.\n",
        "- Ordering: Sequence of features matters (fixed or random).\n",
        "- Training: Exact likelihood maximization.\n",
        "\n",
        "Interpretation: A fully tractable probabilistic model for high-dimensional data.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Transformer-based Generative Models\n",
        "\n",
        "**Self-Attention Mechanism:**\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n",
        "$$\n",
        "\n",
        "### Components:\n",
        "- Query (Q): Representation of the current token.\n",
        "- Key (K): Encodes context of past tokens.\n",
        "- Value (V): Carries the actual information.\n",
        "- Softmax Scaling: Normalizes attention scores.\n",
        "\n",
        "Interpretation: Each token attends to others for context, enabling long-range dependencies.  \n",
        "Examples: GPT-3/4/5, BERT (masked variant).\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Hybrid and Specialized Models\n",
        "\n",
        "- VAE-GAN: Combines reconstruction likelihood with adversarial loss.  \n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\mathcal{L}_\\text{VAE} + \\lambda \\mathcal{L}_\\text{GAN}\n",
        "$$\n",
        "\n",
        "- Diffusion + Transformers: Stable Diffusion uses U-Net with cross-attention.\n",
        "- Energy-Guided Diffusion: Injects energy-based priors into diffusion sampling.\n",
        "\n",
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "- GANs: Adversarial game → fake vs real.\n",
        "- VAEs: Latent variable model → ELBO maximization.\n",
        "- Autoregressive: Sequential factorization.\n",
        "- Normalizing Flows: Exact likelihood via invertible transforms.\n",
        "- Diffusion: Reverse noise → denoise to generate.\n",
        "- EBMs: Energy landscape.\n",
        "- Score Models: Gradient-based sampling.\n",
        "- NADE: Exact conditional factorization.\n",
        "- Transformers: Attention-powered autoregression.\n",
        "- Hybrids: Combine strengths of multiple models.\n"
      ],
      "metadata": {
        "id": "N_v9VrkS5vOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI Models: Probabilistic Essence\n",
        "\n",
        "---\n",
        "\n",
        "## 1. GANs\n",
        "\n",
        "**Essence**: The discriminator outputs a probability between 0 and 1 (real vs fake).\n",
        "\n",
        "**Why the logarithm?**  \n",
        "Logarithms turn multiplicative probabilities into additive terms, making optimization stable.  \n",
        "\n",
        "- For real data: maximize  \n",
        "$$\n",
        "\\log D(x)\n",
        "$$  \n",
        "which forces \\(D(x)\\) close to 1.  \n",
        "\n",
        "- For fake data: maximize  \n",
        "$$\n",
        "\\log (1 - D(G(z)))\n",
        "$$  \n",
        "which forces \\(D(G(z))\\) close to 0.  \n",
        "\n",
        "**Heart of idea**: The generator improves by making the discriminator’s prediction uncertain (pushing its probability close to 0.5). This back-and-forth ensures generated samples approximate the true distribution.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. VAEs\n",
        "\n",
        "**Essence**: Probability of data is often intractable, so we lower-bound it with ELBO.\n",
        "\n",
        "**Why the KL divergence?**  \n",
        "It measures how different the encoder’s distribution \\(q_\\phi(z|x)\\) is from a prior (usually Gaussian). By constraining it, the latent space becomes smooth and generalizable.\n",
        "\n",
        "**Why the log-likelihood term?**  \n",
        "$$\n",
        "\\log p_\\theta(x|z)\n",
        "$$  \n",
        "is like a “chance” of reconstructing the input from latent codes. Higher likelihood → better reconstructions.\n",
        "\n",
        "**Heart of idea**: Balance between two pressures:  \n",
        "1. Data reconstruction accuracy.  \n",
        "2. Keeping latent variables in a structured, normalized space.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Autoregressive Models\n",
        "\n",
        "**Essence**: Probability of a sequence is factorized step by step.\n",
        "\n",
        "Each token must lie in [0,1] probability space, and the full sentence probability is a product of these terms.\n",
        "\n",
        "**Factorization**:  \n",
        "$$\n",
        "p(x) = \\prod_{t=1}^T p(x_t \\mid x_{<t})\n",
        "$$\n",
        "\n",
        "**Why logs matter?**  \n",
        "Maximizing  \n",
        "$$\n",
        "\\log p(x)\n",
        "$$  \n",
        "avoids multiplying many small numbers (underflow) and instead adds them.\n",
        "\n",
        "**Heart of idea**: Predict the next token given history → chance of the sequence is just the product of these conditional chances.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Normalizing Flows\n",
        "\n",
        "**Essence**: Exact probability modeling using invertible transformations.\n",
        "\n",
        "**Change of Variables**:  \n",
        "$$\n",
        "p_X(x) = p_Z(f(x)) \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right|\n",
        "$$\n",
        "\n",
        "**Why determinant of Jacobian?**  \n",
        "When you stretch or compress space, the density changes accordingly. The determinant tells how much “volume” is scaled.\n",
        "\n",
        "**Heart of idea**: Start with a simple distribution (like Gaussian), apply reversible transformations, and compute the exact likelihood of data.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Diffusion Models\n",
        "\n",
        "**Essence**: Probability is modeled as a noisy process.\n",
        "\n",
        "- Forward process gradually destroys structure (adding Gaussian noise).  \n",
        "- Reverse process learns how likely it is to denoise step by step.  \n",
        "\n",
        "**Forward process**:  \n",
        "$$\n",
        "q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)\n",
        "$$\n",
        "\n",
        "**Reverse process**:  \n",
        "$$\n",
        "p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t,t), \\Sigma_\\theta(x_t,t))\n",
        "$$\n",
        "\n",
        "**Why Gaussian assumption?**  \n",
        "Normal distributions are mathematically convenient and stable under addition.\n",
        "\n",
        "**Heart of idea**: The chance of clean data is recovered by chaining probabilities of many denoising steps backward in time.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. EBMs\n",
        "\n",
        "**Essence**: Assigns an “energy” score to every configuration. Low energy = high probability.\n",
        "\n",
        "**Boltzmann Distribution**:  \n",
        "$$\n",
        "p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z_\\theta}, \\quad\n",
        "Z_\\theta = \\int e^{-E_\\theta(x)} dx\n",
        "$$\n",
        "\n",
        "**Why exponentials and partition function?**  \n",
        "The exponential ensures probabilities are always positive and between 0–1 once normalized by \\(Z\\).\n",
        "\n",
        "**Heart of idea**: The world is modeled as an energy surface; real data sits in valleys, and fake data in high-energy peaks.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Score-Based / Flow-Matching Models\n",
        "\n",
        "**Essence**: Instead of modeling probability directly, model its gradient (the score function).\n",
        "\n",
        "**Stochastic Differential Equation**:  \n",
        "$$\n",
        "\\frac{dx}{dt} = \\nabla_x \\log p_\\theta(x) + \\sqrt{2}\\, dW_t\n",
        "$$\n",
        "\n",
        "**Why gradient of log probability?**  \n",
        "It points in the direction of higher probability density — essentially, “where data is more likely.”\n",
        "\n",
        "**Heart of idea**: Generate by simulating dynamics that climb toward regions of higher data likelihood.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. NADE\n",
        "\n",
        "**Essence**: Like autoregressive models but for high-dimensional data (e.g., vectors).\n",
        "\n",
        "**Density Factorization**:  \n",
        "$$\n",
        "p(x) = \\prod_{i=1}^D p(x_i \\mid x_{<i}; \\theta)\n",
        "$$\n",
        "\n",
        "Every conditional probability is modeled explicitly so that the product remains a valid probability in [0,1].\n",
        "\n",
        "**Heart of idea**: Fully tractable density estimation with neural nets.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Transformers\n",
        "\n",
        "**Essence**: Attention mechanism computes a probability distribution over keys given a query.\n",
        "\n",
        "**Self-Attention**:  \n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "**Why softmax?**  \n",
        "Converts raw similarity scores into probabilities (0–1, summing to 1).\n",
        "\n",
        "**Heart of idea**: Each token “chooses” how much chance it gives to others when forming its new representation.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Hybrids\n",
        "\n",
        "**Essence**: Mix and match the probability tricks above.\n",
        "\n",
        "**Example (VAE-GAN)**:  \n",
        "$$\n",
        "\\mathcal{L} = \\mathcal{L}_\\text{VAE} + \\lambda \\mathcal{L}_\\text{GAN}\n",
        "$$\n",
        "\n",
        "**Heart of idea**: Multiple probability constraints together yield sharper, more realistic models.\n",
        "\n",
        "---\n",
        "\n",
        "# Core Intuition Across All Models\n",
        "\n",
        "- Probabilities must lie in [0,1] — hence the use of sigmoid, softmax, and normalization.  \n",
        "- Logarithms stabilize products of probabilities (turning them into sums).  \n",
        "- KL divergence / energy functions constrain distributions so they don’t collapse or spread arbitrarily.  \n",
        "- Noise vs. structure — most models balance the randomness of generation with the order of real data.  \n",
        "- Gradients guide learning — whether via adversarial games, variational bounds, or score matching, the key is optimizing probability surfaces.\n"
      ],
      "metadata": {
        "id": "MwfImXdgIyOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding GAN Training as a Game\n",
        "\n",
        "The GAN objective is formulated as:\n",
        "\n",
        "\\[\n",
        "\\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_{data}} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log (1 - D(G(z)))]\n",
        "\\]\n",
        "\n",
        "- **Discriminator (D):** judges whether an input is real (from data) or fake (from generator).  \n",
        "- **Generator (G):** takes random noise \\(z\\) and tries to produce samples \\(G(z)\\) that fool the discriminator.\n",
        "\n",
        "---\n",
        "\n",
        "## Step-by-Step Flow\n",
        "\n",
        "1. Draw real samples \\(x \\sim p_{data}(x)\\).  \n",
        "   - Feed into \\(D\\).  \n",
        "   - D’s goal: output probability close to 1 (real).  \n",
        "\n",
        "2. Draw random noise \\(z \\sim p_z(z)\\).  \n",
        "   - Feed into \\(G\\).  \n",
        "   - Generator outputs a fake sample \\(G(z)\\).  \n",
        "\n",
        "3. Feed \\(G(z)\\) into \\(D\\).  \n",
        "   - D’s goal: output probability close to 0 (fake).  \n",
        "   - G’s goal: push D’s output closer to 1 (make fake look real).  \n",
        "\n",
        "**Training alternates:**  \n",
        "- Fix \\(G\\), update \\(D\\) to better separate real/fake.  \n",
        "- Fix \\(D\\), update \\(G\\) to fool \\(D\\).  \n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "\n",
        "- If **D is perfect**: it outputs 1 for real, 0 for fake. Generator gets crushed.  \n",
        "- If **G is perfect**: its fakes are indistinguishable from real, so D outputs ~0.5 for everything. D is confused.  \n",
        "\n",
        "The training aims for a **balance point**: the generator learns the true data distribution.\n",
        "\n",
        "---\n",
        "\n",
        "## Truth Table of Cases\n",
        "\n",
        "| Input type   | D’s output = 1 (calls it Real) | D’s output = 0 (calls it Fake) | Who wins? |\n",
        "|--------------|--------------------------------|--------------------------------|-----------|\n",
        "| Real data \\(x\\) |  Correct (good for D, \\(\\log D(x)\\) high) |  Mistake (bad for D, \\(\\log D(x) \\to -\\infty\\)) | D wins if 1, loses if 0 |\n",
        "| Fake data \\(G(z)\\) |  Mistake (bad for D, good for G, \\(\\log(1-D(G(z))) \\to -\\infty\\)) |  Correct (good for D, bad for G, \\(\\log(1-D(G(z)))\\) high) | D wins if 0, G wins if 1 |\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "- **Discriminator’s job:** maximize correctness — say “1” for real, “0” for fake.  \n",
        "- **Generator’s job:** flip the second row of the table — try to force D to say “1” for its fakes.  \n",
        "\n",
        "**Training is a push–pull game:** each improvement by D pushes G to improve, and vice versa.\n"
      ],
      "metadata": {
        "id": "GYFRZP1686en"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Generative AI Models & Their Origins\n",
        "\n",
        "| Generative Model | Core Origin / Inspiration | Theoretical Foundation |\n",
        "|------------------|--------------------------|-------------------------|\n",
        "| **GANs (Generative Adversarial Networks)** | Game theory (two-player minimax) + probability | Adversarial training = minimax optimization (Goodfellow et al., 2014) |\n",
        "| **VAEs (Variational Autoencoders)** | Bayesian inference + variational methods | Variational inference, KL divergence, ELBO maximization |\n",
        "| **Autoregressive Models (RNN, PixelRNN, GPT)** | Probability theory + Markov property extensions | Chain rule factorization of joint distributions |\n",
        "| **Normalizing Flows (RealNVP, Glow)** | Probability & calculus | Change-of-variables theorem + Jacobian determinants |\n",
        "| **Diffusion Models (DDPM, Stable Diffusion)** | Markov chains + statistical physics (diffusion process) | Forward noising as Gaussian Markov chain, reverse denoising learned |\n",
        "| **EBMs (Energy-Based Models, Boltzmann Machines)** | Statistical mechanics (Boltzmann distribution) | Probability from energy landscapes (low energy → high probability) |\n",
        "| **Score-Based Models / Flow Matching** | Stochastic calculus + differential equations | Langevin dynamics, SDE/ODE probability flows |\n",
        "| **NADE / MADE (Neural Autoregressive Density Estimators)** | Autoregressive probability modeling | Explicit factorization of densities with neural parametrization |\n",
        "| **Transformers (GPT, BERT, etc.)** | Sequence modeling + attention mechanisms | Probability distributions over tokens via softmax attention |\n",
        "| **Hybrids (VAE-GAN, Energy-Guided Diffusion)** | Combinations of above | Mix variational inference, adversarial games, and energy principles |\n",
        "\n",
        "---\n",
        "\n",
        "##  Key Insights\n",
        "\n",
        "- **Markov roots:** Autoregressive & Diffusion models → rely on step-by-step probabilistic transitions.  \n",
        "- **Physics roots:** EBMs & Diffusion → inspired by thermodynamics (energy, heat, diffusion).  \n",
        "- **Game theory roots:** GANs → adversarial min-max optimization.  \n",
        "- **Bayesian roots:** VAEs → approximate inference with variational bounds.  \n",
        "- **Neural architectures roots:** Transformers, RNNs → sequence probability modeling.  \n",
        "\n",
        "---\n",
        "\n",
        "#  Generative, Unsupervised, Probabilistic?\n",
        "\n",
        "### 1. Are they all **Generative**?\n",
        "Yes — every model in the list (GAN, VAE, Diffusion, etc.) is a generative model, because its purpose is to model  \n",
        "\n",
        "$$ p(x) $$  \n",
        "\n",
        "(the data distribution) and produce new samples that look like the real data.  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. Are they all **Unsupervised**?\n",
        "Mostly yes, because they don’t require labeled data — they learn from the raw data distribution itself.  \n",
        "\n",
        " But:  \n",
        "- Some can be used in **semi-supervised** or **conditional** modes (e.g., Conditional GANs, Conditional Diffusion, Conditional VAE).  \n",
        "- Autoregressive models (like GPT) are trained with **teacher forcing** on sequences, which looks like supervised learning, but conceptually it’s still density estimation (so many call them unsupervised).  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. Are they all **Probabilistic**?\n",
        "Yes, at the core they are **probability-based**, but in different ways:  \n",
        "\n",
        "- **Explicit probabilistic models:**  \n",
        "  VAEs, Flows, Autoregressive, NADE (directly compute $$ p(x) $$).  \n",
        "\n",
        "- **Implicit probabilistic models:**  \n",
        "  GANs, EBMs, Diffusion (don’t always give a closed-form $$ p(x) $$, but still define a probability model).  \n",
        "\n",
        "---\n",
        "\n",
        "##  Final Answer\n",
        " These models are **generative, probabilistic, and largely unsupervised** approaches to modeling data distributions.  \n",
        "\n",
        "With nuance:  \n",
        "- GANs and EBMs = **implicit** probabilistic.  \n",
        "- Autoregressive, VAEs, Flows, NADE = **explicit** probabilistic.  \n",
        "- Most are **unsupervised**, but many have **conditional / semi-supervised** variants.  \n"
      ],
      "metadata": {
        "id": "MIwV0alBNcll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Teacher-Forcing vs Residual Connections\n",
        "\n",
        "| Aspect | Teacher-Forcing | Residual (Skip) Connections |\n",
        "|--------|-----------------|-----------------------------|\n",
        "| **Type** | Training strategy | Network architecture design |\n",
        "| **Purpose** | Stabilize sequence learning by guiding the model with ground-truth tokens | Stabilize deep networks by easing gradient flow |\n",
        "| **Where used** | RNNs, LSTMs, GRUs, autoregressive Transformers | Deep networks (CNNs, ResNets, Transformers, etc.) |\n",
        "| **Mechanism** | Feed the true previous token instead of the model’s own prediction | Add a direct shortcut: output = f(x) + x |\n",
        "| **Problem addressed** | Exposure bias (model drifting on wrong predictions) | Vanishing/exploding gradients in deep layers |\n",
        "| **Stage** | Training-time only | Architectural (always present, during training & inference) |\n",
        "| **Analogy** | Teacher guiding step by step with correct answers | Shortcut path that lets information bypass obstacles |\n"
      ],
      "metadata": {
        "id": "1XBe2GaYSQL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Generative AI Models & Their Theoretical Foundations (Rooted)\n",
        "\n",
        "| Generative Model | Deeper Theoretical Roots | Theoretical Foundation |\n",
        "|------------------|--------------------------|-------------------------|\n",
        "| **GANs (Generative Adversarial Networks)** | **Game theory** (Von Neumann, 1928: Minimax theorem) + **statistical decision theory** + probability distributions | Adversarial training = minimax optimization (Goodfellow et al., 2014) |\n",
        "| **VAEs (Variational Autoencoders)** | **Bayesian inference** (Bayes, 1763) + **variational methods** in physics/statistics (mean-field theory, 1950s) + EM algorithm (Dempster, 1977) | Variational inference, KL divergence, ELBO maximization |\n",
        "| **Autoregressive Models (RNN, PixelRNN, GPT)** | **Markov chains** (A. Markov, 1906) + **probability chain rule** + sequential data models (Shannon’s information theory, 1948) | Chain rule factorization of joint distributions |\n",
        "| **Normalizing Flows (RealNVP, Glow)** | **Change-of-variables theorem** (Jacobian determinants in multivariate calculus, 19th c.) + **invertible transformations** in probability | Exact likelihood modeling via invertible mappings |\n",
        "| **Diffusion Models (DDPM, Stable Diffusion)** | **Markov processes** (Kolmogorov, 1930s) + **statistical physics diffusion equations** (Fick’s law, 1855; Einstein’s Brownian motion, 1905) | Forward noising as Gaussian Markov chain, reverse denoising learned |\n",
        "| **EBMs (Energy-Based Models, Boltzmann Machines)** | **Statistical mechanics** (Boltzmann distribution, 1872) + Gibbs distributions + Hopfield networks (1982) | Probability from energy landscapes (low energy → high probability) |\n",
        "| **Score-Based Models / Flow Matching** | **Stochastic calculus** (Ito, 1940s) + **Langevin dynamics** (1908) + Fokker-Planck / SDE theory | Langevin dynamics, SDE/ODE probability flows |\n",
        "| **NADE / MADE (Neural Autoregressive Density Estimators)** | **Autoregressive probability models** (Markov, Kolmogorov) + neural nets as parametrizers | Explicit factorization of densities with neural parametrization |\n",
        "| **Transformers (GPT, BERT, etc.)** | **Sequence-to-sequence modeling** (Shannon, 1948 → encoder-decoder RNNs, 1997) + **attention mechanisms** (Bahdanau, 2014) | Probability distributions over tokens via softmax attention |\n",
        "| **Hybrids (VAE-GAN, Energy-Guided Diffusion)** | Cross-pollination of the above theories: Bayesian inference + game theory + statistical mechanics + stochastic processes | Mix variational inference, adversarial games, and energy principles |\n"
      ],
      "metadata": {
        "id": "ALLUwdL1Sxef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Observations\n",
        "\n",
        "- **Physics roots:** Diffusion, EBMs, Score-based models all trace back to **statistical mechanics** and **stochastic processes** (Einstein, Boltzmann, Langevin).  \n",
        "- **Math roots:** GANs, VAEs, Flows, NADE build on **probability theory, calculus, and optimization**.  \n",
        "- **Information theory roots:** Autoregressive models and Transformers trace to **Shannon** and **Markov** for sequence modeling.  \n",
        "- **Hybrid roots:** Combine these traditions into modern architectures.  \n"
      ],
      "metadata": {
        "id": "Vc-1-37oS6mL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Generative AI Models & Their Theoretical Foundations (Rooted)\n",
        "\n",
        "---\n",
        "\n",
        "### 1. GANs (Generative Adversarial Networks)\n",
        "- **Root Theory & Scientist:** Game theory by **John von Neumann (1928, minimax theorem)**; later extended in statistical decision theory.  \n",
        "- **How it works:** Two networks (Generator vs Discriminator) play a minimax game: the generator produces samples, the discriminator distinguishes real vs fake.  \n",
        "- **Applications:** Image synthesis (faces, art), deepfake generation, data augmentation, super-resolution.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. VAEs (Variational Autoencoders)\n",
        "- **Root Theory & Scientist:** **Thomas Bayes (1763)** — Bayesian inference; **variational methods** in physics/statistics (1950s, mean-field theory); EM algorithm by **Dempster et al., 1977**.  \n",
        "- **How it works:** Encode data into a latent distribution, regularized by KL divergence, then decode to reconstruct; balances accuracy and smooth latent space.  \n",
        "- **Applications:** Anomaly detection, latent space representation, molecule generation, semi-supervised learning.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Autoregressive Models (RNN, PixelRNN, GPT)\n",
        "- **Root Theory & Scientist:** **Andrey Markov (1906)** — Markov chains; **Claude Shannon (1948)** — information theory; chain rule of probability.  \n",
        "- **How it works:** Factorize probability of a sequence step-by-step: each token depends on previous ones. Neural nets (RNNs, Transformers) parameterize conditionals.  \n",
        "- **Applications:** Text generation (GPT), speech synthesis (WaveNet), image modeling (PixelRNN).\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Normalizing Flows (RealNVP, Glow)\n",
        "- **Root Theory & Scientist:** **19th-century calculus & multivariate analysis** — change-of-variables theorem and Jacobians.  \n",
        "- **How it works:** Apply invertible transformations to a simple base distribution (Gaussian) while tracking the Jacobian determinant to compute exact likelihood.  \n",
        "- **Applications:** Density estimation, generative image modeling, likelihood-based anomaly detection.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Diffusion Models (DDPM, Stable Diffusion)\n",
        "- **Root Theory & Scientist:** **Adolf Fick (1855)** — diffusion equations; **Albert Einstein (1905)** — Brownian motion; **Andrey Kolmogorov (1930s)** — Markov processes.  \n",
        "- **How it works:** Add Gaussian noise step by step (forward process), then learn to reverse the process (denoising).  \n",
        "- **Applications:** Text-to-image (Stable Diffusion, Imagen, DALL·E), audio generation, protein design.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. EBMs (Energy-Based Models, Boltzmann Machines)\n",
        "- **Root Theory & Scientist:** **Ludwig Boltzmann (1872)** — statistical mechanics; Gibbs distributions; neural energy models by **Hopfield (1982)**.  \n",
        "- **How it works:** Assign energy values to states; low energy = high probability. Use Boltzmann distribution to normalize.  \n",
        "- **Applications:** Feature learning, collaborative filtering (recommendation), early generative models (RBMs for Deep Belief Nets).\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Score-Based Models / Flow Matching\n",
        "- **Root Theory & Scientist:** **Paul Langevin (1908)** — Langevin dynamics; **Kiyoshi Ito (1940s)** — stochastic calculus; Fokker–Planck / SDE theory.  \n",
        "- **How it works:** Learn the score function (gradient of log-density); sample using stochastic differential equations or deterministic ODE flows.  \n",
        "- **Applications:** High-quality image/audio generation, diffusion alternatives, controllable sampling.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. NADE / MADE (Neural Autoregressive Density Estimators)\n",
        "- **Root Theory & Scientist:** **Markov/Kolmogorov** — autoregressive probability models; extended with neural nets for parameterization.  \n",
        "- **How it works:** Factorize density dimension by dimension, each conditioned on earlier variables. Neural networks efficiently compute these conditionals.  \n",
        "- **Applications:** Exact likelihood modeling, density estimation in high-dimensional data.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. Transformers (GPT, BERT, etc.)\n",
        "- **Root Theory & Scientist:** **Claude Shannon (1948)** — sequence modeling; Encoder-decoder RNNs (1997); **Bahdanau et al. (2014)** — attention mechanism.  \n",
        "- **How it works:** Use self-attention to compute contextualized probabilities over tokens; autoregressive or masked training for generative tasks.  \n",
        "- **Applications:** Large language models (GPT, LLaMA), translation, summarization, multimodal generative AI.\n",
        "\n",
        "---\n",
        "\n",
        "### 10. Hybrids (VAE-GAN, Energy-Guided Diffusion)\n",
        "- **Root Theory & Scientist:** Combine Bayesian inference (Bayes), game theory (Von Neumann), statistical mechanics (Boltzmann), and stochastic processes (Einstein, Langevin).  \n",
        "- **How it works:** Fuse techniques — e.g., VAE-GAN merges reconstruction likelihood with adversarial loss; energy-guided diffusion injects energy priors into denoising.  \n",
        "- **Applications:** Text-to-image with high fidelity (Stable Diffusion), sharper reconstructions, domain-specific generative models.\n"
      ],
      "metadata": {
        "id": "tlYSKT5BYDIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Why Use Logs in Probability & Machine Learning\n",
        "\n",
        "Logs play a central role in probability, statistics, and machine learning.  \n",
        "Here’s **why they are so powerful**:\n",
        "\n",
        "---\n",
        "\n",
        "## 1️ Logs turn products into sums\n",
        "- Probabilities of sequences are **tiny numbers** multiplied together.  \n",
        "- Example:  \n",
        "  $$P(x_1, x_2, ..., x_n) = \\prod_{i=1}^n P(x_i)$$  \n",
        "- Multiplying many small numbers becomes unwieldy.  \n",
        "- Taking logs:  \n",
        "  $$\\log P(x_1, x_2, ..., x_n) = \\sum_{i=1}^n \\log P(x_i)$$  \n",
        "-  Much easier to compute and reason about.\n",
        "\n",
        "---\n",
        "\n",
        "## 2️ Logs avoid underflow\n",
        "- Computers struggle with **very small numbers**: multiplying them can push results to **0 (underflow)**.  \n",
        "- Logs keep values in a stable range.  \n",
        "- Example:  \n",
        "  $$10^{-1000} \\to \\text{underflow} \\quad \\text{but} \\quad \\log(10^{-1000}) = -1000 \\quad \\text{safe!}$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3️ Logs smooth growth\n",
        "- Probabilities can span **huge ranges**.  \n",
        "- Logs compress these ranges into **manageable scales**.  \n",
        "- Example:  \n",
        "  $$\\log(1{,}000{,}000) = 6 \\quad \\text{vs raw value } 1{,}000{,}000$$  \n",
        "-  Makes comparison and optimization more stable.\n",
        "\n",
        "---\n",
        "\n",
        "## 4️ Logs make optimization easier\n",
        "- In maximum likelihood estimation (MLE):  \n",
        "  - Goal: maximize  \n",
        "    $$P(\\text{data}|\\theta) = \\prod_{i=1}^n P(x_i|\\theta)$$  \n",
        "  - Equivalent (but easier): maximize  \n",
        "    $$\\log P(\\text{data}|\\theta) = \\sum_{i=1}^n \\log P(x_i|\\theta)$$  \n",
        "- Same optimum, but **log-likelihood** gives simpler math, better gradients, and easier optimization.\n",
        "\n",
        "---\n",
        "\n",
        "#  In short\n",
        "**Logs make probabilities easier to compute, compare, and optimize.**  \n",
        "They are the **mathematical lens** that turns fragile probability products into stable, scalable, and learnable objectives.\n"
      ],
      "metadata": {
        "id": "4gBYVLK5GLS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN Objective Function (Minimax Game)\n",
        "\n",
        "## Equation\n",
        "$$\n",
        "\\min_{G} \\max_{D} V(D, G) \\;=\\;\n",
        "\\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\big[ \\log D(x) \\big]\n",
        "+ \\mathbb{E}_{z \\sim p_z(z)} \\big[ \\log (1 - D(G(z))) \\big]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Explanation of Symbols\n",
        "\n",
        "- $$\\min_G \\max_D$$  \n",
        "  A **two-player minimax game**:  \n",
        "  - The **Discriminator (D)** tries to maximize the objective (classify real vs fake correctly).  \n",
        "  - The **Generator (G)** tries to minimize the objective (fool D).  \n",
        "\n",
        "- $$V(D, G)$$  \n",
        "  The **value function**, i.e., the score of the adversarial game.  \n",
        "\n",
        "- $$\\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\cdot]$$  \n",
        "  Expectation over **real data samples** from the true data distribution $$p_{\\text{data}}(x)$$.  \n",
        "\n",
        "- $$\\log D(x)$$  \n",
        "  The log-probability that D assigns to a real sample being real.  \n",
        "  - If $$D(x) \\to 1$$ → high confidence for real.  \n",
        "\n",
        "- $$\\mathbb{E}_{z \\sim p_z(z)}[\\cdot]$$  \n",
        "  Expectation over **noise vectors** $$z$$ sampled from a prior distribution $$p_z(z)$$ (e.g., Gaussian or uniform).  \n",
        "\n",
        "- $$G(z)$$  \n",
        "  The generator’s output — a fake sample created from noise $$z$$.  \n",
        "\n",
        "- $$D(G(z))$$  \n",
        "  The discriminator’s probability that the generated sample $$G(z)$$ is real.  \n",
        "\n",
        "- $$\\log (1 - D(G(z)))$$  \n",
        "  The log-probability that D correctly identifies the fake as fake.  \n",
        "  - If $$D(G(z)) \\to 0$$ → D wins.  \n",
        "  - If $$D(G(z)) \\to 1$$ → G wins.  \n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "- **Discriminator (D):** Maximizes the objective by making $$D(x) \\to 1$$ for real data and $$D(G(z)) \\to 0$$ for fake data.  \n",
        "- **Generator (G):** Minimizes the objective by pushing $$D(G(z)) \\to 1$$, making its fakes look real.  \n",
        "- Training is a dynamic **push–pull game**.  \n",
        "- The equilibrium occurs when the generator produces samples indistinguishable from real data, i.e., when $$p_g(x) \\approx p_{\\text{data}}(x)$$.\n"
      ],
      "metadata": {
        "id": "yN0SWG5QSlJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Autoencoder (VAE) — Evidence Lower Bound (ELBO)\n",
        "\n",
        "## Equation\n",
        "$$\n",
        "\\log p_{\\theta}(x) \\;\\geq\\;\n",
        "\\mathbb{E}_{z \\sim q_{\\phi}(z|x)} \\big[ \\log p_{\\theta}(x|z) \\big]\n",
        "- \\text{KL}\\!\\left(q_{\\phi}(z|x) \\,\\|\\, p(z)\\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Explanation of Symbols\n",
        "\n",
        "- $$\\log p_{\\theta}(x)$$  \n",
        "  The **log-likelihood** of the observed data $$x$$ under the generative model with parameters $$\\theta$$.  \n",
        "  - Goal: maximize this, but it’s usually intractable.  \n",
        "\n",
        "- $$\\geq$$  \n",
        "  Indicates we are computing a **lower bound** (the ELBO) on the true log-likelihood.  \n",
        "\n",
        "- $$\\mathbb{E}_{z \\sim q_{\\phi}(z|x)}[\\cdot]$$  \n",
        "  Expectation with respect to the **approximate posterior distribution** $$q_{\\phi}(z|x)$$, which the encoder learns.  \n",
        "\n",
        "- $$q_{\\phi}(z|x)$$  \n",
        "  The **encoder (inference model)** with parameters $$\\phi$$.  \n",
        "  - Approximates the true but intractable posterior $$p(z|x)$$.  \n",
        "\n",
        "- $$\\log p_{\\theta}(x|z)$$  \n",
        "  The **reconstruction likelihood**: probability of reconstructing $$x$$ given latent variable $$z$$, under decoder parameters $$\\theta$$.  \n",
        "\n",
        "- $$p(z)$$  \n",
        "  The **prior distribution** over latent variables (commonly standard Gaussian $$\\mathcal{N}(0, I)$$).  \n",
        "\n",
        "- $$\\text{KL}\\!\\left(q_{\\phi}(z|x) \\,\\|\\, p(z)\\right)$$  \n",
        "  Kullback–Leibler divergence: measures how far the encoder’s posterior $$q_{\\phi}(z|x)$$ is from the prior $$p(z)$$.  \n",
        "  - Acts as a **regularizer** for the latent space.  \n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "- The ELBO has **two competing terms**:  \n",
        "  1. **Reconstruction term**: $$\\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]$$ → encourages accurate reconstructions.  \n",
        "  2. **Regularization term**: $$-\\text{KL}(q_{\\phi}(z|x)\\,\\|\\,p(z))$$ → keeps latent space well-structured, close to the prior.  \n",
        "\n",
        "- Together, this balances **data fidelity** and **latent smoothness**, making VAEs powerful generative models.\n"
      ],
      "metadata": {
        "id": "Uk0NuDoeS_QZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Factorization of Probability (Chain Rule)\n",
        "\n",
        "## Equation\n",
        "$$\n",
        "p(x) \\;=\\; \\prod_{t=1}^{T} \\; p\\big(x_t \\mid x_{<t}\\big)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Explanation of Symbols\n",
        "\n",
        "- $$x = (x_1, x_2, \\dots, x_T)$$  \n",
        "  The full sequence (e.g., tokens in a sentence, frames, samples).\n",
        "\n",
        "- $$p(x)$$  \n",
        "  The **joint probability** of the entire sequence.\n",
        "\n",
        "- $$\\prod_{t=1}^{T} (\\cdot)$$  \n",
        "  Product operator: multiply the terms for all time steps \\(t=1,\\dots,T\\).\n",
        "\n",
        "- $$x_{<t} = (x_1, \\dots, x_{t-1})$$  \n",
        "  All elements **before** time \\(t\\) (the “history”).\n",
        "\n",
        "- $$p(x_t \\mid x_{<t})$$  \n",
        "  The **conditional probability** of the current element \\(x_t\\) given all previous elements.\n",
        "\n",
        "---\n",
        "\n",
        "## Small Expansion (example \\(T=3\\))\n",
        "$$\n",
        "p(x_1, x_2, x_3)\n",
        "= p(x_1)\\; p(x_2 \\mid x_1)\\; p(x_3 \\mid x_1, x_2).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "- This is the **chain rule of probability**.  \n",
        "- It underpins **autoregressive modeling**: learn each conditional $$(p(x_t \\mid x_{<t}))$$ and multiply to get $$(p(x))$$.\n"
      ],
      "metadata": {
        "id": "UkhFT-9wTabb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalizing Flows — Change of Variables\n",
        "\n",
        "## Equation\n",
        "$$\n",
        "p_X(x) \\;=\\; p_Z(f(x)) \\; \\Bigg| \\det \\frac{\\partial f(x)}{\\partial x} \\Bigg|\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Explanation of Symbols\n",
        "\n",
        "- $$x \\in \\mathbb{R}^n$$  \n",
        "  A data point in the original (data) space.\n",
        "\n",
        "- $$f: \\mathbb{R}^n \\to \\mathbb{R}^n$$  \n",
        "  An **invertible mapping function** that transforms data $$x$$ into latent variable $$z$$.  \n",
        "  - $$z = f(x)$$.\n",
        "\n",
        "- $$p_X(x)$$  \n",
        "  Probability density of the data point $$x$$ in the **data space**.\n",
        "\n",
        "- $$p_Z(z)$$  \n",
        "  Probability density of the latent variable $$z$$ in the **latent space**.  \n",
        "  - Usually a simple distribution (e.g., standard Gaussian).\n",
        "\n",
        "- $$\\frac{\\partial f(x)}{\\partial x}$$  \n",
        "  The **Jacobian matrix** of function $$f$$ at point $$x$$.  \n",
        "  - It contains all partial derivatives of $$f$$ with respect to $$x$$.  \n",
        "\n",
        "- $$\\det \\frac{\\partial f(x)}{\\partial x}$$  \n",
        "  The **determinant of the Jacobian**, which measures how much volume is stretched or compressed by the mapping $$f$$ at point $$x$$.  \n",
        "\n",
        "- $$\\Big|\\det \\frac{\\partial f(x)}{\\partial x}\\Big|$$  \n",
        "  Absolute value ensures probabilities stay non-negative.  \n",
        "  - Large determinant = space expanded.  \n",
        "  - Small determinant = space compressed.  \n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "- We start with a **simple prior distribution** in latent space $$p_Z(z)$$ (like Gaussian).  \n",
        "- We transform it into a **complex data distribution** $$p_X(x)$$ using invertible mappings $$f$$.  \n",
        "- The Jacobian determinant adjusts probability density to account for this stretching/compression of space.  \n"
      ],
      "metadata": {
        "id": "TJ9HzfZRTvL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion Models — Forward and Reverse Processes\n",
        "\n",
        "## Forward (noising) process\n",
        "$$\n",
        "q(x_t \\mid x_{t-1}) \\;=\\; \\mathcal{N}\\!\\Big(x_t \\; ; \\; \\sqrt{1 - \\beta_t}\\, x_{t-1}, \\; \\beta_t I \\Big)\n",
        "$$\n",
        "\n",
        "### Explanation\n",
        "- $$q(x_t \\mid x_{t-1})$$ → Probability of noisy sample $$x_t$$ given previous step $$x_{t-1}$$.  \n",
        "- $$\\mathcal{N}(\\mu, \\Sigma)$$ → Gaussian distribution with mean $$\\mu$$ and covariance $$\\Sigma$$.  \n",
        "- $$\\sqrt{1 - \\beta_t}\\, x_{t-1}$$ → Scaled version of the previous sample (shrinks signal).  \n",
        "- $$\\beta_t I$$ → Variance term; adds Gaussian noise proportional to step-size $$\\beta_t$$.  \n",
        "- Interpretation: Gradually adds Gaussian noise step by step until data becomes pure noise.  \n",
        "\n",
        "---\n",
        "\n",
        "## Reverse (denoising) process\n",
        "$$\n",
        "p_{\\theta}(x_{t-1} \\mid x_t) \\;=\\; \\mathcal{N}\\!\\Big(x_{t-1} \\; ; \\; \\mu_{\\theta}(x_t, t), \\; \\Sigma_{\\theta}(x_t, t) \\Big)\n",
        "$$\n",
        "\n",
        "### Explanation\n",
        "- $$p_{\\theta}(x_{t-1} \\mid x_t)$$ → Model’s learned distribution for denoising.  \n",
        "- $$\\mu_{\\theta}(x_t, t)$$ → Neural network–predicted mean (denoised signal).  \n",
        "- $$\\Sigma_{\\theta}(x_t, t)$$ → Predicted covariance (uncertainty in denoising).  \n",
        "- Interpretation: Learn to reverse the noising process by predicting cleaner samples step by step.  \n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "- **Forward process:** Data → Noise (fixed Gaussian corruption).  \n",
        "- **Reverse process:** Noise → Data (learned by neural net).  \n",
        "- Final sampling: Start with pure Gaussian noise and iteratively denoise using the reverse process to generate realistic data (e.g., images).\n"
      ],
      "metadata": {
        "id": "48GjSbxyUL-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Energy-Based Models (EBMs) — Boltzmann Distribution\n",
        "\n",
        "## Equation\n",
        "$$\n",
        "p_{\\theta}(x) \\;=\\; \\frac{e^{-E_{\\theta}(x)}}{Z_{\\theta}},\n",
        "\\qquad\n",
        "Z_{\\theta} \\;=\\; \\int e^{-E_{\\theta}(x)} \\, dx\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Explanation of Symbols\n",
        "\n",
        "- $$p_{\\theta}(x)$$  \n",
        "  Probability of data sample $$x$$ under the model, parameterized by $$\\theta$$.\n",
        "\n",
        "- $$E_{\\theta}(x)$$  \n",
        "  The **energy function**.  \n",
        "  - Assigns a scalar \"energy\" to each configuration $$x$$.  \n",
        "  - Lower energy = higher probability (valleys in energy landscape).  \n",
        "  - Higher energy = lower probability (peaks).  \n",
        "\n",
        "- $$e^{-E_{\\theta}(x)}$$  \n",
        "  Exponential weighting.  \n",
        "  - Ensures probabilities are positive.  \n",
        "  - Strongly favors low-energy (more likely) states.  \n",
        "\n",
        "- $$Z_{\\theta}$$ (partition function)  \n",
        "  $$Z_{\\theta} = \\int e^{-E_{\\theta}(x)} dx$$  \n",
        "  - Normalizing constant to ensure $$p_{\\theta}(x)$$ is a valid probability distribution (sums/integrates to 1).  \n",
        "  - Often intractable in high dimensions.  \n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "- The model defines an **energy landscape** over all possible data samples.  \n",
        "- Realistic data lies in **low-energy valleys** (high probability).  \n",
        "- Unrealistic data lies in **high-energy regions** (low probability).  \n",
        "- Training = shaping the energy function so that real data is assigned lower energy than fake/noisy data.\n"
      ],
      "metadata": {
        "id": "s8xaNZT6UdLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Score-Based / Flow-Matching Models\n",
        "\n",
        "## Stochastic Differential Equation (SDE)\n",
        "\n",
        "$$\n",
        "\\frac{dx}{dt} \\;=\\; \\nabla_x \\log p_{\\theta}(x) \\;+\\; \\sqrt{2}\\, dW_t\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Explanation of Symbols\n",
        "\n",
        "- $$\\frac{dx}{dt}$$  \n",
        "  The **change in data sample** $$x$$ with respect to continuous time $$t$$.  \n",
        "  - Describes how $$x$$ evolves as a stochastic process.  \n",
        "\n",
        "- $$\\nabla_x \\log p_{\\theta}(x)$$  \n",
        "  The **score function**: gradient of the log-probability with respect to $$x$$.  \n",
        "  - Points in the direction where data is **more likely**.  \n",
        "  - Guides the process toward regions of higher probability density.  \n",
        "\n",
        "- $$p_{\\theta}(x)$$  \n",
        "  Probability distribution of the data under parameters $$\\theta$$.  \n",
        "\n",
        "- $$\\sqrt{2}\\, dW_t$$  \n",
        "  Noise term:  \n",
        "  - $$W_t$$ = Wiener process (standard Brownian motion).  \n",
        "  - $$dW_t$$ = infinitesimal random Gaussian step.  \n",
        "  - $$\\sqrt{2}$$ = scaling factor that controls noise intensity.  \n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "- This SDE says:  \n",
        "  - Move **towards higher probability regions** (via the score function).  \n",
        "  - Add a bit of **random Gaussian noise** at each step.  \n",
        "\n",
        "- The balance of **gradient flow** and **stochastic noise** allows sampling from complex distributions.  \n",
        "- In generative modeling:  \n",
        "  - Start from noise.  \n",
        "  - Follow the reverse-time SDE using a neural network that learns the score function.  \n",
        "  - Result: realistic samples (images, text, etc.).  \n"
      ],
      "metadata": {
        "id": "YA-I5_c6U6Xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Autoregressive Density Estimators (NADE)\n",
        "\n",
        "## Density Factorization\n",
        "$$\n",
        "p(x) \\;=\\; \\prod_{i=1}^{D} \\; p(x_i \\mid x_{<i}; \\theta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Explanation of Symbols\n",
        "\n",
        "- $$x = (x_1, x_2, \\dots, x_D)$$  \n",
        "  The full data vector with $$D$$ dimensions (e.g., pixels, features, tokens).\n",
        "\n",
        "- $$p(x)$$  \n",
        "  The **joint probability distribution** of the entire data vector.\n",
        "\n",
        "- $$\\prod_{i=1}^{D} (\\cdot)$$  \n",
        "  Product operator — multiply the conditional probabilities of all components.\n",
        "\n",
        "- $$p(x_i \\mid x_{<i}; \\theta)$$  \n",
        "  The conditional probability of the current component $$x_i$$ given all the **previous components** $$x_{<i} = (x_1, \\dots, x_{i-1})$$.  \n",
        "  - Parameterized by neural network weights $$\\theta$$.  \n",
        "\n",
        "- $$\\theta$$  \n",
        "  The parameters of the neural network (weights and biases) that learn to model the conditionals.  \n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "- The **chain rule of probability** lets us break down the joint distribution into conditionals.  \n",
        "- NADE uses a neural network to model each conditional distribution.  \n",
        "- This gives an **exact, tractable density estimator** (unlike GANs or EBMs, which are implicit).  \n",
        "\n",
        "---\n",
        "\n",
        "## Example (D = 3)\n",
        "$$\n",
        "p(x_1, x_2, x_3) = p(x_1)\\; p(x_2 \\mid x_1)\\; p(x_3 \\mid x_1, x_2).\n",
        "$$\n"
      ],
      "metadata": {
        "id": "wD3xZNocVKGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer-Based Generative Models\n",
        "\n",
        "## Self-Attention Mechanism\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) \\;=\\;\n",
        "\\text{softmax}\\!\\left( \\frac{QK^{\\top}}{\\sqrt{d_k}} \\right) V\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Explanation of Symbols\n",
        "\n",
        "- $$Q$$ (**Query matrix**)  \n",
        "  Represents the current token (or position) we are focusing on.  \n",
        "  - Shape: (sequence length, attention dimension).\n",
        "\n",
        "- $$K$$ (**Key matrix**)  \n",
        "  Represents the context of all tokens (encodings of inputs).  \n",
        "  - Used to decide how much each token should attend to others.\n",
        "\n",
        "- $$V$$ (**Value matrix**)  \n",
        "  Contains the actual information content associated with tokens.  \n",
        "  - The weighted sum of these values forms the new representation.\n",
        "\n",
        "- $$QK^{\\top}$$  \n",
        "  Dot product between queries and keys.  \n",
        "  - Measures **similarity** or relevance between tokens.\n",
        "\n",
        "- $$\\sqrt{d_k}$$  \n",
        "  Scaling factor, where $$d_k$$ is the dimension of the key vectors.  \n",
        "  - Prevents the dot product from growing too large, which stabilizes softmax.\n",
        "\n",
        "- $$\\text{softmax}(\\cdot)$$  \n",
        "  Converts similarity scores into a **probability distribution** over tokens (values between 0 and 1, summing to 1).\n",
        "\n",
        "- Final multiplication with $$V$$  \n",
        "  Produces a weighted combination of values, where weights are the attention scores.\n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "- Each token asks a **query** about all other tokens’ **keys**.  \n",
        "- Softmax decides how much attention each token should pay to others.  \n",
        "- The output is a context-aware representation: every token becomes enriched with information from relevant tokens in the sequence.  \n",
        "- This mechanism enables Transformers to capture **long-range dependencies** in data, unlike RNNs which struggle with distant context.\n"
      ],
      "metadata": {
        "id": "AU4IPUxgVg1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Generative Models\n",
        "\n",
        "## Essence\n",
        "Hybrid models **combine techniques** from multiple generative frameworks to leverage their strengths and overcome weaknesses.  \n",
        "\n",
        "- Example: **VAE-GAN** combines the **variational inference** power of VAEs with the **adversarial training** of GANs.  \n",
        "\n",
        "---\n",
        "\n",
        "## Example Equation (VAE-GAN)\n",
        "$$\n",
        "\\mathcal{L} \\;=\\; \\mathcal{L}_{\\text{VAE}} \\;+\\; \\lambda \\, \\mathcal{L}_{\\text{GAN}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Explanation of Symbols\n",
        "\n",
        "- $$\\mathcal{L}$$  \n",
        "  The **total loss function** for the hybrid model.  \n",
        "\n",
        "- $$\\mathcal{L}_{\\text{VAE}}$$  \n",
        "  The **VAE loss** (Evidence Lower Bound, ELBO):  \n",
        "  $$\n",
        "  \\mathcal{L}_{\\text{VAE}} \\;=\\;\n",
        "  \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\n",
        "  - \\text{KL}\\!\\left(q_\\phi(z|x) \\,\\|\\, p(z)\\right)\n",
        "  $$\n",
        "  - First term = reconstruction accuracy.  \n",
        "  - Second term = KL divergence regularizer on latent space.  \n",
        "\n",
        "- $$\\mathcal{L}_{\\text{GAN}}$$  \n",
        "  The **GAN loss**:  \n",
        "  $$\n",
        "  \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)]\n",
        "  + \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))]\n",
        "  $$\n",
        "  - Discriminator tries to separate real/fake.  \n",
        "  - Generator tries to fool discriminator.  \n",
        "\n",
        "- $$\\lambda$$  \n",
        "  A **weighting coefficient** that balances how much influence the GAN loss has relative to the VAE loss.  \n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "- **VAE alone** → generates blurry but consistent samples (good coverage, smooth latent space).  \n",
        "- **GAN alone** → generates sharp but sometimes unstable samples (mode collapse).  \n",
        "- **VAE-GAN hybrid** → combines them:  \n",
        "  - VAE ensures a structured latent space and coverage.  \n",
        "  - GAN sharpens results via adversarial feedback.  \n"
      ],
      "metadata": {
        "id": "v82HKiePWHhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI Models and Their Applications\n",
        "\n",
        "| Generative Model | Core Idea | Typical Applications |\n",
        "|------------------|-----------|-----------------------|\n",
        "| **GANs (Generative Adversarial Networks)** | Adversarial game between Generator and Discriminator | Image generation (e.g., deepfakes), super-resolution, style transfer, data augmentation |\n",
        "| **VAEs (Variational Autoencoders)** | Latent-variable model with ELBO training (reconstruction + KL regularization) | Image reconstruction, anomaly detection, representation learning, molecule design |\n",
        "| **Autoregressive Models (RNN, LSTM, GPT, PixelRNN, WaveNet)** | Factorize joint distribution using chain rule; predict one token at a time | Language modeling (GPT), speech synthesis (WaveNet), image generation (PixelRNN) |\n",
        "| **Normalizing Flows (RealNVP, Glow)** | Invertible transformations with Jacobian determinant for exact likelihoods | Density estimation, audio synthesis, molecular modeling, uncertainty estimation |\n",
        "| **Diffusion Models (DDPM, Stable Diffusion, Imagen)** | Forward Gaussian noising (Markov chain) + learned reverse denoising | High-quality image generation (Stable Diffusion, Imagen), text-to-image synthesis, audio generation |\n",
        "| **Energy-Based Models (EBMs, Boltzmann Machines)** | Probability from energy landscapes (low energy = high probability) | Representation learning, generative modeling, reinforcement learning, scientific modeling |\n",
        "| **Score-Based / Flow-Matching Models** | Stochastic differential equations; simulate with score function (∇ log p(x)) | Image generation (score-based diffusion), physics-informed generative models |\n",
        "| **NADE / MADE (Neural Autoregressive Density Estimators)** | Exact tractable density estimation via autoregressive factorization | Density estimation, structured data modeling, probabilistic reasoning |\n",
        "| **Transformer-Based Generative Models (GPT, BERT, etc.)** | Self-attention + softmax to model token dependencies | Large language models (GPT), translation, summarization, code generation |\n",
        "| **Hybrids (VAE-GAN, Diffusion+Transformers, Energy-Guided Diffusion)** | Combine multiple generative frameworks to balance strengths | VAE-GAN: sharper images; Stable Diffusion: text-to-image with cross-attention; Energy-guided: physics + diffusion |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "NXPKlGKKQuUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a compact, up-to-date reading list of the most-cited/representative academic papers for each generative model family we discussed. I included both seminal work and influential follow-ups so you can go deep fast.\n",
        "\n",
        "# Generative Model Families → Key Papers\n",
        "\n",
        "| Model family | Seminal paper(s) | Influential follow-ups / recent guides |\n",
        "|--------------|------------------|---------------------------------------|\n",
        "| **GANs** | Goodfellow et al., “Generative Adversarial Networks” (2014). [arXiv][1] | WGAN (Arjovsky et al., 2017) and WGAN-GP (Gulrajani et al., 2017) for stability; StyleGAN (Karras et al., 2019) for controllable synthesis. [arXiv][2] |\n",
        "| **VAEs** | Kingma & Welling, “Auto-Encoding Variational Bayes” (2013). [arXiv][3] | Doersch, “Tutorial on VAEs” (2016); β-VAE (Higgins et al., 2017) for disentanglement. [arXiv][4] |\n",
        "| **Autoregressive (PixelRNN/WaveNet/GPT)** | PixelRNN (van den Oord et al., 2016) and WaveNet (2016). [arXiv][5] | Transformer “Attention Is All You Need” (2017); GPT-3 (2020) as a large-scale autoregressive LM. [openreview.net][6] |\n",
        "| **Normalizing flows** | RealNVP (Dinh et al., 2017). [semanticscholar.org][7] | Glow (Kingma & Dhariwal, 2018); FFJORD (Grathwohl et al., 2019) for continuous-time flows. [ar5iv][8] |\n",
        "| **Diffusion models (DDPM, latent diffusion)** | Sohl-Dickstein et al., “Deep Unsupervised Learning using Nonequilibrium Thermodynamics” (2015). [arXiv][9] | DDPM (Ho et al., 2020); Improved DDPM & Guided Diffusion (Nichol & Dhariwal, 2021); Latent Diffusion / Stable Diffusion (Rombach et al., 2022); SDXL (2023). [arXiv][10] |\n",
        "| **EBMs (Boltzmann/energy)** | LeCun et al., “A Tutorial on Energy-Based Learning” (2006). [Stanford University][11] | Hinton’s RBM/DBN line (2006–2010) for practical training of energy models. [cs.toronto.edu][12] |\n",
        "| **Score-based generative models / SDEs** | Score Matching (Hyvärinen, 2005) as the estimator; NCSN (Song & Ermon, 2019). [jmlr.org][13] | Score-based SDE framework (Song et al., 2021); Flow/Rectified-Flow perspectives (Lipman et al., 2022; Liu et al., 2022); practical guide to Flow Matching (2024). [arXiv][14] |\n",
        "| **NADE / MADE (neural autoregressive density estimators)** | NADE (Larochelle & Murray, 2011). [Proceedings of Machine Learning Research][15] | MADE (Germain et al., 2015) and later NADE surveys/extensions. [arXiv][16] |\n",
        "| **Transformers (as generative sequence models)** | Transformer (Vaswani et al., 2017). [openreview.net][6] | GPT-3 (Brown et al., 2020) highlighting scaling for generative modeling. [semanticscholar.org][7] |\n",
        "| **Hybrids (e.g., VAE-GAN, guided diffusion)** | VAE-GAN (Larsen et al., 2016). [arXiv][17] | Guided diffusion: classifier guidance (Dhariwal & Nichol, 2021) and classifier-free guidance (Ho & Salimans, 2022). [proceedings.nips.cc][18] |\n",
        "\n",
        "---\n",
        "\n",
        "*Notes*:\n",
        "\n",
        "- I favored authoritative sources (original papers, PMLR/NeurIPS/ICLR/CVPR proceedings, arXiv) and added modern guides where they materially improve understanding (e.g., Flow Matching guide).\n",
        "- If you want this tailored to a specific application (vision, audio, code, molecules), say the word and I’ll filter the list to that domain with the most relevant, recent papers.\n",
        "\n",
        "[1]: https://arxiv.org/abs/1406.2661 \"[1406.2661] Generative Adversarial Networks\"  \n",
        "[2]: https://arxiv.org/abs/1701.07875 \"Wasserstein GAN\"  \n",
        "[3]: https://arxiv.org/abs/1312.6114 \"Auto-Encoding Variational Bayes\"  \n",
        "[4]: https://arxiv.org/abs/1606.05908 \"Tutorial on Variational Autoencoders\"  \n",
        "[5]: https://arxiv.org/abs/1601.06759 \"Pixel Recurrent Neural Networks\"  \n",
        "[6]: https://openreview.net/forum?id=HkpbnH9lx \"Density estimation using Real NVP\"  \n",
        "[7]: https://www.semanticscholar.org/paper/Density-estimation-using-Real-NVP-Dinh-Sohl-Dickstein/09879f7956dddc2a9328f5c1472feeb8402bcbcf/figure/0 \"Figure 1 from Density estimation using Real NVP\"  \n",
        "[8]: https://ar5iv.labs.arxiv.org/html/1807.03039 \"Glow: Generative Flow with Invertible 1×1 Convolutions - ar5iv\"  \n",
        "[9]: https://arxiv.org/abs/1503.03585 \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\"  \n",
        "[10]: https://arxiv.org/abs/2006.11239 \"Denoising Diffusion Probabilistic Models\"  \n",
        "[11]: https://web.stanford.edu/class/cs379c/archive/2012/suggested_reading_list/documents/LeCunetal06.pdf \"[PDF] A Tutorial on Energy-Based Learning - Stanford University\"  \n",
        "[12]: https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf \"A fast learning algorithm for deep belief nets - Computer Science\"  \n",
        "[13]: https://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf \"Estimation of Non-Normalized Statistical Models by Score ...\"  \n",
        "[14]: https://arxiv.org/abs/2011.13456 \"Score-Based Generative Modeling through Stochastic ...\"  \n",
        "[15]: https://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf \"The Neural Autoregressive Distribution Estimator\"  \n",
        "[16]: https://arxiv.org/abs/1502.03509 \"MADE: Masked Autoencoder for Distribution Estimation\"  \n",
        "[17]: https://arxiv.org/abs/1512.09300 \"Autoencoding beyond pixels using a learned similarity metric\"  \n",
        "[18]: https://proceedings.nips.cc/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf \"Diffusion Models Beat GANs on Image Synthesis\"  \n"
      ],
      "metadata": {
        "id": "udWznm1WRTwd"
      }
    }
  ]
}