{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOA3bPbNEIe5afkN4yyU4c5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ“œ Loss Functions in Dimensionality Reduction (AI/ML/DL)\n","\n","---\n","\n","## ðŸ”¹ 1. Classical ML / Statistical Losses\n","\n","* **PCA (Principal Component Analysis)**  \n","  * **Loss:** Reconstruction error (squared Euclidean).  \n","  * $$L = \\|X - X W W^T\\|^2$$  \n","  * Minimizes variance lost in projection.\n","\n","* **MDS (Multidimensional Scaling, 1950s)**  \n","  * **Loss:** Stress function = difference in pairwise distances.  \n","  * Preserves global geometry.\n","\n","* **Isomap (2000)**  \n","  * **Loss:** Preserves geodesic (shortest-path) distances.  \n","  * Captures nonlinear manifolds.\n","\n","* **t-SNE (2008)**  \n","  * **Loss:** KL divergence between high- and low-dim neighbor probabilities.  \n","  * Preserves local neighborhoods.  \n","\n","  $$L = D_{KL}(P \\parallel Q) = \\sum_{i \\ne j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$$  \n","\n","* **UMAP (2018)**  \n","  * **Loss:** Cross-entropy between neighbor graphs.  \n","  * Balances local & global structure.\n","\n","---\n","\n","## ðŸ”¹ 2. Autoencoder-Based Losses (DL)\n","\n","* **Basic Autoencoder** (1986 â†’ deep revival 2006)  \n","  * **Loss:**  \n","    $$L = \\|x - \\hat{x}\\|^2$$  \n","\n","* **Denoising Autoencoder (Vincent, 2008)**  \n","  * Input corrupted, output clean.  \n","  * **Loss:** MSE on reconstructions.  \n","\n","* **Sparse Autoencoder**  \n","  * **Loss:** Reconstruction MSE + L1 penalty on activations.  \n","  * $$L = \\|x - \\hat{x}\\|^2 + \\lambda \\sum |h|$$  \n","\n","* **Contractive Autoencoder (Rifai, 2011)**  \n","  * **Loss:**  \n","    $$L = \\|x - \\hat{x}\\|^2 + \\lambda \\|\\nabla_x h(x)\\|_F^2$$  \n","  * Encourages robustness to perturbations.\n","\n","---\n","\n","## ðŸ”¹ 3. Generative & Probabilistic Losses\n","\n","* **Variational Autoencoder (VAE, 2013)**  \n","  * **Loss:**  \n","\n","    $$\n","    \\mathcal{L} = \\mathbb{E}_{q(z|x)}[\\|x - \\hat{x}\\|^2]\n","    + \\beta \\, D_{KL}(q(z|x) \\parallel p(z))\n","    $$  \n","\n","* **Î²-VAE (2017)**  \n","  * Same as VAE but $$\\beta > 1$$ â†’ disentanglement.\n","\n","* **VQ-VAE (2017)**  \n","  * **Loss:** Reconstruction + Vector Quantization + Commitment.  \n","  * Discrete latent embeddings.\n","\n","---\n","\n","## ðŸ”¹ 4. Contrastive & Representation Learning\n","\n","* **SimCLR (2020)**  \n","  * **Loss:** NT-Xent (normalized temperature-scaled cross-entropy).  \n","\n","    $$\n","    L = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k \\ne i} \\exp(\\text{sim}(z_i, z_k)/\\tau)}\n","    $$  \n","\n","* **BYOL (2020), SwAV (2020)**  \n","  * Losses align augmented views without negatives.  \n","  * Dimensionality reduced to embeddings.\n","\n","* **Deep Embedding Clustering (DEC, 2016)**  \n","  * **Loss:** KL divergence between soft cluster assignments and target distribution.\n","\n","---\n","\n","## ðŸ”¹ 5. Domain-Specific\n","\n","* **Triplet Loss (FaceNet, 2015):**  \n","  $$L = \\max(0, d(a,p) - d(a,n) + \\alpha)$$  \n","  Forces similar samples close, dissimilar far.\n","\n","* **Manifold Regularization:**  \n","  Penalizes deviations from neighborhood graph.\n","\n","* **Deep t-SNE / Parametric t-SNE:**  \n","  Neural networks + KL divergence objective.\n","\n","---\n","\n","## âœ… Quick Comparative Table\n","\n","| Method / Family       | Loss Type                           | Preserves / Encourages           |\n","| --------------------- | ----------------------------------- | -------------------------------- |\n","| PCA                   | MSE (linear recon.)                 | Variance (global)                |\n","| MDS / Isomap          | Distance stress / geodesics         | Geometry, manifolds              |\n","| t-SNE                 | KL divergence (neighbors)           | Local clusters                   |\n","| UMAP                  | Cross-entropy (graphs)              | Local + global balance           |\n","| Autoencoders          | MSE (+ sparse/contractive terms)    | Compact latent recon.            |\n","| VAE / Î²-VAE / VQ-VAE  | Recon + KL (+ quantization)         | Probabilistic latent structure   |\n","| SimCLR, BYOL, SwAV    | Contrastive/SSL losses              | Representation alignment         |\n","| Triplet / Metric      | Distance-based (margin, triplets)   | Semantic similarity in embedding |\n","\n","---\n"],"metadata":{"id":"lvJhYh-nXT2x"}},{"cell_type":"markdown","source":["# ðŸ“Š Comparative Table: Loss Functions in Dimensionality Reduction (AI/ML/DL)\n","\n","| Method / Loss            | Formula (simplified)                                                                 | Intuition                                   | Pros                                     | Cons                          | When to Use                                |\n","|---------------------------|---------------------------------------------------------------------------------------|---------------------------------------------|------------------------------------------|-------------------------------|--------------------------------------------|\n","| **PCA (Reconstruction)** | $$L = \\|X - X W W^T\\|^2$$                                                             | Preserve variance by minimizing reconstruction error | Simple, convex, closed-form              | Linear only, ignores local structure | Large-scale linear DR, preprocessing       |\n","| **MDS (Stress Loss)**    | $$L = \\sum_{i,j} (d_{ij}^{HD} - d_{ij}^{LD})^2$$                                      | Preserve pairwise distances                 | Captures global geometry                  | Sensitive to noise, costly for large $$n$$ | Visualization with distance preservation   |\n","| **Isomap**               | Geodesic distance preservation                                                        | Preserve manifold structure                 | Good for nonlinear manifolds              | Heavy compute, noise-sensitive   | Data lying on curved manifolds             |\n","| **t-SNE (KL Divergence)**| $$L = D_{KL}(P^{HD} \\parallel Q^{LD}) = \\sum_{i \\ne j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$$ | Match high vs low-dim neighborhood distributions | Great for local clustering                | Poor global structure, non-parametric | Visualizing clusters in high-dim data      |\n","| **UMAP (Cross-Entropy)** | $$L = -\\sum \\Big[p_{ij}\\log q_{ij} + (1-p_{ij})\\log(1-q_{ij})\\Big]$$                  | Graph-based neighbor preservation           | Scales better than t-SNE, preserves both local & global | Sensitive to hyperparams        | Scalable nonlinear embedding               |\n","| **Autoencoder (MSE)**    | $$L = \\|X - g(f(X))\\|^2$$                                                             | Encodeâ€“decode to minimize reconstruction    | Learns nonlinear embeddings               | Needs large data, may overfit    | General nonlinear DR, DL pipelines         |\n","| **Denoising AE**          | $$L = \\|X - g(f(\\tilde{X}))\\|^2$$                                                    | Reconstruct clean from noisy inputs         | Robust feature learning                   | Needs noise design               | Robust representation learning             |\n","| **Sparse AE**            | $$L = \\|X - \\hat{X}\\|^2 + \\lambda \\|h\\|_1$$                                          | Enforce sparse hidden features              | Feature selection in latent space         | Extra hyperparam tuning          | Compressed sensing, embeddings             |\n","| **Contractive AE**       | $$L = \\|X - \\hat{X}\\|^2 + \\lambda \\|\\nabla_x f(x)\\|_F^2$$                             | Invariance to small input perturbations     | Smooth, robust representations            | Heavy (Jacobian cost)            | Robust DR in vision, speech                |\n","| **VAE (ELBO)**           | $$\\mathcal{L} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - KL(q(z|x)\\parallel p(z))$$         | Probabilistic latent space                  | Generative, interpretable latent vars     | May blur details                 | Generative modeling + DR                   |\n","| **Î²-VAE**                | Same as VAE but KL scaled by $$\\beta > 1$$                                            | Disentangled latent factors                 | Interpretable, structured representations | May underfit                     | Disentangled representation learning       |\n","| **VQ-VAE**               | Recon. + quantization + commitment losses                                             | Discrete latent embeddings                  | Good for discrete data (speech, text)     | Codebook collapse risk            | Discrete embeddings, compression           |\n","| **DEC (Clustering)**     | $$L = KL(P \\parallel Q)$$ between soft assignments                                    | Align latent clusters with targets          | Clustering + DR jointly                   | Sensitive to initialization       | Unsupervised clustering + embedding        |\n","| **SimCLR (NT-Xent)**     | $$L = -\\log \\frac{\\exp(\\text{sim}(z_i,z_j)/\\tau)}{\\sum_{k \\ne i}\\exp(\\text{sim}(z_i,z_k)/\\tau)}$$ | Pull positives close, push negatives apart  | Strong embeddings via contrastive SSL     | Needs large batch sizes           | Vision/NLP self-supervised embeddings      |\n","| **Triplet Loss**         | $$L = \\max(0, d(a,p) - d(a,n) + m)$$                                                  | Anchorâ€“positive close, negative far         | Good for metric learning                  | Needs careful triplet mining      | Face verification, retrieval tasks         |\n","| **BYOL / SwAV**          | Variants of contrastive / clustering losses w/o negatives                             | Self-supervised latent structuring          | No negatives needed                       | Trickier stability                | SSL rep. learning, multimodal embeddings   |\n","\n","---\n","\n","âœ… **Insights**\n","\n","- **Classical ML losses** (PCA, MDS, t-SNE, UMAP): preserve **variance, distances, or neighborhoods**.  \n","- **Deep Learning losses** (Autoencoders, VAEs, VQ-VAEs): reconstruction + probabilistic/discrete latent modeling.  \n","- **Modern SSL/contrastive losses** (SimCLR, BYOL, SwAV, Triplet): learn embeddings through **instance discrimination**.  \n","\n","ðŸ‘‰ Choose depending on data + goal:  \n","- **Linear DR** â†’ PCA.  \n","- **Nonlinear manifold visualization** â†’ t-SNE, UMAP.  \n","- **Generative latent space** â†’ VAE, Î²-VAE.  \n","- **Discrete codebook embeddings** â†’ VQ-VAE.  \n","- **Joint clustering** â†’ DEC.  \n","- **Strong SSL features** â†’ Contrastive losses.  \n"],"metadata":{"id":"zzU4RO5JX0LC"}}]}