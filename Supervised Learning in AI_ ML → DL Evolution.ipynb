{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPVw8EvWsq5rbiCTZmleOJG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üìú Supervised Learning in AI: ML ‚Üí DL Evolution\n","\n","---\n","\n","## üîπ Definition\n","- **Supervised learning** is the paradigm where a model learns a mapping:  \n","  \\[\n","  f(x) \\; \\to \\; y\n","  \\]  \n","  from labeled data.  \n","- Each input is paired with a target label, and the model optimizes parameters to minimize prediction error.  \n","- **Goal:** Generalize to unseen data.  \n","- **Tasks:** Classification, regression, ranking, structured prediction.  \n","\n","---\n","\n","## üîπ Supervised Learning in Classical ML (Pre-Deep Era)\n","\n","Before deep learning, most AI relied on **shallow supervised learners**:\n","\n","| **Model / Algorithm** | **Year** | **Authors** | **Key Idea / Use** |\n","|------------------------|----------|-------------|---------------------|\n","| **Linear Regression** | 1805 | Legendre, Gauss | First regression model. |\n","| **Logistic Regression** | 1958 | Cox, others | Probabilistic binary classification. |\n","| **k-Nearest Neighbors (kNN)** | 1967 | Cover & Hart | Instance-based supervised classification. |\n","| **Decision Trees (ID3/C4.5)** | 1986‚Äì1993 | Quinlan | Rule-based supervised learning. |\n","| **SVM (Support Vector Machines)** | 1995 | Cortes & Vapnik | Margin maximization for classification. |\n","| **Random Forests** | 2001 | Breiman | Ensembles of decision trees. |\n","| **Gradient Boosting / XGBoost** | 2001 / 2016 | Friedman, Chen | Boosted ensembles; strong for structured/tabular data. |\n","\n","‚û°Ô∏è Dominated supervised AI in **1980s‚Äì2000s**, especially for structured/tabular data.  \n","\n","---\n","\n","## üîπ Supervised Learning in Deep Learning\n","\n","Deep learning scaled supervised training to massive datasets across vision, speech, and language:\n","\n","### 1. Feedforward Networks (FNN/MLP)\n","- **Perceptron** ‚Äì Rosenblatt (1958): First supervised NN classifier.  \n","- **MLPs + Backpropagation** ‚Äì Rumelhart, Hinton & Williams (1986): Trained multilayer networks for classification & regression.  \n","\n","### 2. Convolutional Neural Networks (CNNs)\n","- **LeNet-5** ‚Äì LeCun et al. (1998): Handwriting recognition.  \n","- **AlexNet** ‚Äì Krizhevsky et al. (2012): ImageNet breakthrough, first large-scale supervised CNN success.  \n","- **ResNet** ‚Äì He et al. (2015): Residual connections enabled ultra-deep supervised CNNs.  \n","- **Modern CNNs:** EfficientNet (2019), ConvNeXt (2022).  \n","\n","### 3. Recurrent Neural Networks (RNNs)\n","- **LSTM** ‚Äì Hochreiter & Schmidhuber (1997): Solved vanishing gradients; sequence modeling.  \n","- **Deep Speech (2014):** Supervised LSTM-based speech recognition at scale.  \n","\n","### 4. Transformers in Supervised NLP & Vision\n","- **BERT fine-tuning** ‚Äì Devlin et al. (2018): Pretrain unsupervised ‚Üí fine-tune supervised, new paradigm in NLP.  \n","- **Vision Transformers (ViT)** ‚Äì Dosovitskiy et al. (2021): Supervised Transformer on large image datasets.  \n","- **ConvNeXt** ‚Äì Liu et al. (2022): CNNs redesigned for large-scale supervised benchmarks.  \n","\n","---\n","\n","## üîπ Applications of Supervised Learning\n","- **Computer Vision:** Object detection, classification (ImageNet, COCO).  \n","- **NLP:** Sentiment analysis, translation (pre-Transformer), QA.  \n","- **Speech Recognition:** End-to-end supervised systems (e.g., Deep Speech).  \n","- **Healthcare:** Medical image diagnosis, disease classification.  \n","- **Finance:** Fraud detection, risk modeling, credit scoring.  \n","\n","---\n","\n","## ‚úÖ Key Insights\n","- In **classical ML**, supervised learning = the **core paradigm** (SVM, trees, ensembles).  \n","- In **deep learning**, supervised learning = the **engine** behind CNN breakthroughs (AlexNet, ResNet), RNN-based speech (Deep Speech), and Transformer-based NLP (BERT, ViT).  \n","- **Today:** Supervised fine-tuning is often combined with **self-supervised pretraining**, yielding foundation models adapted to downstream tasks.  \n"],"metadata":{"id":"IYKhqPJICt69"}}]}