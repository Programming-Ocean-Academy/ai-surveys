{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSUt97jTr+PD+C8WIbOiyd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ“œ Feedforward Neural Networks (FNN) Timeline & Breakthroughs\n","\n","---\n","\n","## ðŸ“š Key Milestones\n","\n","| **Era** | **Model / Concept** | **Year** | **Authors / Org** | **Key Contributions** |\n","|---------|----------------------|----------|-------------------|-----------------------|\n","| **Foundations** | **Perceptron** | 1958 | Frank Rosenblatt | First algorithm for a single-layer feedforward net; trainable via weight updates. |\n","| | **Multilayer Perceptron (MLP)** | 1962 | Rosenblatt (*Principles of Neurodynamics*) | Introduced multiple hidden layers, but lacked an effective training algorithm. |\n","| **Core Training Breakthroughs** | **Backpropagation (reverse-mode autodiff)** | 1970 | Seppo Linnainmaa | First formal description of reverse-mode differentiation. |\n","| | **Backpropagation in Neural Nets** | 1986 | Rumelhart, Hinton & Williams | *Nature 1986*: Popularized backpropagation as practical training for multilayer FNNs. |\n","| **Theory** | **Universal Approximation Theorem** | 1989 | Cybenko; Hornik | Proved that a single hidden-layer FNN with nonlinear activations can approximate any continuous function. |\n","| **Scaling & Deep FNNs** | **Deep Belief Nets (pretraining)** | 2006 | Hinton, Osindero & Teh | Unsupervised pretraining with stacked RBMs to train deep multilayer FNNs. |\n","| | **ReLU Activation** | 2011 | Glorot, Bordes & Bengio | *AISTATS 2011*: Showed ReLUs enabled stable training of deeper FNNs. |\n","| **Modern Usage & Variants** | **Wide & Deep Models** | 2016 | Cheng et al., Google | Combined shallow (memorization) and deep (generalization) FNNs for recommender systems. |\n","| | **MLP-Mixer** | 2021 | Tolstikhin et al., Google Research | Pure FNN-style architecture (no conv, no attention) for vision; competitive with large-scale training. |\n","\n","---\n","\n","## âœ… Summary\n","- **Origin:** Perceptron (1958) â†’ MLPs (1960s).  \n","- **Training solved:** Backpropagation (1970â€“1986).  \n","- **Theory foundation:** Universal Approximation (1989).  \n","- **Deep era:** Pretraining (2006), ReLU (2011).  \n","- **Modern usage:** Wide & Deep (2016), MLP-Mixer (2021).  \n"],"metadata":{"id":"7Gv4nqMS6o4T"}}]}