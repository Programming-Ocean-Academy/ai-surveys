{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMH2H0PPXNGl42ttzIKF7zW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üìú Evaluation Metrics for Classification in AI/ML/DL\n","\n","---\n","\n","## üîπ 1. Basic Metrics\n","\n","**Accuracy**\n","\n","$$\n","\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n","$$\n","\n","‚úÖ Intuitive, widely used  \n","‚ùå Misleading with class imbalance  \n","\n","**Error Rate**\n","\n","$$\n","\\text{Error Rate} = 1 - \\text{Accuracy}\n","$$\n","\n","---\n","\n","## üîπ 2. Class-Specific Metrics (Precision, Recall, F1)\n","\n","**Precision (Positive Predictive Value)**\n","\n","$$\n","\\text{Precision} = \\frac{TP}{TP + FP}\n","$$\n","\n","‚ûù Out of predicted positives, how many were correct.\n","\n","**Recall (Sensitivity / True Positive Rate)**\n","\n","$$\n","\\text{Recall} = \\frac{TP}{TP + FN}\n","$$\n","\n","‚ûù Out of actual positives, how many were detected.\n","\n","**F1-Score** (harmonic mean of precision & recall)\n","\n","$$\n","F1 = \\frac{2 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{\\text{Precision} + \\text{Recall}}\n","$$\n","\n","**Specificity (True Negative Rate)**\n","\n","$$\n","\\text{Specificity} = \\frac{TN}{TN + FP}\n","$$\n","\n","‚ûù Measures correct rejection of negatives.\n","\n","---\n","\n","## üîπ 3. Threshold-Based Metrics\n","\n","- **ROC Curve:** plots TPR vs. FPR at various thresholds.  \n","- **AUC (ROC-AUC):** probability that classifier ranks a random positive higher than a random negative.  \n","- **PR Curve (Precision‚ÄìRecall):** better in imbalanced datasets.  \n","- **AUC-PR:** summarizes PR curve into a single number.  \n","\n","---\n","\n","## üîπ 4. Advanced & Multi-Class Metrics\n","\n","- **Macro Averaging:** average metric across classes (treats all equally).  \n","- **Micro Averaging:** aggregate contributions across classes (class-size weighted).  \n","- **Weighted Averaging:** weighted by class frequency.  \n","- **Top-K Accuracy:** correct if true label in top-K predictions (common in ImageNet).  \n","\n","---\n","\n","## üîπ 5. Imbalanced Classification Metrics\n","\n","**Balanced Accuracy**\n","\n","$$\n","\\text{Balanced Accuracy} = \\frac{\\text{TPR} + \\text{TNR}}{2}\n","$$\n","\n","**Cohen‚Äôs Kappa**  \n","Measures agreement beyond chance.\n","\n","**Matthews Correlation Coefficient (MCC):**\n","\n","$$\n","\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n","$$\n","\n","‚úÖ Balanced even with skewed classes.  \n","\n","---\n","\n","## üîπ 6. Domain-Specific / Task-Specific Metrics\n","\n","- **Log-Loss (Cross-Entropy):** evaluates probabilistic predictions.  \n","- **Brier Score:** mean squared difference between predicted probability and true outcome.  \n","- **Hamming Loss:** fraction of misclassified labels in multi-label classification.  \n","- **Jaccard Index (IoU):** overlap metric, widely used in segmentation/multi-label tasks.  \n","\n","---\n","\n","## ‚úÖ Summary by Context\n","\n","- **Balanced datasets:** Accuracy, F1, ROC-AUC  \n","- **Imbalanced datasets:** Precision, Recall, PR-AUC, MCC  \n","- **Probabilistic outputs:** Log-Loss, Brier Score  \n","- **Multi-class:** Macro/micro F1, Top-K accuracy  \n","- **Multi-label:** Hamming Loss, Jaccard Index  \n"],"metadata":{"id":"yV_xSQQeZrYU"}},{"cell_type":"markdown","source":["# üìä Comparative Table: Evaluation Metrics for Classification (AI/ML/DL)\n","\n","| Metric | Formula (simplified) | Intuition | Pros | Cons | When to Use |\n","|--------|----------------------|-----------|------|------|-------------|\n","| **Accuracy** | $$\\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}$$ | Fraction of correct predictions | Simple, intuitive | Misleading in imbalanced data | Balanced datasets |\n","| **Error Rate** | $$1 - \\text{Accuracy}$$ | Proportion of mistakes | Easy interpretation | Same issues as Accuracy | Quick error reporting |\n","| **Precision** | $$\\text{Precision} = \\frac{TP}{TP+FP}$$ | Of predicted positives, how many were right | Good for reducing false alarms | Ignores false negatives | Imbalanced data where FP are costly |\n","| **Recall (Sensitivity, TPR)** | $$\\text{Recall} = \\frac{TP}{TP+FN}$$ | Of actual positives, how many caught | Captures completeness | Ignores false positives | Medical tests, fraud detection |\n","| **Specificity (TNR)** | $$\\text{Specificity} = \\frac{TN}{TN+FP}$$ | Of actual negatives, how many rejected | Complements Recall | Ignores false negatives | Screening where FP are expensive |\n","| **F1-Score** | $$F1 = \\frac{2 \\cdot P \\cdot R}{P+R}$$ | Harmonic mean of Precision & Recall | Balances FP & FN | Hard to interpret for business | Imbalanced data, NLP, CV |\n","| **Balanced Accuracy** | $$\\frac{TPR + TNR}{2}$$ | Averages sensitivity & specificity | Handles imbalance better | Still ignores class priors | Imbalanced datasets |\n","| **MCC (Matthews Corr. Coef.)** | $$\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$ | Correlation between prediction & truth | Balanced, robust | Harder to explain | Medical, bioinformatics, imbalanced data |\n","| **Cohen‚Äôs Kappa** | $$\\kappa = \\frac{p_o - p_e}{1 - p_e}$$ | Agreement beyond chance | Adjusts for random guessing | Less common in DL | Multi-class imbalance |\n","| **ROC-AUC** | Area under ROC (TPR vs FPR) | Threshold-free separability | Robust to imbalance | Can overestimate under skew | Binary classification, ranking |\n","| **PR-AUC** | Area under Precision‚ÄìRecall curve | Focus on positive class | Better than ROC in imbalance | Less stable at low recall | Highly imbalanced datasets |\n","| **Log-Loss (Cross-Entropy)** | $$-\\frac{1}{n} \\sum \\left[ y \\log p + (1-y)\\log(1-p) \\right]$$ | Penalizes wrong probabilities | Measures calibration | Sensitive to outliers | Probabilistic classifiers |\n","| **Brier Score** | $$\\frac{1}{n}\\sum (p-y)^2$$ | Squared error of probabilities | Calibration measure | Doesn‚Äôt handle imbalance | Weather forecasting, risk models |\n","| **Hamming Loss** | $$\\frac{1}{n}\\sum 1(y \\ne \\hat{y})$$ | Fraction of misclassified labels | Works for multi-label | Not informative for imbalance | Multi-label classification |\n","| **Jaccard Index (IoU)** | $$\\text{IoU} = \\frac{TP}{TP+FP+FN}$$ | Overlap between predicted & true labels | Clear overlap metric | Ignores TNs | Multi-label, segmentation |\n","| **Top-K Accuracy** | $$1 \\; \\text{if true label} \\in \\text{top K predictions}$$ | Checks rank correctness | Great for multi-class | Less useful in binary | ImageNet, NLP with large vocabularies |\n","\n","---\n","\n","## ‚úÖ Key Insights\n","- **Balanced data ‚Üí** Accuracy, ROC-AUC, F1  \n","- **Imbalanced data ‚Üí** Precision, Recall, PR-AUC, MCC  \n","- **Probabilistic models ‚Üí** Log-Loss, Brier score  \n","- **Multi-class ‚Üí** Macro/micro/weighted F1, Top-K accuracy  \n","- **Multi-label ‚Üí** Hamming Loss, Jaccard Index  \n"],"metadata":{"id":"zHmZiwChaArO"}},{"cell_type":"markdown","source":["# üìú Timeline of Classification Evaluation Metrics (1950‚Äì2025)\n","\n","---\n","\n","## üîπ 1950s‚Äì1960s: Early Metrics\n","- **Accuracy & Error Rate**  \n","  - First formalized in early statistical classification.  \n","  - Used in **perceptrons** (Rosenblatt, 1958) and **logistic regression** (1958).  \n","  - Formula:  \n","    $$\\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}, \\quad \\text{Error Rate} = 1 - \\text{Accuracy}$$  \n","  - ‚úÖ Intuitive, simple  \n","  - ‚ùå Misleading with class imbalance  \n","\n","---\n","\n","## üîπ 1970s: Precision & Recall\n","- **Precision & Recall** (Salton et al., Information Retrieval)  \n","  - Precision = correctness of positives.  \n","    $$\\text{Precision} = \\frac{TP}{TP+FP}$$  \n","  - Recall = completeness of positives.  \n","    $$\\text{Recall} = \\frac{TP}{TP+FN}$$  \n","  - ‚ûù Established the **Precision‚ÄìRecall trade-off**, later critical for IR and NLP.  \n","\n","---\n","\n","## üîπ 1980s: ROC & AUC\n","- **ROC Curves**: Originated in **signal detection theory**, applied to ML evaluation.  \n","- **AUC (Area Under ROC Curve)**: Summarizes classifier performance across thresholds.  \n","  - Widely adopted in **medical diagnostics** (sensitivity vs specificity).  \n","  - Became a **gold standard** for binary classification evaluation.  \n","\n","---\n","\n","## üîπ 1990s: Beyond Accuracy\n","- **F1-Score** (IR/NLP standard):  \n","  - Harmonic mean of Precision & Recall.  \n","    $$F1 = \\frac{2 \\cdot P \\cdot R}{P+R}$$  \n","- **MCC (Matthews Correlation Coefficient)** (Matthews, 1975 ‚Üí popularized 1990s):  \n","  - Balanced metric using all confusion matrix entries.  \n","- **Cohen‚Äôs Kappa** (1960s ‚Üí adopted in ML in the 1990s):  \n","  - Measures agreement beyond chance, useful for multi-class imbalance.  \n","\n","---\n","\n","## üîπ 2000s: Multi-Class & Probabilistic Metrics\n","- **Macro/Micro/Weighted Averaging** ‚Üí standardized for **multi-class classification**.  \n","- **Log-Loss (Cross-Entropy as eval)**:  \n","  - Penalizes wrong probabilities.  \n","- **Brier Score**:  \n","  - Probabilistic calibration metric, widely used in **weather/risk models**.  \n","\n","---\n","\n","## üîπ 2010s: Deep Learning & Large-Scale Classification\n","- **Top-K Accuracy** (ImageNet, 2012):  \n","  - Critical for CNN benchmarks (AlexNet, VGG, ResNet).  \n","- **PR-AUC (Precision‚ÄìRecall AUC)**:  \n","  - Highlighted as superior to ROC-AUC for **imbalanced datasets**.  \n","- **Hamming Loss & Jaccard Index (IoU)**:  \n","  - Adopted in **multi-label classification** and **image segmentation**.  \n","\n","---\n","\n","## üîπ 2020s: Foundation Model Era\n","- **Calibration Metrics (ECE, Brier variants)**:  \n","  - Ensure probability outputs reflect real-world frequencies.  \n","- **Ranking Metrics (NDCG, MAP)**:  \n","  - Crucial for **retrieval-based multimodal AI** (e.g., CLIP, search engines).  \n","- **Task-Specific Metrics**:  \n","  - NLP ‚Üí BLEU, ROUGE, F1.  \n","  - CV ‚Üí IoU/Jaccard for segmentation.  \n","- **AI Safety & Fairness Metrics**:  \n","  - Fairness, robustness, bias evaluation in classification tasks.  \n","\n","---\n","\n","## ‚úÖ Key Trends\n","- **1950s‚Äì1980s**: Accuracy ‚Üí Precision/Recall ‚Üí ROC-AUC.  \n","- **1990s**: Metrics for imbalance (F1, MCC, Kappa).  \n","- **2000s**: Multi-class & probabilistic (Log-Loss, Brier).  \n","- **2010s**: Deep learning benchmarks (Top-K, PR-AUC, IoU).  \n","- **2020s**: Foundation models, fairness & calibration metrics.  \n"],"metadata":{"id":"C6y1DPvPaPiP"}}]}