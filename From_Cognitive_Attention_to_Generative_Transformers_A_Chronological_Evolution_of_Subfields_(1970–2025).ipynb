{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìñ Chronological Evolution of Attention & Transformer Subfields (1970‚Äì2025)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. üß† Foundations & Precursor Ideas (1970‚Äì1990s)\n",
        "\n",
        "- 1970s (Cognitive Science) ‚Äì ‚ÄúAttention‚Äù studied as selective focus in human perception. Inspired computational analogies.  \n",
        "- 1980s‚Äì1990s ‚Äì Early alignment & focus mechanisms in statistical MT: IBM alignment models (word-based, fertility, distortion probabilities).  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. üîÑ Neural Alignment & Early Attention (2000‚Äì2013)\n",
        "\n",
        "- 2003 ‚Äì Bengio et al.: first neural LM ‚Üí neural sequence modeling becomes possible.  \n",
        "- 2013 ‚Äì Kalchbrenner & Blunsom: **RCTM (Recurrent Continuous Translation Models)** ‚Äî CNN encoder + RNN LM, without explicit attention.  \n",
        "- **Limitation**: Fixed-length vector bottleneck in Seq2Seq ‚Üí motivated attention.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. üéØ Additive Attention Era (2014‚Äì2015)\n",
        "\n",
        "- 2014 (ICLR) ‚Äì **Seq2Seq (Sutskever, Vinyals, Le)**: RNN encoder-decoder with fixed vector ‚Üí bottleneck exposed.  \n",
        "- 2015 (ICLR) ‚Äì **Bahdanau et al. (Neural Machine Translation by Jointly Learning to Align and Translate):**  \n",
        "  - Introduces **Additive Attention**: learns soft alignment over source sequence.  \n",
        "  - Context vector becomes dynamic, variable-length, solving Seq2Seq bottleneck.  \n",
        "  - Foundation of modern neural attention.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. ‚ö° Dot-Product & Multiplicative Attention (2015‚Äì2016)\n",
        "\n",
        "- 2015 (EMNLP) ‚Äì **Luong et al.**: multiplicative (dot-product) attention.  \n",
        "- More efficient than Bahdanau‚Äôs additive formulation.  \n",
        "- Widely adopted in RNN seq2seq NMT.  \n",
        "- Attention becomes mainstream in MT, speech recognition, summarization.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. üèóÔ∏è Self-Attention & The Transformer Revolution (2017)\n",
        "\n",
        "- 2017 (NeurIPS) ‚Äì **Vaswani et al. ‚ÄúAttention Is All You Need‚Äù**:  \n",
        "  - Introduces **Scaled Dot-Product Self-Attention**.  \n",
        "  - Replaces recurrence entirely with parallelizable attention.  \n",
        "  - Transformer encoder-decoder ‚Üí SOTA in NMT.  \n",
        "  - **Key contributions**: Multi-Head Attention, Positional Encoding, LayerNorm, Residuals.  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. üìö Transformer Subfields (2018‚Äì2020)\n",
        "\n",
        "**Machine Translation & Language Modeling:**  \n",
        "- Transformer-big scales NMT (**Ott et al., 2018**).  \n",
        "- Tensor2Tensor, OpenNMT: standardized Transformer frameworks.  \n",
        "\n",
        "**Representation Learning:**  \n",
        "- **BERT (2018):** masked LM, bidirectional encoder ‚Üí breakthroughs in QA, classification.  \n",
        "- **GPT (2018):** autoregressive Transformer decoders.  \n",
        "- **XLNet, RoBERTa (2019):** improved pretraining regimes.  \n",
        "\n",
        "**Speech & Multimodal:**  \n",
        "- **Speech-Transformer** (Dong et al., 2018).  \n",
        "- **ViT** (Dosovitskiy et al., 2020): pure Transformer for images.  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. üåç Scaling & Efficiency Subfields (2019‚Äì2022)\n",
        "\n",
        "- **Scaling Laws:** Kaplan et al. (2020) formalize compute/data/parameter scaling laws.  \n",
        "- **Efficient Transformers:** Linformer, Performer, Reformer, Longformer ‚Üí address O(N¬≤) cost.  \n",
        "- **Multilingual Transformers:** mBERT, XLM-R.  \n",
        "- **Pretraining Paradigms:** T5 (Text-to-Text Transfer Transformer, 2019), BART (denoising autoencoder).  \n",
        "- **Generative Power:** GPT-2 (2019), GPT-3 (2020).  \n",
        "\n",
        "---\n",
        "\n",
        "## 8. üß© Applied Transformer Subfields\n",
        "\n",
        "**Vision:**  \n",
        "- **ViT (2020), DeiT, Swin Transformer** (hierarchical).  \n",
        "- **CoAtNet** (CNN+Transformer hybrid, 2021).  \n",
        "\n",
        "**Speech:**  \n",
        "- **wav2vec 2.0 (2020), HuBERT**.  \n",
        "\n",
        "**Protein & Science:**  \n",
        "- **AlphaFold2 (2021):** Transformer-based folding.  \n",
        "\n",
        "**Multimodality:**  \n",
        "- **CLIP (2021), Flamingo (2022), GPT-4V (2023).**  \n",
        "\n",
        "---\n",
        "\n",
        "## 9. ü§ñ LLM & Generative AI Subfields (2020‚Äì2025)\n",
        "\n",
        "**Conversational LLMs:**  \n",
        "- GPT-3.5, GPT-4, ChatGPT (2022).  \n",
        "- Claude (Anthropic), Gemini (Google), LLaMA (Meta).  \n",
        "\n",
        "**Instruction Tuning:**  \n",
        "- **InstructGPT (2022).**  \n",
        "\n",
        "**RLHF:**  \n",
        "- Reinforcement Learning from Human Feedback ‚Üí alignment with human preferences.  \n",
        "\n",
        "**Mixture-of-Experts:**  \n",
        "- **Switch Transformer (2021), Mixtral (2023).**  \n",
        "\n",
        "**RAG:**  \n",
        "- Retrieval-Augmented Transformers (Lewis et al., 2020) ‚Üí combine memory + attention.  \n",
        "\n",
        "---\n",
        "\n",
        "## 10. üèõÔ∏è Transformer Hybrids & Beyond (2023‚Äì2025)\n",
        "\n",
        "- **CNN + Transformer Hybrids:** ConvNeXt, CoAtNet.  \n",
        "- **Graph Transformers:** Graphormer (2021), Graph-BERT hybrids.  \n",
        "- **Neural-Symbolic Hybrids:** combining reasoning + attention.  \n",
        "- **Efficient AI at Edge:** quantized, distillation-based Transformers.  \n",
        "- **Reasoning Transformers:** chain-of-thought prompting, tool use, program-of-thought networks.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìë Summary\n",
        "\n",
        "The chronological evolution of attention & Transformers shows a paradigm shift:  \n",
        "\n",
        "- **1970s‚Äì1990s ‚Üí** Cognitive theories, statistical alignments.  \n",
        "- **2000‚Äì2013 ‚Üí** Neural seq2seq bottlenecks.  \n",
        "- **2015 ‚Üí** Bahdanau + Luong: attention as alignment.  \n",
        "- **2017 ‚Üí** Vaswani‚Äôs Transformer: self-attention replaces recurrence.  \n",
        "- **2018‚Äì2020 ‚Üí** Pretrained Transformers (BERT, GPT, ViT).  \n",
        "- **2020‚Äì2025 ‚Üí** Scaling laws, efficient Transformers, multimodal & LLMs (ChatGPT, Gemini, Claude).  \n"
      ],
      "metadata": {
        "id": "kBXwFa6yKh_H"
      }
    }
  ]
}