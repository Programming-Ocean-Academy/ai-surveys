{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNbYgLe/3ONY4CZhBB7WL2g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üìú Clustering Loss Functions in AI/ML/DL\n","\n","---\n","\n","## üîπ 1. Classical ML Clustering Losses\n","\n","### a) Partitioning-Based Losses\n","\n","**k-Means Loss (Distortion / Inertia):**\n","\n","$$\n","L = \\sum_{i=1}^n \\|x_i - c_{z_i}\\|^2\n","$$\n","\n","- Minimize squared distance between data points and their assigned cluster centroid.  \n","‚úÖ Simple, efficient  \n","‚ùå Sensitive to initialization, assumes spherical clusters  \n","\n","**k-Medoids Loss:**\n","- Uses actual data points (medoids) instead of centroids.  \n","‚úÖ More robust to outliers  \n","‚ùå Computationally heavier  \n","\n","---\n","\n","### b) Density-Based Clustering (DBSCAN, OPTICS)\n","\n","- No strict global loss.  \n","- Based on **density reachability**: clusters = dense regions separated by sparse areas.  \n","‚úÖ Arbitrary-shaped clusters  \n","‚ùå No explicit objective function  \n","\n","---\n","\n","### c) Probabilistic Clustering\n","\n","**Gaussian Mixture Models (GMMs):**\n","\n","$$\n","L = -\\sum_{i=1}^n \\log \\sum_{k=1}^K \\pi_k \\, \\mathcal{N}(x_i \\,|\\, \\mu_k, \\Sigma_k)\n","$$\n","\n","- Negative log-likelihood of the mixture distribution.  \n","‚úÖ Captures **soft memberships**  \n","‚ùå Assumes Gaussian components  \n","\n","**EM Algorithm:**  \n","- Maximizes expected complete-data likelihood.  \n","\n","---\n","\n","### d) Spectral Clustering\n","\n","**Normalized Cut Loss:**\n","\n","$$\n","\\text{Ncut}(A,B) = \\frac{\\text{cut}(A,B)}{\\text{assoc}(A,V)} + \\frac{\\text{cut}(A,B)}{\\text{assoc}(B,V)}\n","$$\n","\n","- Minimize edge weights between clusters relative to total edge weights.  \n","‚úÖ Good for graph-based clustering  \n","‚ùå Requires costly eigen-decomposition  \n","\n","---\n","\n","## üîπ 2. Clustering Losses in Deep Learning\n","\n","### a) Autoencoder-Based\n","\n","**Deep Embedded Clustering (DEC, 2016):**\n","\n","$$\n","L = KL(P \\parallel Q) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n","$$\n","\n","- KL divergence between soft assignments and target distribution.  \n","‚úÖ Joint feature learning + clustering  \n","‚ùå Sensitive to initialization  \n","\n","**Denoising Autoencoder Clustering:**  \n","- Loss = **Reconstruction error + clustering objective**.  \n","\n","---\n","\n","### b) Generative Model-Based\n","\n","**ClusterGAN (2018):**  \n","- Combines **GAN adversarial loss** + **clustering loss on latent codes**.  \n","‚úÖ Joint generation + clustering  \n","‚ùå GAN instability  \n","\n","**VAE + GMM (Variational Deep Embedding):**  \n","- Combines **VAE reconstruction loss + KL divergence + GMM likelihood**.  \n","\n","---\n","\n","### c) Contrastive / Self-Supervised Clustering\n","\n","**DeepCluster (2018):**  \n","- Loss = Cross-entropy between pseudo-labels (from k-Means) and predicted features.  \n","\n","**SwAV (2020):**  \n","- Loss = **Optimal transport assignment + contrastive loss**.  \n","- Enables **online clustering**.  \n","\n","---\n","\n","## üîπ 3. Clustering Evaluation Losses\n","\n","(Not training losses, but assessment metrics.)\n","\n","- **Silhouette Score:**  \n","$$\n","s = \\frac{b - a}{\\max(a, b)}\n","$$\n","\n","- **Davies‚ÄìBouldin Index:** average similarity between clusters.  \n","- **Adjusted Rand Index (ARI) / Mutual Information (MI):** compare to ground truth labels.  \n","\n","---\n","\n","## ‚úÖ Summary Families\n","\n","**Classical ML:**\n","- k-Means ‚Üí squared distance loss.  \n","- GMMs ‚Üí log-likelihood.  \n","- Spectral clustering ‚Üí graph cut objective.  \n","\n","**Deep Learning:**\n","- DEC ‚Üí KL divergence.  \n","- VAE+GMM ‚Üí likelihood + KL.  \n","- ClusterGAN ‚Üí adversarial + clustering.  \n","- Self-supervised ‚Üí DeepCluster, SwAV.  \n"],"metadata":{"id":"3X8hqtmiYsi0"}},{"cell_type":"markdown","source":["# üìä Comparative Table: Clustering Loss Functions (AI/ML/DL)\n","\n","| Loss / Method                  | Formula (simplified)                                                                 | Intuition                                      | Pros                                     | Cons                                       | When to Use                                   |\n","|--------------------------------|---------------------------------------------------------------------------------------|-----------------------------------------------|------------------------------------------|--------------------------------------------|-----------------------------------------------|\n","| **k-Means (Distortion)**       | $$L = \\sum_{i=1}^n \\|x_i - c_{z_i}\\|^2$$                                             | Minimize distance between points & centroids  | Simple, efficient                        | Assumes spherical clusters, sensitive init | General-purpose, large $$n$$, fast clustering |\n","| **k-Medoids**                  | $$L = \\sum_{i=1}^n \\|x_i - m_{z_i}\\|$$                                               | Same as k-Means but center = actual datapoint | Robust to outliers                       | More expensive than k-Means                | Small datasets, robustness needed              |\n","| **GMM (Gaussian Mixtures)**    | $$L = -\\sum_i \\log \\sum_k \\pi_k \\,\\mathcal{N}(x_i \\mid \\mu_k,\\Sigma_k)$$             | Fit Gaussian mixtures via NLL                 | Soft assignments, flexible                | Assumes Gaussian shape, expensive          | Probabilistic clustering, density estimation   |\n","| **Spectral Clustering (Ncut)** | $$\\text{Ncut}(A,B) = \\frac{\\text{cut}(A,B)}{\\text{assoc}(A,V)} + \\frac{\\text{cut}(A,B)}{\\text{assoc}(B,V)}$$ | Minimize inter-cluster graph connections     | Handles non-convex clusters               | Requires eigen-decomposition               | Graph data, manifold / nonlinear clustering   |\n","| **DBSCAN / OPTICS**            | Density-based (no closed loss)                                                       | Clusters = dense regions separated by noise   | Arbitrary shapes, noise-resilient         | Sensitive to density params                | Spatial data, irregular shapes                |\n","| **DEC (2016)**                 | $$L = KL(P \\parallel Q) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$$          | KL divergence between soft & target assigns   | Joint rep. learning + clustering          | Sensitive to initialization                | Deep unsupervised clustering                  |\n","| **VAE + GMM (VaDE)**           | $$L = L_{\\text{recon}} + KL + \\text{GMM likelihood}$$                                | Latent VAE + GMM clustering                  | Uncertainty-aware, probabilistic          | Complex training                           | DR + clustering with generative models        |\n","| **ClusterGAN (2018)**          | GAN loss + clustering penalty                                                        | Align latent GAN space with clusters          | Combines generation + clustering          | GAN instability                            | Generative + clustering tasks                 |\n","| **DeepCluster (2018)**         | Cross-Entropy between pseudo-labels & predictions                                   | Alt. between k-Means and CNN training         | Scales to large data                      | Sensitive to noise in labels               | Vision feature learning                       |\n","| **SwAV (2020)**                 | Contrastive + Optimal Transport assignment                                          | Aligns views by swapping assignments          | SSL + clustering jointly                  | Complex optimization                        | Large-scale SSL in vision/NLP                 |\n","\n","---\n","\n","## ‚úÖ Insights\n","\n","**Classical ML losses:**\n","- **k-Means** ‚Üí distance-based, efficient for convex clusters.  \n","- **GMM** ‚Üí probabilistic, soft memberships.  \n","- **Spectral** ‚Üí graph-based, captures nonlinear manifolds.  \n","- **DBSCAN** ‚Üí density-based, robust to noise/outliers.  \n","\n","**Deep Learning clustering losses:**\n","- **DEC** ‚Üí KL divergence, autoencoder-based clustering.  \n","- **VaDE** ‚Üí VAE + GMM, probabilistic latent clustering.  \n","- **ClusterGAN** ‚Üí GAN + clustering in latent space.  \n","- **DeepCluster / SwAV** ‚Üí self-supervised clustering for representation learning.  \n","\n","üëâ **When to choose:**  \n","- **Scalable & simple:** k-Means.  \n","- **Probabilistic clusters:** GMM.  \n","- **Nonlinear manifolds / graphs:** Spectral.  \n","- **High-dimensional deep reps:** DEC, DeepCluster.  \n","- **Foundation SSL:** SwAV, contrastive clustering.  \n"],"metadata":{"id":"rO6fG579Y_sv"}}]}