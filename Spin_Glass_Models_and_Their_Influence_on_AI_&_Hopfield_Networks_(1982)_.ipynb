{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spin Glass Models and Their Influence on AI\n",
        "\n",
        "---\n",
        "\n",
        "## Comparative Table\n",
        "\n",
        "| Aspect | Edwards–Anderson (EA) Model | Sherrington–Kirkpatrick (SK) Model | AI/ML Counterparts |\n",
        "|--------|------------------------------|------------------------------------|--------------------|\n",
        "| **Interaction Range** | Nearest-neighbor couplings on a \\( d \\)-dimensional lattice | Infinite-range couplings (any two spins may interact) | EA → Sparse/local interactions (associative memory); SK → Fully connected networks (dense layers) |\n",
        "| **Hamiltonian** | $$ H = - \\sum_{\\langle i j \\rangle} J_{ij} S_i S_j $$ | $$ H = -\\frac{1}{N} \\sum_{i<j} J_{ij} S_i S_j $$ | Directly analogous to energy functions in Hopfield and Boltzmann networks |\n",
        "| **Disorder** | Random \\( J_{ij} \\sim \\mathcal{N}(J_0, J^2) \\), nearest-neighbor | Same Gaussian random distribution, but global (mean-field) | Captures randomness in weights of early neural network models |\n",
        "| **Order Parameters** | Magnetization \\( m \\to 0 \\); overlap \\( q \\neq 0 \\) in glassy phase | Same, but with hierarchical **Replica Symmetry Breaking (RSB)** | \\( q \\leftrightarrow \\) memory overlap in Hopfield nets; RSB ↔ multiple attractor states in neural nets |\n",
        "| **Key Feature** | Finite-dimensional frustrated system with metastable states | Ultrametric hierarchy of states; non-ergodicity | Hopfield: multiple stable memories; Boltzmann/Deep Nets: rugged non-convex loss landscapes |\n",
        "| **Solution Methods** | Replica trick, mean-field approximations | Parisi’s RSB (1979), cavity method, rigorous proofs (2000s) | Analytical/statistical mechanics of learning; capacity analysis in perceptrons and Hopfield nets |\n",
        "| **Influence on AI** | Inspired Hopfield networks (1982) → associative memory with local stability & overlap parameter | Inspired Boltzmann machines (1985, Hinton & Sejnowski) and neural capacity analysis; analogy to deep learning landscapes | EA ↔ associative memory; SK ↔ global storage capacity & rugged optimization in deep nets |\n",
        "\n",
        "---\n",
        "\n",
        "## Key Connections\n",
        "\n",
        "- **EA → Hopfield Networks (1982)**  \n",
        "  The EA model’s overlap parameter  \n",
        "  $$\n",
        "  q = \\frac{1}{N} \\sum_i S_i^{(\\alpha)} S_i^{(\\beta)}\n",
        "  $$  \n",
        "  is mathematically identical to the overlap measure of stored/retrieved patterns in Hopfield associative memory.\n",
        "\n",
        "- **SK → Boltzmann Machines & Deep Networks**  \n",
        "  - SK’s infinite-range couplings mirror fully connected neural nets.  \n",
        "  - Parisi’s Replica Symmetry Breaking (RSB) maps to **multiple metastable basins** in energy, analogous to the many local minima in modern deep learning.  \n",
        "\n",
        "---\n",
        "\n",
        "## Broader AI Relevance  \n",
        "\n",
        "Both EA and SK models form the **statistical mechanics foundation of learning**:  \n",
        "\n",
        "- Storage capacity of associative memories (Hopfield).  \n",
        "- Generalization analysis (perceptrons, neural nets).  \n",
        "- Rugged optimization dynamics in deep networks.  \n",
        "\n",
        "They illustrate how **frustration, disorder, and hierarchical landscapes** in physics carry over to **neural learning and AI optimization**.  \n"
      ],
      "metadata": {
        "id": "_p8zKUpxnwa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hopfield Networks: From Spin Glasses to Modern Associative Memory\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Origins and Inspirations\n",
        "\n",
        "- **Psychological roots**:  \n",
        "  - Taylor (1956), Steinbuch’s *Lernmatrix* (1961), Kohonen (1974).  \n",
        "  - Modeled human associative recall.\n",
        "\n",
        "- **Statistical mechanics roots**:  \n",
        "  - **Ising model** (1920s): Static magnetism.  \n",
        "  - **Glauber dynamics** (1963): Time evolution of spins.  \n",
        "  - Nakano (1971), Amari (1972), Little (1974): Hebbian learning in Ising-like models.  \n",
        "  - **Spin glasses**: Sherrington–Kirkpatrick (1975) → rugged landscapes, many local minima → inspired Hopfield (1982).\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Classical Hopfield Network (Hopfield, 1982; 1984)\n",
        "\n",
        "- **Structure**: Fully connected recurrent net, symmetric weights (\\( w_{ij} = w_{ji} \\)), no self-connections.\n",
        "- **Energy Function**:  \n",
        "  $$\n",
        "  E = -\\frac{1}{2} \\sum_{i,j} w_{ij} s_i s_j - \\sum_i \\theta_i s_i\n",
        "  $$\n",
        "  Guarantees convergence to local minima (Lyapunov function).\n",
        "\n",
        "- **Dynamics**:  \n",
        "  - Asynchronous or synchronous updates.  \n",
        "  - State evolves to attractors (stored patterns).\n",
        "\n",
        "- **Learning Rule**:  \n",
        "  - Hebbian: “neurons that fire together wire together.”  \n",
        "  - Later: Storkey rule (1997) → higher storage capacity.\n",
        "\n",
        "- **Functionality**: Pattern completion, robust recall from noisy inputs.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Relation to Spin Glass Models\n",
        "\n",
        "- **EA model**: Nearest-neighbor Ising glass → local stability.  \n",
        "- **SK model**: Infinite-range Ising glass → equivalent to Hopfield with random weights.\n",
        "\n",
        "- **Mappings**:  \n",
        "  - Spins ↔ neurons  \n",
        "  - Bonds \\( J_{ij} \\) ↔ synaptic weights \\( w_{ij} \\)  \n",
        "  - Overlap \\( q \\) ↔ memory retrieval overlap  \n",
        "  - Energy landscape ↔ attractor basins\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Extensions and Advances\n",
        "\n",
        "- **Continuous Hopfield networks** (1984): Real-valued neurons, ODE dynamics.  \n",
        "- **Optimization** (Hopfield & Tank, 1985): NP-hard problems (e.g., TSP) mapped to energy minimization.  \n",
        "- **Capacity limits**:  \n",
        "  - Classical storage capacity:  \n",
        "    $$\n",
        "    p_{\\text{max}} \\approx 0.138 N\n",
        "    $$\n",
        "  - Spurious attractors arise if overloaded.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Modern Hopfield Networks (Dense Associative Memories, 2016+)\n",
        "\n",
        "- **Hopfield & Krotov**: Introduced higher-order interactions.  \n",
        "- **Energy Function (generalized)**:  \n",
        "  $$\n",
        "  E = - \\sum_{\\mu=1}^{N_{\\text{mem}}} F\\left( \\sum_{i=1}^N f(\\xi_i^\\mu V_i) \\right)\n",
        "  $$\n",
        "\n",
        "- **Capacity scaling**:  \n",
        "  - Polynomial: \\( F(x) = x^n \\) → storage \\(\\sim \\frac{N^{n-1}}{\\ln N} \\)  \n",
        "  - Exponential: \\( F(x) = e^x \\) → storage \\(\\sim 2^{N/2} \\)\n",
        "\n",
        "- **Connections to Attention**:  \n",
        "  - Continuous Hopfield nets with log-sum-exp reduce to Transformer attention.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Broader Implications\n",
        "\n",
        "- **Physics ↔ AI**: Spin glass → associative memory.  \n",
        "- **Cognitive science**: Memory recall models.  \n",
        "- **Modern AI**: Dense associative memory ↔ attention in Transformers.\n",
        "\n",
        "---\n",
        "\n",
        " **In summary**:  \n",
        "- *Classical Hopfield nets* = SK spin glass with Hebbian learning.  \n",
        "- *Energy landscape* = attractor memory recall + optimization.  \n",
        "- *Modern Hopfield nets* = exponential memory scaling + link to attention mechanisms.\n"
      ],
      "metadata": {
        "id": "RyD0luiAoJWn"
      }
    }
  ]
}