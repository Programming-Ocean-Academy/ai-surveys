{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spin Glass Models and Their Influence on AI\n",
        "\n",
        "---\n",
        "\n",
        "## Comparative Table\n",
        "\n",
        "| Aspect | Edwards–Anderson (EA) Model | Sherrington–Kirkpatrick (SK) Model | AI/ML Counterparts |\n",
        "|--------|------------------------------|------------------------------------|--------------------|\n",
        "| **Interaction Range** | Nearest-neighbor couplings on a \\( d \\)-dimensional lattice | Infinite-range couplings (any two spins may interact) | EA → Sparse/local interactions (associative memory); SK → Fully connected networks (dense layers) |\n",
        "| **Hamiltonian** | $$ H = - \\sum_{\\langle i j \\rangle} J_{ij} S_i S_j $$ | $$ H = -\\frac{1}{N} \\sum_{i<j} J_{ij} S_i S_j $$ | Directly analogous to energy functions in Hopfield and Boltzmann networks |\n",
        "| **Disorder** | Random \\( J_{ij} \\sim \\mathcal{N}(J_0, J^2) \\), nearest-neighbor | Same Gaussian random distribution, but global (mean-field) | Captures randomness in weights of early neural network models |\n",
        "| **Order Parameters** | Magnetization \\( m \\to 0 \\); overlap \\( q \\neq 0 \\) in glassy phase | Same, but with hierarchical **Replica Symmetry Breaking (RSB)** | \\( q \\leftrightarrow \\) memory overlap in Hopfield nets; RSB ↔ multiple attractor states in neural nets |\n",
        "| **Key Feature** | Finite-dimensional frustrated system with metastable states | Ultrametric hierarchy of states; non-ergodicity | Hopfield: multiple stable memories; Boltzmann/Deep Nets: rugged non-convex loss landscapes |\n",
        "| **Solution Methods** | Replica trick, mean-field approximations | Parisi’s RSB (1979), cavity method, rigorous proofs (2000s) | Analytical/statistical mechanics of learning; capacity analysis in perceptrons and Hopfield nets |\n",
        "| **Influence on AI** | Inspired Hopfield networks (1982) → associative memory with local stability & overlap parameter | Inspired Boltzmann machines (1985, Hinton & Sejnowski) and neural capacity analysis; analogy to deep learning landscapes | EA ↔ associative memory; SK ↔ global storage capacity & rugged optimization in deep nets |\n",
        "\n",
        "---\n",
        "\n",
        "## Key Connections\n",
        "\n",
        "- **EA → Hopfield Networks (1982)**  \n",
        "  The EA model’s overlap parameter  \n",
        "  $$\n",
        "  q = \\frac{1}{N} \\sum_i S_i^{(\\alpha)} S_i^{(\\beta)}\n",
        "  $$  \n",
        "  is mathematically identical to the overlap measure of stored/retrieved patterns in Hopfield associative memory.\n",
        "\n",
        "- **SK → Boltzmann Machines & Deep Networks**  \n",
        "  - SK’s infinite-range couplings mirror fully connected neural nets.  \n",
        "  - Parisi’s Replica Symmetry Breaking (RSB) maps to **multiple metastable basins** in energy, analogous to the many local minima in modern deep learning.  \n",
        "\n",
        "---\n",
        "\n",
        "## Broader AI Relevance  \n",
        "\n",
        "Both EA and SK models form the **statistical mechanics foundation of learning**:  \n",
        "\n",
        "- Storage capacity of associative memories (Hopfield).  \n",
        "- Generalization analysis (perceptrons, neural nets).  \n",
        "- Rugged optimization dynamics in deep networks.  \n",
        "\n",
        "They illustrate how **frustration, disorder, and hierarchical landscapes** in physics carry over to **neural learning and AI optimization**.  \n"
      ],
      "metadata": {
        "id": "_p8zKUpxnwa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hopfield Networks: From Spin Glasses to Modern Associative Memory\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Origins and Inspirations\n",
        "\n",
        "- **Psychological roots**:  \n",
        "  - Taylor (1956), Steinbuch’s *Lernmatrix* (1961), Kohonen (1974).  \n",
        "  - Modeled human associative recall.\n",
        "\n",
        "- **Statistical mechanics roots**:  \n",
        "  - **Ising model** (1920s): Static magnetism.  \n",
        "  - **Glauber dynamics** (1963): Time evolution of spins.  \n",
        "  - Nakano (1971), Amari (1972), Little (1974): Hebbian learning in Ising-like models.  \n",
        "  - **Spin glasses**: Sherrington–Kirkpatrick (1975) → rugged landscapes, many local minima → inspired Hopfield (1982).\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Classical Hopfield Network (Hopfield, 1982; 1984)\n",
        "\n",
        "- **Structure**: Fully connected recurrent net, symmetric weights (\\( w_{ij} = w_{ji} \\)), no self-connections.\n",
        "- **Energy Function**:  \n",
        "  $$\n",
        "  E = -\\frac{1}{2} \\sum_{i,j} w_{ij} s_i s_j - \\sum_i \\theta_i s_i\n",
        "  $$\n",
        "  Guarantees convergence to local minima (Lyapunov function).\n",
        "\n",
        "- **Dynamics**:  \n",
        "  - Asynchronous or synchronous updates.  \n",
        "  - State evolves to attractors (stored patterns).\n",
        "\n",
        "- **Learning Rule**:  \n",
        "  - Hebbian: “neurons that fire together wire together.”  \n",
        "  - Later: Storkey rule (1997) → higher storage capacity.\n",
        "\n",
        "- **Functionality**: Pattern completion, robust recall from noisy inputs.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Relation to Spin Glass Models\n",
        "\n",
        "- **EA model**: Nearest-neighbor Ising glass → local stability.  \n",
        "- **SK model**: Infinite-range Ising glass → equivalent to Hopfield with random weights.\n",
        "\n",
        "- **Mappings**:  \n",
        "  - Spins ↔ neurons  \n",
        "  - Bonds \\( J_{ij} \\) ↔ synaptic weights \\( w_{ij} \\)  \n",
        "  - Overlap \\( q \\) ↔ memory retrieval overlap  \n",
        "  - Energy landscape ↔ attractor basins\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Extensions and Advances\n",
        "\n",
        "- **Continuous Hopfield networks** (1984): Real-valued neurons, ODE dynamics.  \n",
        "- **Optimization** (Hopfield & Tank, 1985): NP-hard problems (e.g., TSP) mapped to energy minimization.  \n",
        "- **Capacity limits**:  \n",
        "  - Classical storage capacity:  \n",
        "    $$\n",
        "    p_{\\text{max}} \\approx 0.138 N\n",
        "    $$\n",
        "  - Spurious attractors arise if overloaded.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Modern Hopfield Networks (Dense Associative Memories, 2016+)\n",
        "\n",
        "- **Hopfield & Krotov**: Introduced higher-order interactions.  \n",
        "- **Energy Function (generalized)**:  \n",
        "  $$\n",
        "  E = - \\sum_{\\mu=1}^{N_{\\text{mem}}} F\\left( \\sum_{i=1}^N f(\\xi_i^\\mu V_i) \\right)\n",
        "  $$\n",
        "\n",
        "- **Capacity scaling**:  \n",
        "  - Polynomial: \\( F(x) = x^n \\) → storage \\(\\sim \\frac{N^{n-1}}{\\ln N} \\)  \n",
        "  - Exponential: \\( F(x) = e^x \\) → storage \\(\\sim 2^{N/2} \\)\n",
        "\n",
        "- **Connections to Attention**:  \n",
        "  - Continuous Hopfield nets with log-sum-exp reduce to Transformer attention.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Broader Implications\n",
        "\n",
        "- **Physics ↔ AI**: Spin glass → associative memory.  \n",
        "- **Cognitive science**: Memory recall models.  \n",
        "- **Modern AI**: Dense associative memory ↔ attention in Transformers.\n",
        "\n",
        "---\n",
        "\n",
        " **In summary**:  \n",
        "- *Classical Hopfield nets* = SK spin glass with Hebbian learning.  \n",
        "- *Energy landscape* = attractor memory recall + optimization.  \n",
        "- *Modern Hopfield nets* = exponential memory scaling + link to attention mechanisms.\n"
      ],
      "metadata": {
        "id": "RyD0luiAoJWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Ising–Spin Glass–Neural Network Lineage\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Ernst Ising (1900–1998) and the Ising Model (1924)\n",
        "\n",
        "**Background:** German physicist, PhD student of Wilhelm Lenz.  \n",
        "\n",
        "**Model:** A lattice of binary spins \\( S_i \\in \\{-1, +1\\} \\) with nearest-neighbor interactions.\n",
        "\n",
        "$$\n",
        "E = - \\sum_{ij} J_{ij} S_i S_j\n",
        "$$\n",
        "\n",
        "**Contributions:**\n",
        "- Defined the mathematical framework of binary units with pairwise couplings.  \n",
        "- In 1D, showed no phase transition; later Onsager (1944) proved non-trivial phase transitions in 2D.  \n",
        "- Prototype for interacting systems across physics, biology, and social science.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Spin Glass Generalizations (1975)\n",
        "\n",
        "**Edwards–Anderson (EA) Model** – *Samuel F. Edwards & Philip W. Anderson*  \n",
        "- Introduced *random couplings* \\( J_{ij} \\) → disorder and frustration.  \n",
        "- Revealed **spin glass phase**: frozen disorder with many metastable states.  \n",
        "- Introduced the **overlap order parameter** \\( q \\), key for memory-like states.  \n",
        "\n",
        "**Sherrington–Kirkpatrick (SK) Model** – *David Sherrington & Scott Kirkpatrick*  \n",
        "- Infinite-range (mean-field) version: each spin interacts with every other spin.  \n",
        "- Led to **Replica Symmetry Breaking (RSB)** by *Giorgio Parisi (1979)*.  \n",
        "- Produced **hierarchical, ultrametric, non-ergodic energy landscapes** → analogous to memory organization in the brain.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Neural Network Adaptations\n",
        "\n",
        "**Amari (1972)**  \n",
        "- Incorporated **Hebbian learning** into an Ising-like model.  \n",
        "- First bridge from statistical mechanics → associative memory in neural networks.  \n",
        "\n",
        "**Hopfield Network (1982, John Hopfield)**  \n",
        "- Directly applied SK mathematics to neurons.  \n",
        "- Mapping: *spins ↔ neurons, couplings ↔ synapses*.  \n",
        "- **Energy minima ↔ stored memories (attractors).**  \n",
        "- Made physics-inspired associative memory networks central in AI & neuroscience.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Probabilistic Extensions\n",
        "\n",
        "**Boltzmann Machine (1985, Geoffrey Hinton & Terry Sejnowski)**  \n",
        "- Generalized Hopfield networks by adding **stochastic binary units**.  \n",
        "- Learning driven by the **Boltzmann distribution**, honoring *Ludwig Eduard Boltzmann*.  \n",
        "- Enabled probabilistic **generative modeling**.  \n",
        "\n",
        "**Restricted Boltzmann Machine (RBM)**  \n",
        "- Proposed as *Harmonium* by *Paul Smolensky (1986)*.  \n",
        "- Bipartite architecture: **visible ↔ hidden**, no intra-layer links.  \n",
        "- Efficient training with **Contrastive Divergence (Hinton, 2002)**.  \n",
        "- Foundation for **Deep Belief Networks (2006)** and the early deep learning revival.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Clarification of Names\n",
        "\n",
        "- **Ludwig Eduard Boltzmann (1844–1906):** Austrian physicist, founder of statistical mechanics → inspired *Boltzmann Machines*.  \n",
        "- **Samuel Edwards (1930–2015) & Philip Anderson (1923–2020):** Introduced the EA model → inspired spin glass perspective in AI.  \n",
        "-  No direct relation between Boltzmann and Edwards–Anderson; only a **historical convergence through statistical physics**.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Conclusion: The Correct Intellectual Lineage\n",
        "\n",
        "- **Ising (1924):** binary spin interactions.  \n",
        "- **EA & SK (1975):** disorder, frustration, spin glass theory.  \n",
        "- **Hopfield (1982):** deterministic associative memory.  \n",
        "- **Boltzmann Machine (1985):** stochastic energy-based learning.  \n",
        "- **RBM (1986; revived 2000s):** efficient training → foundation of deep learning.  \n",
        "\n",
        " **In short:**  \n",
        "**Ising → EA → SK → Hopfield → Boltzmann → RBM → Deep Learning.**  \n",
        "\n",
        "Each step enriched the framework — from **binary spins** to **disordered glasses**, to **associative memory models**, to **generative neural networks** that paved the way for modern AI.\n"
      ],
      "metadata": {
        "id": "4uWroiV6urZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Physics to AI: The Lineage of Spin Glasses and Neural Networks\n",
        "\n",
        "---\n",
        "\n",
        "## The Physicists Behind the Names\n",
        "\n",
        "**Ludwig Eduard Boltzmann (1844–1906)**  \n",
        "- Austrian physicist, founder of **statistical mechanics**.  \n",
        "- Introduced the **Boltzmann constant** and **Boltzmann distribution**.  \n",
        "- His ideas on thermal equilibrium inspired **Hinton & Sejnowski** to name the *Boltzmann Machine* (1985).  \n",
        "\n",
        "**Samuel F. Edwards (1930–2015) & Philip W. Anderson (1923–2020)**  \n",
        "- Developed the **Edwards–Anderson (EA) spin glass model** (1975).  \n",
        "- Extended the **Ising model** to include *random, frustrated interactions*.  \n",
        "- Revealed the existence of **spin glass phases** with rugged landscapes.  \n",
        "- Anderson won the **1977 Nobel Prize in Physics** for his work on disordered systems.  \n",
        "\n",
        " **Clarification**:  \n",
        "- *Boltzmann Machines* → named after **Boltzmann**.  \n",
        "- *Edwards–Anderson Model* → named after **Edwards & Anderson**.  \n",
        "- Despite “Eduard” vs “Edwards,” these are **different scientists** with no relation.  \n",
        "\n",
        "---\n",
        "\n",
        "## The Intellectual Lineage of Models\n",
        "\n",
        "### 1. Ising Model (1920s)  \n",
        "- Binary spins \\( S_i \\in \\{+1, -1\\} \\) with **nearest-neighbor interactions**.  \n",
        "- First model of **cooperative phenomena** and **phase transitions**.  \n",
        "- **Hamiltonian**:  \n",
        "$$\n",
        "H = - \\sum_{\\langle i j \\rangle} J_{ij} S_i S_j\n",
        "$$  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. Edwards–Anderson (EA) Model (1975)  \n",
        "- A **disordered Ising model** with random couplings \\( J_{ij} \\).  \n",
        "- Introduced the **overlap parameter** \\( q \\) to capture memory-like frozen states.  \n",
        "- Established the concept of **spin glasses**.  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. Sherrington–Kirkpatrick (SK) Model (1975)  \n",
        "- Infinite-range (mean-field) extension of EA: *every spin interacts with every other spin*.  \n",
        "- Produced a **rugged, hierarchical energy landscape**.  \n",
        "- Solved by **Parisi** with **Replica Symmetry Breaking (RSB)**.  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. Hopfield Network (1982)  \n",
        "- *John Hopfield* applied SK mathematics to **associative memory**.  \n",
        "- Mapping: *Spins ↔ Neurons, Couplings ↔ Synaptic weights*.  \n",
        "- **Energy function identical** to Ising/SK Hamiltonian.  \n",
        "- **Stored patterns = attractors** in the energy landscape.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5. Boltzmann Machine (1985)  \n",
        "- *Hinton & Sejnowski* extended Hopfield nets.  \n",
        "- Added **stochastic binary units** with the **Boltzmann distribution**.  \n",
        "- Allowed **learning** through contrastive phases (clamped vs free).  \n",
        "- Considered a **stochastic Ising model with learning**.  \n",
        "\n",
        "---\n",
        "\n",
        "### 6. Restricted Boltzmann Machine (RBM)  \n",
        "- *Paul Smolensky (1986)* → proposed as “Harmonium.”  \n",
        "- Bipartite structure: **Visible ↔ Hidden**, no intra-layer connections.  \n",
        "- Efficient training via **Contrastive Divergence (Hinton, 2002)**.  \n",
        "- Became the foundation of **Deep Belief Networks (2006)** and the **deep learning revival**.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Unified Conclusion\n",
        "\n",
        "- **Ising model** → foundation of energy-based binary systems.  \n",
        "- **EA/SK models** → added disorder and frustration, creating multiple attractor states.  \n",
        "- **Hopfield networks** → applied SK theory to associative memory in AI.  \n",
        "- **Boltzmann Machines** → introduced stochasticity and learnable probabilities, named after Boltzmann.  \n",
        "- **RBMs** → computationally feasible, enabled **DBNs** and modern deep learning.  \n",
        "\n",
        " **In short:**  \n",
        "**Ising → EA → SK → Hopfield → Boltzmann → RBM → Deep Learning.**  \n",
        "\n",
        "Each step brought us closer to today’s neural architectures, with names tracing back to *Boltzmann, Edwards, and Anderson* — different scientists across different eras.  \n"
      ],
      "metadata": {
        "id": "pzophzITu25_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jS0S3ao5uwOW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}