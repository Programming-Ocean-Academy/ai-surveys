{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1hJWSgzuz7/muMi7Z3nV1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ“œ Reinforcement Learning + Deep Learning Breakthroughs\n","\n","---\n","\n","## ðŸ“š Key Milestones\n","\n","| **Era** | **Model / Algorithm** | **Year** | **Authors / Org** | **Key Contributions** |\n","|---------|------------------------|----------|-------------------|-----------------------|\n","| **Core Algorithms** | **Policy Gradient Methods** | 1999 | Sutton et al. | Learn policies directly; foundation for continuous action RL. |\n","| | **Actorâ€“Critic Architectures** | 1999 â†’ 2015+ | Konda & Tsitsiklis; DeepMind, OpenAI | Combined value + policy learning; stabilized deep RL training. |\n","| | **Deep Q-Network (DQN)** | 2015 | Mnih et al., DeepMind | Combined Q-learning with deep CNNs; human-level Atari play directly from pixels. |\n","| **Landmark Breakthroughs** | **AlphaGo** | 2016 | Silver et al., DeepMind | First AI to beat a Go world champion; policy/value networks + MCTS. |\n","| | **AlphaZero** | 2017 | Silver et al., DeepMind | Tabula rasa self-play; mastered Go, Chess, Shogi without human data. |\n","| | **OpenAI Five** | 2018 | Berner et al., OpenAI | Professional-level play in Dota 2; scaled multi-agent deep RL. |\n","\n","---\n","\n","## âœ… Summary Families\n","- **Value-based RL + Deep Nets:** DQN (2015).  \n","- **Policy-based RL:** Policy Gradient (1999), Actorâ€“Critic (1999 â†’ scaled in 2010s).  \n","- **Self-play Breakthroughs:** AlphaGo (2016), AlphaZero (2017), OpenAI Five (2018).  \n"],"metadata":{"id":"XjdW_l1y-n-H"}}]}